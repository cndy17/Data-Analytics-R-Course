[
  {
    "objectID": "data-projects.html",
    "href": "data-projects.html",
    "title": "Data Projects",
    "section": "",
    "text": "New York City Public School Funding & Spending\n\nA dive into the funding and spending of New York City public schools. This project uses data from the NYC Open Data portal to analyze the relationship between funding and spending in NYC public schools. We will explore the relationship between funding and spending in NYC public schools with academic outcomes.\n\n\nRetirement Plan Monte Carlo Analysis\n\nA comparison of CUNY retirement plans using simulation and Monte Carlo analysis, considering historical data using APIs. We will understand the two retirement plan options by leveraging historical data from FRED and Alpha Vantage. Specifically, we’ll focus on key metrics, such as wage growth, inflation rates, equity returns (from U.S. and international markets), and bond returns.\nView Report\n\n\nElection Data Analysis\n\nAnalysis of the effect of the US Electoral College on US Presidential elections, comparing different ways that states allocate their Electoral College Votes to determine whether this has any effect on who is elected the President of these United States. Visualizes spatial data, working with official electoral records, and accessing public data repositories, such as those of the US Census Bureau.\nView Report\n\n\nProposing a Successful Film\n\nAn investigation of imdb data to act as Hollywood development executives, diving deep into Hollywood history to develop a pitch for a new movie. Working with large data and relational data structures\nView Report\n\n\nFiscal Characteristics of Major US Public Transit Systems\n\nAn exploration of ridership and the fiscal characteristics of US public transit authorities. In this project, I utilize dplyr operations (mutate, group_by, summarize, select, arrange, rename) in R to produce summary statistics.\nView Report"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "US Public Transit System Data Analysis",
    "section": "",
    "text": "When I was in Japan, I got to ride some of the most efficient transit systems in the world, characterized by its punctuality, its service, and the large numbers of passengers. Here in the United States, public transit is a whole different story. At the end of this analysis, I want to figure out what the most efficient transit system in the country is. How do I define efficiency? To me, efficiency can be broken down into ridership (trips, miles, the functionality) and financial efficiency (revenue, expenses). In this project, I will be investigating the usage and financial statistics of the US transit systems."
  },
  {
    "objectID": "mp01.html#introduction",
    "href": "mp01.html#introduction",
    "title": "US Public Transit System Data Analysis",
    "section": "",
    "text": "When I was in Japan, I got to ride some of the most efficient transit systems in the world, characterized by its punctuality, its service, and the large numbers of passengers. Here in the United States, public transit is a whole different story. At the end of this analysis, I want to figure out what the most efficient transit system in the country is. How do I define efficiency? To me, efficiency can be broken down into ridership (trips, miles, the functionality) and financial efficiency (revenue, expenses). In this project, I will be investigating the usage and financial statistics of the US transit systems."
  },
  {
    "objectID": "mp01.html#data-sets",
    "href": "mp01.html#data-sets",
    "title": "US Public Transit System Data Analysis",
    "section": "Data Sets",
    "text": "Data Sets\nThe National Transit Database (NTD) records the financial and operations of transit systems to keep track of the industry and provide public information and statistics. The data is collected by transit agencies and submitted to the Federal Administration (FTA) annually and reviewed by the FTA. The most recent and complete information available at the moment is for 2022. Let’s dive into it by exploring the following data sets from the FTA:\n\n2022 Fare Revenue\n2022 Expenses\nRidership\n\nI will be extracting data on fares, expenses,\n\nFare Revenue Data\n\n\n\n\n\n\n\n\nExpenses Data\n\n\n\n\n\n\n\n\nFinancials Data\nWe can inner join the Revenue and Expenses data into a more comprehensiive financials data set that we can do our analysis with.\nHere’s a sample of the Financials Data.\n\n\n\n\n\n\n\n\nTrips Data\nFrom the ridership data, I will be extracting information on public transportation trips taken by unlinked passengers.\n\n\n\n\n\n\n\n\nMiles Data\nAlso from the ridership data, I will be extracting information on the vehicle revenue miles.\n\n\n\n\n\n\n\n\nUsage Data\nWe can inner join the Trips and Miles data together using NTD ID into a data set on the usage.\nAttributes\nSome of the attribute namings are unclear so let’s use the FTA Glossary to interpret the data.\nRenaming\n\nrenaming UZA Name to metro_area\nreplacing the modes with their respective full names\nrenaming UPT to unlinked_passenger_trips\nrenaming VRM to vehicle_revenue_miles\n\nVRM (vehicle revenue miles): The miles that vehicles travel while in revenue service.\nUPT (unlinked passenger trips): The number of passengers who board public transportation vehicles. Passengers are counted each time they board vehicles no matter how many vehicles they use to travel from their origin to their destination.\nNow,, here’s a sample of the processed Usage Data.\n\n\n\n\n\n\n\n\nJoin Usage and Financial Data\nNext, we can join the Usage and Financial Data sets.\nIn order to do so, we need to get the Usage for 2022 in order to match the 2022 Financial data.\nSince we are joining on Mode, we need to convert the modes of the Financials data as well.\nAfter that, we can LEFT JOIN the two data sets.\nLet’s take a look at the Usage and Financials data."
  },
  {
    "objectID": "mp01.html#project-outcomes",
    "href": "mp01.html#project-outcomes",
    "title": "US Public Transit System Data Analysis",
    "section": "Project Outcomes",
    "text": "Project Outcomes\nI used summary statistics to explore the data sets processed above to extract insights that can shed light on efficiency of the US Public Transit Systems.\nLibraries: tidyverse, dplyr\nLet’s see what the data can tell us about public transit in the US looking at transit Usage and Financial data.\n\nVehicle Revenue Miles\nWhat transit agency had the most total VRM in this sample?\nMTA New York City Transit with 10832855350 total miles\n\nlibrary(dplyr)\nUSAGE |&gt; \n  group_by(Agency) |&gt;\n  summarise(total_VRM = sum(vehicle_revenue_miles)) |&gt;\n  arrange(desc(total_VRM)) |&gt;\n  slice(1)\n\n# A tibble: 1 × 2\n  Agency                      total_VRM\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 MTA New York City Transit 10832855350\n\n\nWhat transit mode had the most total VRM in this sample?\nThe Bus at 49444494088 total miles\n\nUSAGE |&gt;\n  group_by(Mode) |&gt;\n  summarise(total_VRM = sum(vehicle_revenue_miles)) |&gt;\n  arrange(desc(total_VRM)) |&gt;\n  slice(1)\n\n# A tibble: 1 × 2\n  Mode    total_VRM\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Bus   49444494088\n\n\nWhat mode of transport had the longest average trip in May 2024?\nThe Heavy Rail did with 2654864 average miles\n\nUSAGE |&gt; \n  filter(month == \"2024-05-01\") |&gt; \n  group_by(Mode) |&gt; \n  summarise(average_VRM = mean(vehicle_revenue_miles)) |&gt; \n  arrange(desc(average_VRM)) |&gt;\n  slice(1)\n\n# A tibble: 1 × 2\n  Mode       average_VRM\n  &lt;chr&gt;            &lt;dbl&gt;\n1 Heavy Rail    2654864.\n\n\n\n\nUnlinked Passenger Trips\nHow many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\nA total of 237383777 trips were taken.\n\nTRIPS |&gt; \n  filter(Mode == \"HR\", month == \"2024-05-01\") |&gt; \n  summarise(total_trips = sum(UPT))\n\n# A tibble: 1 × 1\n  total_trips\n        &lt;dbl&gt;\n1   237383777\n\n\nHow much did NYC subway ridership fall between April 2019 and April 2020?\nRidership fell by 296864650 between April 2018 and April 2020.\n\napril_2020 &lt;- USAGE |&gt; \n  filter(Mode == \"Heavy Rail\") |&gt; \n  filter(month == \"2020-04-01\") |&gt; \n  summarise(total_riders = sum(unlinked_passenger_trips))\n\napril_2019 &lt;- USAGE |&gt; \n  filter(Mode == \"Heavy Rail\") |&gt; \n  filter(month == \"2019-04-01\") |&gt; \n  summarise(total_riders = sum(unlinked_passenger_trips))\n\ndifference = abs(april_2020 - april_2019)\nprint(difference)\n\n  total_riders\n1    296864650\n\n\nWhich transit system (agency and mode) had the most UPT in 2022?\nThe MTA New York City Transit Heavy Rail had the most UPT in 2022 at 1793073801.\n\nUSAGE_AND_FINANCIALS |&gt; select(Agency, Mode, UPT) |&gt; \n  arrange(desc(UPT))|&gt;\n  slice(1)\n\n# A tibble: 1 × 3\n  Agency                    Mode              UPT\n  &lt;chr&gt;                     &lt;chr&gt;           &lt;dbl&gt;\n1 MTA New York City Transit Heavy Rail 1793073801\n\n\n\n\nThree more interesting transit facts\nWhich month had the highest number of average trips between 2002 and 2024?\nOctober with an average of 768205 trips.\n\nUSAGE &lt;- USAGE |&gt; mutate(month_number = month(month))\nUSAGE |&gt; group_by(month_number) |&gt; summarise(avg_UPT = mean(unlinked_passenger_trips)) |&gt; arrange(desc(avg_UPT)) |&gt;\n  slice(1)\n\n# A tibble: 1 × 2\n  month_number avg_UPT\n         &lt;dbl&gt;   &lt;dbl&gt;\n1           10 768206.\n\nUSAGE &lt;- USAGE |&gt; select(-month_number)\n\nWhich metro area had the most unlinked passenger trips?\nThe New York, Jersey City, and Newark area has the greatest total UPT.\n\nUSAGE |&gt; group_by(metro_area) |&gt; \n  summarise(total_UPT = sum(unlinked_passenger_trips)) |&gt; \n  arrange(desc(total_UPT)) |&gt; \n  slice(1)\n\n# A tibble: 1 × 2\n  metro_area                              total_UPT\n  &lt;chr&gt;                                       &lt;dbl&gt;\n1 New York--Jersey City--Newark, NY--NJ 84020935224\n\n\nWhich metro area offers the most modes of transit?\nSan Francisco–Oakland, CA with 13 Modes\n\nUSAGE |&gt; group_by(metro_area) |&gt; \n  summarise(mode_count = n_distinct(Mode)) |&gt; \n  arrange(desc(mode_count)) |&gt; \n  slice(1)\n\n# A tibble: 1 × 2\n  metro_area                 mode_count\n  &lt;chr&gt;                           &lt;int&gt;\n1 San Francisco--Oakland, CA         13\n\n\n\ndistinct(USAGE |&gt; select(Mode, metro_area) |&gt; filter(metro_area == \"San Francisco--Oakland, CA\"))\n\n# A tibble: 13 × 2\n   Mode              metro_area                \n   &lt;chr&gt;             &lt;chr&gt;                     \n 1 Heavy Rail        San Francisco--Oakland, CA\n 2 Monorail          San Francisco--Oakland, CA\n 3 Demand Response   San Francisco--Oakland, CA\n 4 Bus               San Francisco--Oakland, CA\n 5 Commuter Bus      San Francisco--Oakland, CA\n 6 Bus Rapid Transit San Francisco--Oakland, CA\n 7 Cable Car         San Francisco--Oakland, CA\n 8 Light Rail        San Francisco--Oakland, CA\n 9 Streetcar         San Francisco--Oakland, CA\n10 Trolleybus        San Francisco--Oakland, CA\n11 Ferryboat         San Francisco--Oakland, CA\n12 Vanpool           San Francisco--Oakland, CA\n13 Commuter Rail     San Francisco--Oakland, CA"
  },
  {
    "objectID": "mp01.html#financial-efficiency",
    "href": "mp01.html#financial-efficiency",
    "title": "US Public Transit System Data Analysis",
    "section": "Financial Efficiency",
    "text": "Financial Efficiency\n\nFarebox Recovery Among Major Systems\nFarebox recovery is defined as the highest ratio of Total Fares to Expenses and can be used to measure efficiency.\nWhich transit system (agency and mode) had the highest farebox recovery?\nTransit Authority of Central Kentucky Vanpool has the highest farebox recovery at 2.38.\n\nUSAGE_AND_FINANCIALS |&gt; select(Agency, Mode, `Total Fares`, Expenses) |&gt; \n  mutate(farebox_recovery = `Total Fares`/Expenses) |&gt; \n  arrange(desc(farebox_recovery))|&gt;\n  slice(1)\n\n# A tibble: 1 × 5\n  Agency                           Mode  `Total Fares` Expenses farebox_recovery\n  &lt;chr&gt;                            &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;\n1 Transit Authority of Central Ke… Vanp…         97300    40801             2.38\n\n\nWhich transit system (agency and mode) has the lowest expenses per UPT?\nSan Francisco Bay Area Rapid Transit District Heavy Rail has the lowest expenses per UPT at 0.396.\n\nUSAGE_AND_FINANCIALS |&gt; select(Agency, Mode, UPT, Expenses) |&gt; \n  mutate(Expenses_per_UPT = Expenses/UPT) |&gt; \n  arrange(Expenses_per_UPT)|&gt;\n  slice(1)\n\n# A tibble: 1 × 5\n  Agency                                  Mode     UPT Expenses Expenses_per_UPT\n  &lt;chr&gt;                                   &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;\n1 San Francisco Bay Area Rapid Transit D… Heav… 4.53e7 17965407            0.396\n\n\nWhich transit system (agency and mode) has the highest total fares per UPT?\nThe highest total fares per UPT belongs to Altoona Metro Transit’s Demand Response at 656 per UPT.\n\nUSAGE_AND_FINANCIALS |&gt; select(Agency, Mode, `Total Fares`, UPT) |&gt; \n  mutate(Total_Fares_per_UPT = `Total Fares`/UPT) |&gt; \n  arrange(desc(Total_Fares_per_UPT))|&gt;\n  slice(1)\n\n# A tibble: 1 × 5\n  Agency                Mode            `Total Fares`   UPT Total_Fares_per_UPT\n  &lt;chr&gt;                 &lt;chr&gt;                   &lt;dbl&gt; &lt;dbl&gt;               &lt;dbl&gt;\n1 Altoona Metro Transit Demand Response         17058    26                656.\n\n\nWhich transit system (agency and mode) has the lowest expenses per VRM?\nSan Francisco Bay Area Rapid Transit District’s Heavy Rail at 0.217 per VRM.\n\nUSAGE_AND_FINANCIALS |&gt; select(Agency, Mode, Expenses, VRM) |&gt; \n  mutate(Expense_VRM = Expenses/VRM) |&gt; \n  arrange(Expense_VRM)|&gt;\n  slice(1)\n\n# A tibble: 1 × 5\n  Agency                                       Mode  Expenses    VRM Expense_VRM\n  &lt;chr&gt;                                        &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n1 San Francisco Bay Area Rapid Transit Distri… Heav… 17965407 8.27e7       0.217\n\n\nWhich transit system (agency and mode) has the highest total fares per VRM?\nChicago Water Taxi (Wendella)’s Ferryboat at 237 total fares per VRM\n\nUSAGE_AND_FINANCIALS |&gt; select(Agency, Mode, `Total Fares`, VRM) |&gt; \n  mutate(Fares_VRM = `Total Fares`/VRM) |&gt;\n  arrange(desc(Fares_VRM)) |&gt;\n  slice(1)\n\n# A tibble: 1 × 5\n  Agency                        Mode      `Total Fares`   VRM Fares_VRM\n  &lt;chr&gt;                         &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Chicago Water Taxi (Wendella) Ferryboat        142473   600      237."
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "US Public Transit System Data Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIn terms of ridership, the MTA New York City Transit takes the win in with the most Vehicle Revenue Miles and the most Unlinked Passenger Trips in 2022. Ridership in the NYC, NJ, Newark area overall is the highest and the transit systems in the area are some of the most utilized public transit systems in the US. Financially, San Francisco’s BART Heavy Rail/Subway comes out on top with both the lowest expense per VRM and lowest expense per UPT. Additionally, San Francisco/Oakland, CA also offers the most modes of transportation. When it comes to usage, the MTA is the transit system that shines, covering the most revenue miles with its vehicles and servicing the most passenger trips. When finances are added to the picture, the BART seems to be the most cost effective transit system."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "The U.S. Electoral College (EC) system, by design, has a significant impact on presidential elections, often making the distribution of votes much more complex than a simple nationwide popular vote. The system has been debated, especially when the results diverge from the popular vote. This analysis’ primary goal is to assess how the allocation schemes impact the election outcomes and whether any bias exists, especially in favor of one political party."
  },
  {
    "objectID": "mp03.html#data-i-election-data",
    "href": "mp03.html#data-i-election-data",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Data I: ELection Data",
    "text": "Data I: ELection Data\nData Source: MIT Election Data Science Lab datasets From the MIT Election Data Science Lab, we are retrieving two data sets. First, are votes from all biennial congressional races in all 50 states from 1976 to 2020. Second, are statewide presidential vote cotes. This requires a download from the link\n\n\nLoad Libraries\nif (!require(\"readr\")) install.packages(\"readr\")\nif (!require(\"sf\")) install.packages(\"sf\")\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nif (!require(\"tidyr\")) install.packages(\"tidyr\")\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"DT\")) install.packages(\"DT\")\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"gt\")) install.packages(\"gt\")\nif (!require(\"plotly\")) install.packages(\"plotly\")\n\nlibrary(readr)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(plotly)\n\n\n\n1976-2022 House Data\n\n\nView Code\nHOUSE &lt;- read_csv(\"1976-2022-house.csv\")\nPRESIDENT &lt;- read_csv(\"1976-2020-president.csv\")\n\nsample_n(HOUSE, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n1976-2020 President Data\n\n\nView Code\nsample_n(HOUSE, 1000) |&gt;\n  DT::datatable()"
  },
  {
    "objectID": "mp03.html#data-ii-congressional-boundary-files-1976-to-2012",
    "href": "mp03.html#data-ii-congressional-boundary-files-1976-to-2012",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Data II: Congressional Boundary Files 1976 to 2012",
    "text": "Data II: Congressional Boundary Files 1976 to 2012\nData Source: Jeffrey B. Lewis, Brandon DeVine, and Lincoln Pritcher with Kenneth C. Martis This source give us the shapefiles for all US congressional districts from 1789 to 2012.\n\n\nView Code\nget_file &lt;- function(fname){\n  BASE_URL &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n  fname_ext &lt;- paste0(fname, \".zip\")\n  fname_ext1 &lt;- paste0(fname, \".shp\")\n  fname_extunzip &lt;- gsub(\".zip$\", \"\", fname_ext)\n  subfolder &lt;- \"districtshapes\"  # Subfolder where the shapefile is located\n  if(!file.exists(fname_ext)){\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL, \n                  destfile = fname_ext)\n  }\n  # Unzip the contents and save unzipped content\n  unzip(zipfile = fname_ext, exdir = fname_extunzip)\n  # Define File Path\n  shapefile_path &lt;- file.path(fname_extunzip, subfolder, fname_ext1)\n  # Read the shapefile\n  read_sf(shapefile_path)\n}\n\n# Download files by iterating through\nstart_congress = 95\nend_congress = 114\nfor (i in start_congress:end_congress) {\n  district_name &lt;- sprintf(\"districts%03d\", i)  # Formats as district001, district002, etc.\n  district_data &lt;- get_file(district_name)   # Download and read the shapefile\n  assign(district_name, district_data, envir = .GlobalEnv)  # Assign the data frame to a variable in the global environment\n}"
  },
  {
    "objectID": "mp03.html#data-iii-congressional-boundary-files-2014-to-present",
    "href": "mp03.html#data-iii-congressional-boundary-files-2014-to-present",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Data III: Congressional Boundary Files 2014 to Present",
    "text": "Data III: Congressional Boundary Files 2014 to Present\nData Source: US Census Bureau This data source provides district boundaries for more recent congressional elections.\n\n\nView Code\nget_congress_file &lt;- function(fname, year){\n  BASE_URL &lt;- sprintf(\"https://www2.census.gov/geo/tiger/TIGER%d/CD/\", year) #replace %d with year\n  fname_ext &lt;- paste0(fname, \".zip\")\n  fname_ext1 &lt;- paste0(fname, \".shp\")\n  fname_extunzip &lt;- gsub(\".zip$\", \"\", fname_ext)\n  \n  # Download File\n  if(!file.exists(fname_ext)){\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL, \n                  destfile = fname_ext)\n  }\n  # Unzip the contents and save unzipped content\n  unzip(zipfile = fname_ext, exdir = fname_extunzip)\n  # Define File Path\n  shapefile_path &lt;- file.path(fname_extunzip, fname_ext1)\n  # Read the shapefile\n  read_sf(shapefile_path)\n}\n\n# Download file for each district by iterating through each year\nbase_year = 2022\nbase_congress = 116  # Congress number for 2012\nfor (i in 0:10) {  # i will range from 0 (2022) to 10 (2012)\n  year &lt;- base_year - i\n  if (year &gt;= 2018) {congress &lt;- 116} \n  else if (year &gt;= 2016) {congress &lt;- 115} \n  else if (year &gt;= 2014) {congress &lt;- 114} \n  else if (year == 2013) {congress &lt;- 113} \n  else if (year == 2012) {congress &lt;- 112}\n  district_name &lt;- sprintf(\"tl_%d_us_cd%d\", year, congress)\n  district_data &lt;- get_congress_file(district_name, year)  # Download and read the shapefile\n  assign(district_name, district_data, envir = .GlobalEnv)  # Assign the data frame to a variable in the global environment\n  }"
  },
  {
    "objectID": "mp03.html#maps-shapefiles",
    "href": "mp03.html#maps-shapefiles",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Maps & Shapefiles",
    "text": "Maps & Shapefiles\n\nChloropleth Visualization of the 2000 Presidential Election Electoral College Results\nFilter Election Data for 2000: To create a map of the results broken down by states, we will need to find the election results of each state. The first step involves filtering the election dataset PRESIDENT to get the results for the year 2000. We specifically focus on the U.S. Presidential election and filter for the two main candidates, George W. Bush and Al Gore. We then calculate the winner for each state based on who received the most votes and assign the appropriate party.\n\n\nView Code\nelection_2000 &lt;- PRESIDENT |&gt;\n  filter(year == 2000, office == \"US PRESIDENT\") |&gt; # filter for 2000 and president office\n  filter(candidate %in% c(\"BUSH, GEORGE W.\", \"GORE, AL\")) |&gt; # filter for Bush and Gore\n  group_by(state) |&gt;\n  summarise( # Winner based on the candidate with the most votes\n    winner = if_else(sum(candidatevotes[candidate == \"BUSH, GEORGE W.\"]) &gt; sum(candidatevotes[candidate == \"GORE, AL\"]),\n      \"Bush\", \"Gore\"),\n    winner_party = case_when(# Party based on the candidate\n      winner == \"Bush\" ~ \"Republican\",\n      winner == \"Gore\" ~ \"Democrat\"\n    )) |&gt;\n  ungroup()\n\n\nJoin Election Data with Shapefiles: The next step is to join the election results with the geographical shapefile data. This step ensures that we can visualize the election results on a map by linking the state names in both datasets. The shapefile data is modified to ensure the state names are in uppercase to match the election data. After merging the data, we create a choropleth map of the contiguous U.S. states. We use geom_sf() to plot the states and color them based on the winning party (Republican or Democrat). The map is then customized to remove axis labels and grid lines for a clean visualization.\n\n\nView Code\n# join with shapefile\ndistricts106$STATENAME &lt;- toupper(districts106$STATENAME) # uppercase state name to match\n\ndis_election_2000 &lt;- left_join(districts106, election_2000, by = c(\"STATENAME\" = \"state\"), relationship = \"many-to-many\")\n\nmain_us &lt;- dis_election_2000 |&gt; filter(!STATENAME %in% c(\"ALASKA\", \"HAWAII\"))\n\nggplot(main_us, aes(geometry = geometry, fill = winner_party)) +\n  geom_sf() + \n  scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democrat\" = \"blue\")) +\n  theme_minimal() +\n  labs(title = \"U.S. Presidential Election Results by State in 2000\",\n       fill = \"Winning Party\") +\n  theme(\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  ) \n\n\n\n\n\n\n\n\n\nAdd Insets for Alaska and Hawaii: Because Alaska and Hawaii are geographically distant from the mainland U.S., we create insets for these two states. The data for Alaska and Hawaii is filtered separately, and individual maps are created for each. These insets are then added to the main U.S. map.\n\n\nView Code\n# contiguous US \nmain_us &lt;- dis_election_2000 |&gt; filter(!STATENAME %in% c(\"ALASKA\", \"HAWAII\"))\nmap_us &lt;- ggplot(main_us, aes(geometry = geometry, fill = winner_party)) +\n  geom_sf() + \n  scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democrat\" = \"blue\")) +\n  theme_minimal() +\n  labs(title = \"U.S. Presidential Election Results by State in 2000\",\n       fill = \"Winning Party\") +\n  theme_void() +\n  coord_sf(xlim = c(-130, -60), ylim = c(20, 50), expand = FALSE) \n\n# filter data for Alaska and Hawaii\nalaska &lt;- dis_election_2000 |&gt; filter(STATENAME == \"ALASKA\")\nhawaii &lt;- dis_election_2000 |&gt; filter(STATENAME == \"HAWAII\")\n\n# Alaska Inset\ninset_alaska &lt;- ggplot(alaska, aes(geometry = geometry, fill = winner_party)) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democrat\" = \"blue\")) +\n  theme_void() +\n  theme(legend.position = \"none\") + \n  coord_sf(xlim = c(-180, -140), ylim = c(50, 72), expand = FALSE)\n\n# Hawaii Inset\ninset_hawaii &lt;- ggplot(hawaii, aes(geometry = geometry, fill = winner_party)) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"Republican\" = \"red\", \"Democrat\" = \"blue\")) +\n  theme_void() +\n  theme(legend.position = \"none\") +\n  coord_sf(xlim = c(-161, -154), ylim = c(18, 23), expand = FALSE)\n\n# Combine Maps\ncombined_map &lt;- map_us +\n  annotation_custom(ggplotGrob(inset_alaska),\n                    xmin = -120, xmax = -130, # position\n                    ymin = 15, ymax = 40) +  # size\n  annotation_custom(ggplotGrob(inset_hawaii),\n                    xmin = -115, xmax = -100, # position\n                    ymin = 20, ymax = 30)    # size\nprint(combined_map)\n\n\n\n\n\n\n\n\n\n\n\nChloropleth Visualization of Electoral College Results Over Time\nData Preparation First, we need to clean the data to ensure they join properly. First, we convert to the same CRS. Then, I am adding a STATENAME column based on the STATEFP as well as changing STATENAME values to uppercase to match.\n\n\nView Code\n# convert to the same crs\ndistricts095 &lt;- st_transform(districts095, crs = st_crs(districts112))\ndistricts097 &lt;- st_transform(districts097, crs = st_crs(districts112))\ndistricts098 &lt;- st_transform(districts098, crs = st_crs(districts112))\ndistricts101 &lt;- st_transform(districts101, crs = st_crs(districts112))\ndistricts102 &lt;- st_transform(districts102, crs = st_crs(districts112))\ndistricts103 &lt;- st_transform(districts103, crs = st_crs(districts112))\ndistricts106 &lt;- st_transform(districts106, crs = st_crs(districts112))\ndistricts108 &lt;- st_transform(districts108, crs = st_crs(districts112))\ndistricts111 &lt;- st_transform(districts111, crs = st_crs(districts112))\ntl_2016_us_cd115 &lt;- st_transform(tl_2016_us_cd115, crs = st_crs(districts112))\ntl_2020_us_cd116 &lt;- st_transform(tl_2020_us_cd116, crs = st_crs(districts112))\n\n# convert state names for to uppercase join\ndistricts095$STATENAME &lt;- toupper(districts095$STATENAME)  \ndistricts097$STATENAME &lt;- toupper(districts097$STATENAME)  \ndistricts098$STATENAME &lt;- toupper(districts098$STATENAME)  \ndistricts101$STATENAME &lt;- toupper(districts101$STATENAME)  \ndistricts102$STATENAME &lt;- toupper(districts102$STATENAME)  \ndistricts103$STATENAME &lt;- toupper(districts103$STATENAME)  \ndistricts106$STATENAME &lt;- toupper(districts106$STATENAME)  \ndistricts108$STATENAME &lt;- toupper(districts108$STATENAME)  \ndistricts111$STATENAME &lt;- toupper(districts111$STATENAME)  \ndistricts112$STATENAME &lt;- toupper(districts112$STATENAME)  \n\n# add STATENAME column using statefp\n# https://www.mercercountypa.gov/dps/state_fips_code_listing.htm \ntl_2020_us_cd116 &lt;- tl_2020_us_cd116 |&gt;\n  mutate(STATENAME = case_when(\n    STATEFP == \"01\" ~ \"ALABAMA\",\n    STATEFP == \"02\" ~ \"ALASKA\",\n    STATEFP == \"04\" ~ \"ARIZONA\",\n    STATEFP == \"05\" ~ \"ARKANSAS\",\n    STATEFP == \"06\" ~ \"CALIFORNIA\",\n    STATEFP == \"08\" ~ \"COLORADO\",\n    STATEFP == \"09\" ~ \"CONNECTICUT\",\n    STATEFP == \"10\" ~ \"DELAWARE\",\n    STATEFP == \"11\" ~ \"DISTRICT OF COLUMBIA\",\n    STATEFP == \"12\" ~ \"FLORIDA\",\n    STATEFP == \"13\" ~ \"GEORGIA\",\n    STATEFP == \"15\" ~ \"HAWAII\",\n    STATEFP == \"16\" ~ \"IDAHO\",\n    STATEFP == \"17\" ~ \"ILLINOIS\",\n    STATEFP == \"18\" ~ \"INDIANA\",\n    STATEFP == \"19\" ~ \"IOWA\",\n    STATEFP == \"20\" ~ \"KANSAS\",\n    STATEFP == \"21\" ~ \"KENTUCKY\",\n    STATEFP == \"22\" ~ \"LOUISIANA\",\n    STATEFP == \"23\" ~ \"MAINE\",\n    STATEFP == \"24\" ~ \"MARYLAND\",\n    STATEFP == \"25\" ~ \"MASSACHUSETTS\",\n    STATEFP == \"26\" ~ \"MICHIGAN\",\n    STATEFP == \"27\" ~ \"MINNESOTA\",\n    STATEFP == \"28\" ~ \"MISSISSIPPI\",\n    STATEFP == \"29\" ~ \"MISSOURI\",\n    STATEFP == \"30\" ~ \"MONTANA\",\n    STATEFP == \"31\" ~ \"NEBRASKA\",\n    STATEFP == \"32\" ~ \"NEVADA\",\n    STATEFP == \"33\" ~ \"NEW HAMPSHIRE\",\n    STATEFP == \"34\" ~ \"NEW JERSEY\",\n    STATEFP == \"35\" ~ \"NEW MEXICO\",\n    STATEFP == \"36\" ~ \"NEW YORK\",\n    STATEFP == \"37\" ~ \"NORTH CAROLINA\",\n    STATEFP == \"38\" ~ \"NORTH DAKOTA\",\n    STATEFP == \"39\" ~ \"OHIO\",\n    STATEFP == \"40\" ~ \"OKLAHOMA\",\n    STATEFP == \"41\" ~ \"OREGON\",\n    STATEFP == \"42\" ~ \"PENNSYLVANIA\",\n    STATEFP == \"44\" ~ \"RHODE ISLAND\",\n    STATEFP == \"45\" ~ \"SOUTH CAROLINA\",\n    STATEFP == \"46\" ~ \"SOUTH DAKOTA\",\n    STATEFP == \"47\" ~ \"TENNESSEE\",\n    STATEFP == \"48\" ~ \"TEXAS\",\n    STATEFP == \"49\" ~ \"UTAH\",\n    STATEFP == \"50\" ~ \"VERMONT\",\n    STATEFP == \"51\" ~ \"VIRGINIA\",\n    STATEFP == \"53\" ~ \"WASHINGTON\",\n    STATEFP == \"54\" ~ \"WEST VIRGINIA\",\n    STATEFP == \"55\" ~ \"WISCONSIN\",\n    STATEFP == \"56\" ~ \"WYOMING\"\n  ))\n\ntl_2016_us_cd115 &lt;- tl_2016_us_cd115 |&gt;\n  mutate(STATENAME = case_when(\n    STATEFP == \"01\" ~ \"ALABAMA\",\n    STATEFP == \"02\" ~ \"ALASKA\",\n    STATEFP == \"04\" ~ \"ARIZONA\",\n    STATEFP == \"05\" ~ \"ARKANSAS\",\n    STATEFP == \"06\" ~ \"CALIFORNIA\",\n    STATEFP == \"08\" ~ \"COLORADO\",\n    STATEFP == \"09\" ~ \"CONNECTICUT\",\n    STATEFP == \"10\" ~ \"DELAWARE\",\n    STATEFP == \"11\" ~ \"DISTRICT OF COLUMBIA\",\n    STATEFP == \"12\" ~ \"FLORIDA\",\n    STATEFP == \"13\" ~ \"GEORGIA\",\n    STATEFP == \"15\" ~ \"HAWAII\",\n    STATEFP == \"16\" ~ \"IDAHO\",\n    STATEFP == \"17\" ~ \"ILLINOIS\",\n    STATEFP == \"18\" ~ \"INDIANA\",\n    STATEFP == \"19\" ~ \"IOWA\",\n    STATEFP == \"20\" ~ \"KANSAS\",\n    STATEFP == \"21\" ~ \"KENTUCKY\",\n    STATEFP == \"22\" ~ \"LOUISIANA\",\n    STATEFP == \"23\" ~ \"MAINE\",\n    STATEFP == \"24\" ~ \"MARYLAND\",\n    STATEFP == \"25\" ~ \"MASSACHUSETTS\",\n    STATEFP == \"26\" ~ \"MICHIGAN\",\n    STATEFP == \"27\" ~ \"MINNESOTA\",\n    STATEFP == \"28\" ~ \"MISSISSIPPI\",\n    STATEFP == \"29\" ~ \"MISSOURI\",\n    STATEFP == \"30\" ~ \"MONTANA\",\n    STATEFP == \"31\" ~ \"NEBRASKA\",\n    STATEFP == \"32\" ~ \"NEVADA\",\n    STATEFP == \"33\" ~ \"NEW HAMPSHIRE\",\n    STATEFP == \"34\" ~ \"NEW JERSEY\",\n    STATEFP == \"35\" ~ \"NEW MEXICO\",\n    STATEFP == \"36\" ~ \"NEW YORK\",\n    STATEFP == \"37\" ~ \"NORTH CAROLINA\",\n    STATEFP == \"38\" ~ \"NORTH DAKOTA\",\n    STATEFP == \"39\" ~ \"OHIO\",\n    STATEFP == \"40\" ~ \"OKLAHOMA\",\n    STATEFP == \"41\" ~ \"OREGON\",\n    STATEFP == \"42\" ~ \"PENNSYLVANIA\",\n    STATEFP == \"44\" ~ \"RHODE ISLAND\",\n    STATEFP == \"45\" ~ \"SOUTH CAROLINA\",\n    STATEFP == \"46\" ~ \"SOUTH DAKOTA\",\n    STATEFP == \"47\" ~ \"TENNESSEE\",\n    STATEFP == \"48\" ~ \"TEXAS\",\n    STATEFP == \"49\" ~ \"UTAH\",\n    STATEFP == \"50\" ~ \"VERMONT\",\n    STATEFP == \"51\" ~ \"VIRGINIA\",\n    STATEFP == \"53\" ~ \"WASHINGTON\",\n    STATEFP == \"54\" ~ \"WEST VIRGINIA\",\n    STATEFP == \"55\" ~ \"WISCONSIN\",\n    STATEFP == \"56\" ~ \"WYOMING\"\n  ))\n\n\nCreating a Systematic Election Data Function for Visualization In this section, I have created a function that systematically processes U.S. Presidential election data for each election year. The function takes as input the election year and the corresponding shapefile data and returns a prepared dataset. This allows for easy handling of election data from multiple years, and it can be used to visualize and analyze the results for any given year.\nThe create_election_data takes two arguments: - election_year: the specific year of the presidential election (e.g., 2000, 2004, etc.). - shapefile_data: the shapefile containing the geographical data for that election year. and it returns: - all_election_simplified: the merged dataset, which includes both the election results and the shapefile data.\n\n\nView Code\n# Function to create election data\ncreate_election_data &lt;- function(election_year, shapefile_data) {\n  # Step 1: Filter for the specific year and the simplified party\n  election_data &lt;- PRESIDENT |&gt;\n    filter(year == election_year, office == \"US PRESIDENT\") |&gt;  # Filter for the specific year and presidential election\n    filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n    group_by(state, state_fips, year) |&gt;  # Group by state and party\n    summarise(\n      winner_party = if_else(sum(candidatevotes[party_simplified == \"DEMOCRAT\"]) &gt; sum(candidatevotes[party_simplified == \"REPUBLICAN\"]),\n                             \"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n    ungroup() |&gt; \n    filter(!is.na(winner_party))\n  \n  # Step 2: Join with the shapefile data\n  dis_election &lt;- left_join(shapefile_data, election_data, by = c(\"STATENAME\" = \"state\"), relationship = \"many-to-many\")\n  #dis_election$year &lt;- year # add year column\n  return(dis_election)\n}\n# bind election data for each year into one file\nall_election_data &lt;- bind_rows(\n  election_data_2020 &lt;- create_election_data(2020, tl_2020_us_cd116),\n  election_data_2016 &lt;- create_election_data(2016, tl_2016_us_cd115),\n  election_data_2012 &lt;- create_election_data(2012, districts112),\n  election_data_2008 &lt;- create_election_data(2008, districts111),\n  election_data_2004 &lt;- create_election_data(2004, districts108),\n  election_data_2000 &lt;- create_election_data(2000, districts106),\n  election_data_1996 &lt;- create_election_data(1996, districts103),\n  election_data_1992 &lt;- create_election_data(1992, districts102),\n  election_data_1988 &lt;- create_election_data(1988, districts101),\n  election_data_1984 &lt;- create_election_data(1984, districts098),\n  election_data_1980 &lt;- create_election_data(1980, districts097),\n  election_data_1976 &lt;- create_election_data(1976, districts095)\n)\n\n# simplify map data\nsf::sf_use_s2(FALSE)\nall_election_simplified &lt;- st_simplify(all_election_data, dTolerance = 0.01)\n\n\nCreating the Election Results Map With the combined and simplified election data, we can now create a series of maps to visualize the election results for each year. The code below creates a map of the contiguous U.S. (excluding Alaska and Hawaii).\n\n\nView Code\nall_alaska &lt;- all_election_simplified |&gt; filter(STATENAME == \"ALASKA\")\nall_hawaii &lt;- all_election_simplified |&gt; filter(STATENAME == \"HAWAII\") \nall_main_us &lt;- all_election_simplified |&gt; filter(!STATENAME %in% c(\"ALASKA\", \"HAWAII\"), !is.na(winner_party))\n  \n  # Step 3: Main map for the contiguous U.S.\nall_map_us &lt;- ggplot(all_main_us, aes(geometry = geometry, fill = winner_party)) +\n  geom_sf() + \n  scale_fill_manual(values = c(\"REPUBLICAN\" = \"red\", \"DEMOCRAT\" = \"blue\")) +\n  theme_minimal() +\n  labs(title = \"U.S. Presidential Election Results by State and Year\",\n       fill = \"Winning Party\") +\n  theme_void() +\n  facet_wrap(~ year, ncol=3) \n\nprint(all_map_us)"
  },
  {
    "objectID": "mp03.html#state-wide-winner-take-all",
    "href": "mp03.html#state-wide-winner-take-all",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "State-Wide Winner-Take-All",
    "text": "State-Wide Winner-Take-All\nn this system, the candidate who wins the most votes in a state receives all of that state’s Electoral College votes, regardless of the margin of victory. In most states (except Nebraska and Maine), if Candidate A wins 51% of the vote in a state, they will receive all of that state’s Electoral Votes, even if Candidate B got 49% of the vote. Each state has a certain number of electoral votes (ECVs), based on its representation in Congress (Senators + House Representatives). Under this system, only the winner of the popular vote in the state gets those votes.\n\n\nView Code\nstate_wide_winner_take_all &lt;- PRESIDENT |&gt;\n  group_by(state, year) |&gt;\n  filter(candidatevotes == max(candidatevotes)) |&gt;\n  left_join(ECV, by = c(\"state\" = \"state\", \"year\" = \"year\")) |&gt;\n  select(state, year, candidate, party_simplified, ecv) |&gt;\n  filter(!is.na(ecv))\n\nstate_wide_winner_take_all &lt;- state_wide_winner_take_all |&gt; \n  group_by(year, candidate, party_simplified) |&gt; \n  summarise(total_ecv = sum(ecv), .groups = \"drop\") |&gt; # total ecv\n  arrange(year, total_ecv) |&gt; \n  group_by(year) |&gt; \n  mutate(winner = if_else(total_ecv == max(total_ecv), \"Yes\", \"No\")) |&gt;  # Mark winner\n  ungroup() |&gt;\n  arrange(year, party_simplified)\n\nstate_wide_winner_take_all |&gt; gt() |&gt;\n  tab_header(\n    title = \"State-Wide Winner-Take-All\"\n  ) |&gt;\n  cols_label( # display column names\n    year = \"Year\",\n    candidate = \"Candidate\",\n    party_simplified = \"Party\",\n    total_ecv = \"Electoral Votes\",\n    winner = \"Winning Candidate\"\n  )\n\n\n\n\n\n\n\n\nState-Wide Winner-Take-All\n\n\nYear\nCandidate\nParty\nElectoral Votes\nWinning Candidate\n\n\n\n\n1976\nCARTER, JIMMY\nDEMOCRAT\n294\nYes\n\n\n1976\nFORD, GERALD\nREPUBLICAN\n241\nNo\n\n\n1980\nCARTER, JIMMY\nDEMOCRAT\n87\nNo\n\n\n1980\nREAGAN, RONALD\nREPUBLICAN\n448\nYes\n\n\n1984\nMONDALE, WALTER\nDEMOCRAT\n10\nNo\n\n\n1984\nREAGAN, RONALD\nREPUBLICAN\n525\nYes\n\n\n1988\nDUKAKIS, MICHAEL\nDEMOCRAT\n109\nNo\n\n\n1988\nBUSH, GEORGE H.W.\nREPUBLICAN\n426\nYes\n\n\n1992\nCLINTON, BILL\nDEMOCRAT\n367\nYes\n\n\n1992\nBUSH, GEORGE H.W.\nREPUBLICAN\n168\nNo\n\n\n1996\nCLINTON, BILL\nDEMOCRAT\n376\nYes\n\n\n1996\nDOLE, ROBERT\nREPUBLICAN\n159\nNo\n\n\n2000\nGORE, AL\nDEMOCRAT\n264\nNo\n\n\n2000\nBUSH, GEORGE W.\nREPUBLICAN\n271\nYes\n\n\n2004\nKERRY, JOHN\nDEMOCRAT\n249\nNo\n\n\n2004\nBUSH, GEORGE W.\nREPUBLICAN\n286\nYes\n\n\n2008\nOBAMA, BARACK H.\nDEMOCRAT\n361\nYes\n\n\n2008\nMCCAIN, JOHN\nREPUBLICAN\n174\nNo\n\n\n2012\nOBAMA, BARACK H.\nDEMOCRAT\n329\nYes\n\n\n2012\nROMNEY, MITT\nREPUBLICAN\n206\nNo\n\n\n2016\nCLINTON, HILLARY\nDEMOCRAT\n230\nNo\n\n\n2016\nTRUMP, DONALD J.\nREPUBLICAN\n305\nYes\n\n\n2020\nBIDEN, JOSEPH R. JR\nDEMOCRAT\n306\nYes\n\n\n2020\nTRUMP, DONALD J.\nREPUBLICAN\n232\nNo\n\n\n\n\n\n\n\n\n\nView Code\nggplot(state_wide_winner_take_all, aes(x = factor(year), y = total_ecv, fill = party_simplified)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  #  keeps bars side-by-side\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n  theme_minimal() +\n  labs(\n    title = \"Total ECV Votes for Each Candidate in U.S. Presidential Elections\",\n    x = \"Year\",\n    y = \"Total ECV\",\n    fill = \"Party\"\n  )"
  },
  {
    "objectID": "mp03.html#district-wide-winner-take-all-state-wide-at-large-votes",
    "href": "mp03.html#district-wide-winner-take-all-state-wide-at-large-votes",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "District-Wide Winner-Take-All + State-Wide “At Large” Votes",
    "text": "District-Wide Winner-Take-All + State-Wide “At Large” Votes\nThis method allocates R ECVs to popular vote winner by congressional district and the remaining 2 ECVs to the state-wide popular vote winner. The hybrid system is used in Maine and Nebraska. In each congressional district, the candidate who wins the popular vote gets one electoral vote. Then, the state as a whole gives two additional “at-large” ECVs to the candidate who wins the overall state-wide popular vote. In Nebraska, if Candidate A wins three of the state’s districts, and Candidate B wins the other district and the statewide popular vote, the electoral votes might be split like this: - Candidate A: 3 ECVs from the districts. - Candidate B: 2 ECVs for winning the state-wide vote. This system allows for a split in how ECVs are allocated, unlike the traditional winner-take-all system where the candidate winning the state by a narrow margin would still receive all the state’s votes.\n\n\nView Code\n# look at statewide winner - assign 2 ecv\nstate_wide_winner &lt;- PRESIDENT |&gt;\n  group_by(state, year) |&gt;\n  mutate(statewide_winner = if_else(candidatevotes == max(candidatevotes), \"Yes\", \"No\")) |&gt;  # Mark statewide winner\n  ungroup() |&gt;\n  # Assign ECV based on who won the state\n  mutate(ECV = if_else(statewide_winner == \"Yes\", 2, 0)) |&gt; # assign the 2 ECV if statewide winner, else 0 ECV\n  select(state, year, candidate, candidatevotes, ECV) |&gt;\n  filter(!is.na(candidate))\n\n# look at winner of district - assign 1 ecv per district\n# Assume that the presidential candidate of the same party as the congressional representative wins that election.\n# Find the winner of each district in the HOUSE dataset\ndistrict_winners &lt;- HOUSE |&gt;\n  filter(year %in% c(\"1976\", \"1980\", \"1984\", \"1988\", \"1992\", \"1996\", \"2000\", \"2004\", \"2008\", \"2012\", \"2016\", \"2020\")) |&gt;\n  group_by(state, year, district) |&gt;\n  filter(candidatevotes == max(candidatevotes)) |&gt;\n  ungroup() |&gt;\n  mutate(ecv = 1)  # Assign 1 ECV for each winning district\n\n# Join the district winners with the PRESIDENT dataset to match the party\necv_assignment &lt;- district_winners |&gt;\n  left_join(PRESIDENT, by = c(\"state\", \"year\", \"party\" = \"party_simplified\"), relationship = \"many-to-many\") |&gt;\n  mutate(ecv_presidential = 1) |&gt;\n  select(state, year, district, candidate.y, party, ecv_presidential)\n\n#  Find total ecv from districts\ndistrict_ecv_summary &lt;- ecv_assignment |&gt;\n  group_by(state, year, candidate.y, party) |&gt;\n  summarise(district_total_ecv = sum(ecv_presidential), .groups = \"drop\")\n\n#  Join the district-level ECV summary with the statewide ECVs\necv_combined &lt;- state_wide_winner |&gt;\n  left_join(district_ecv_summary, by = c(\"state\", \"year\", \"candidate\" = \"candidate.y\")) |&gt;\n  # Add the statewide ECV to the district-level ECVs\n  mutate(total_ecv = district_total_ecv + ECV) |&gt;\n  filter(!is.na(total_ecv)) \n\necv_combined &lt;- ecv_combined |&gt; \n  group_by(year, candidate, party) |&gt; \n  summarise(total_ecv = sum(total_ecv), .groups = \"drop\") |&gt; # total ecv\n  arrange(year, total_ecv) |&gt; \n  group_by(year) |&gt; \n  mutate(winner = if_else(total_ecv == max(total_ecv), \"Yes\", \"No\")) |&gt;  # Mark winner\n  ungroup() \n\necv_combined |&gt; gt() |&gt;\n  tab_header(\n    title = \"District-Wide Winner-Take-All + State-Wide At Large Votes\"\n  ) |&gt;\n  cols_label( # display column names\n    year = \"Year\",\n    candidate = \"Candidate\",\n    party = \"Party\",\n    total_ecv = \"Electoral Votes\",\n    winner = \"Winning Candidate\"\n  )\n\n\n\n\n\n\n\n\nDistrict-Wide Winner-Take-All + State-Wide At Large Votes\n\n\nYear\nCandidate\nParty\nElectoral Votes\nWinning Candidate\n\n\n\n\n1976\nFORD, GERALD\nREPUBLICAN\n204\nNo\n\n\n1976\nCARTER, JIMMY\nDEMOCRAT\n362\nYes\n\n\n1980\nCARTER, JIMMY\nDEMOCRAT\n258\nNo\n\n\n1980\nREAGAN, RONALD\nREPUBLICAN\n287\nYes\n\n\n1984\nMONDALE, WALTER\nDEMOCRAT\n276\nNo\n\n\n1984\nREAGAN, RONALD\nREPUBLICAN\n283\nYes\n\n\n1988\nBUSH, GEORGE H.W.\nREPUBLICAN\n262\nNo\n\n\n1988\nDUKAKIS, MICHAEL\nDEMOCRAT\n292\nYes\n\n\n1992\nBUSH, GEORGE H.W.\nREPUBLICAN\n228\nNo\n\n\n1992\nCLINTON, BILL\nDEMOCRAT\n329\nYes\n\n\n1996\nCLINTON, BILL\nDEMOCRAT\n282\nNo\n\n\n1996\nDOLE, ROBERT\nREPUBLICAN\n283\nYes\n\n\n2000\nGORE, AL\nDEMOCRAT\n280\nNo\n\n\n2000\nBUSH, GEORGE W.\nREPUBLICAN\n290\nYes\n\n\n2004\nOTHER\nDEMOCRAT\n12\nNo\n\n\n2004\nKERRY, JOHN\nDEMOCRAT\n248\nNo\n\n\n2004\nBUSH, GEORGE W.\nREPUBLICAN\n299\nYes\n\n\n2008\nMCCAIN, JOHN\nREPUBLICAN\n224\nNo\n\n\n2008\nOBAMA, BARACK H.\nDEMOCRAT\n330\nYes\n\n\n2012\nOBAMA, BARACK H.\nDEMOCRAT\n275\nNo\n\n\n2012\nROMNEY, MITT\nREPUBLICAN\n286\nYes\n\n\n2016\nCLINTON, HILLARY\nDEMOCRAT\n291\nNo\n\n\n2016\nTRUMP, DONALD J.\nREPUBLICAN\n313\nYes\n\n\n2020\nTRUMP, DONALD J.\nREPUBLICAN\n263\nNo\n\n\n2020\nBIDEN, JOSEPH R. JR\nDEMOCRAT\n275\nYes\n\n\n\n\n\n\n\n\n\nView Code\nggplot(ecv_combined, aes(x = factor(year), y = total_ecv, fill = party)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  #  keeps bars side-by-side\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n  theme_minimal() +\n  labs(\n    title = \"Total ECV Votes for Each Candidate in U.S. Presidential Elections\",\n    x = \"Year\",\n    y = \"Total ECV\",\n    fill = \"Party\"\n  )"
  },
  {
    "objectID": "mp03.html#state-wide-proportional",
    "href": "mp03.html#state-wide-proportional",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "State-Wide Proportional",
    "text": "State-Wide Proportional\nUnder this system, electoral votes are distributed proportionally based on the percentage of votes each candidate receives in the state. If a candidate wins 60% of the vote in a state with 10 electoral votes, they get 60% of those electoral votes (6 ECVs). The approach here invovles calculating the total number of votes for each candidate in each state. Then, determine the proportion of the total vote that each candidate received in each state.\nNote: The rounding issue in proportional allocation methods does lead to the loss of some ECVs because, after rounding, the sum of the allocated votes may not match the total number of ECVs available for that state or for the entire country. Here, I allocate the remaining ECV to the candidate with the greatest proportion of votes.\n\n\nView Code\n# Allocate ECVs based on that proportion with rounding \nstate_proportional_votes &lt;- PRESIDENT |&gt;\n  group_by(state, year) |&gt;\n  mutate(vote_share = candidatevotes / sum(candidatevotes)) |&gt; # Proportion of votes\n  ungroup() |&gt;\n  left_join(ECV, by = c(\"state\", \"year\")) |&gt;\n  mutate(proportional_ecv = round(vote_share * ecv))  # Round to allocate ECVs\n\n# Summarize the total ECVs for each candidate by state and year\nstate_proportional_summary &lt;- state_proportional_votes |&gt;\n  group_by(state, year, candidate, party_simplified) |&gt;\n  summarise(total_proportional_ecv = sum(proportional_ecv), .groups = \"drop\") |&gt;\n  arrange(state, year, total_proportional_ecv) |&gt;\n  group_by(state, year) |&gt;\n  # Mark the winner with the most ECVs in each state and year\n  mutate(winner = if_else(total_proportional_ecv == max(total_proportional_ecv), \"Yes\", \"No\")) |&gt; \n  ungroup() \n\n# When we use proportions and round, some ECV  go unallocated\n# Allocate ECVs proportionally and round down\nstate_wide_prop &lt;- PRESIDENT |&gt;\n  group_by(state, year) |&gt;\n  mutate(vote_share = candidatevotes / sum(candidatevotes)) |&gt; # Proportion of votes\n  ungroup() |&gt;\n  left_join(ECV, by = c(\"state\", \"year\")) |&gt;\n  mutate(prop_ecv = vote_share * ecv, round_prop_ecv = round(vote_share * ecv))  |&gt;  # Round ECVs\n  group_by(state, year) |&gt;\n  mutate(remaining_ecvs = ecv - sum(round_prop_ecv)) |&gt;  # Calculate how many ECVs are left to allocate\n  ungroup() |&gt;\n  \n  # assign remainder to the max unrounded proportion\n  group_by(state, year) |&gt;\n\n  mutate(final_ecv = ifelse(vote_share == max(vote_share), \n                            round_prop_ecv + remaining_ecvs, \n                            round_prop_ecv)) |&gt;  # Allocate remaining ECVs to the candidate with max vote share\n  ungroup() |&gt;\n  select(year, state, candidate, party_simplified, ecv, prop_ecv, round_prop_ecv, remaining_ecvs, final_ecv)\n\n# Summarize the total allocated ECVs for each candidate\nstate_wide_prop_summary &lt;- state_wide_prop |&gt;\n  group_by(state, year, candidate, party_simplified) |&gt;\n  summarise(total_prop_ecv = sum(final_ecv), .groups = \"drop\") |&gt;\n  group_by(year, state) |&gt;\n  mutate(winner = if_else(total_prop_ecv == max(total_prop_ecv), \"Yes\", \"No\")) |&gt; \n  ungroup() |&gt;\n  filter(total_prop_ecv &gt; 0) |&gt;\n  select(year, candidate, party_simplified, total_prop_ecv, winner)\n\n# across states for the year\nstate_wide_totals &lt;- state_wide_prop_summary |&gt;\n  group_by(year, candidate, party_simplified) |&gt;\n  summarise(total_ecv = sum(total_prop_ecv), .groups = \"drop\") |&gt;\n  group_by(year) |&gt;\n  mutate(winner = if_else(total_ecv == max(total_ecv), \"Yes\", \"No\")) |&gt; \n  ungroup() |&gt;\n  filter(total_ecv &gt; 0) |&gt;\n  select(year, candidate, party_simplified, total_ecv, winner) |&gt;\n  arrange(year, desc(total_ecv))\n\n\n\n\nView Code\nggplot(state_wide_totals, aes(x = factor(year), y = total_ecv, fill = party_simplified)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  #  keeps bars side-by-side\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\", \"LIBERTARIAN\" = \"beige\", \"OTHER\" = \"gray\")) +\n  theme_minimal() +\n  labs(\n    title = \"Total ECV Votes for Each Candidate in U.S. Presidential Elections\",\n    x = \"Year\",\n    y = \"Total ECV\",\n    fill = \"Party\"\n  )\n\n\n\n\n\n\n\n\n\nView Code\nstate_wide_totals |&gt; gt() |&gt;\n  tab_header(\n    title = \"State-Wide Proportional\"\n  ) |&gt;\n  cols_label( # display column names\n    year = \"Year\",\n    candidate = \"Candidate\",\n    party_simplified = \"Party\",\n    total_ecv = \"Electoral Votes\",\n    winner = \"Winning Candidate\"\n  )\n\n\n\n\n\n\n\n\nState-Wide Proportional\n\n\nYear\nCandidate\nParty\nElectoral Votes\nWinning Candidate\n\n\n\n\n1976\nCARTER, JIMMY\nDEMOCRAT\n270\nYes\n\n\n1976\nFORD, GERALD\nREPUBLICAN\n261\nNo\n\n\n1976\nFORD, GERALD\nOTHER\n2\nNo\n\n\n1976\nCARTER, JIMMY\nOTHER\n1\nNo\n\n\n1976\nOTHER\nOTHER\n1\nNo\n\n\n1980\nREAGAN, RONALD\nREPUBLICAN\n281\nYes\n\n\n1980\nCARTER, JIMMY\nDEMOCRAT\n220\nNo\n\n\n1980\nANDERSON, JOHN B.\nOTHER\n31\nNo\n\n\n1980\nREAGAN, RONALD\nOTHER\n2\nNo\n\n\n1980\nCLARK, EDWARD \"\"ED\"\"\nLIBERTARIAN\n1\nNo\n\n\n1984\nREAGAN, RONALD\nREPUBLICAN\n321\nYes\n\n\n1984\nMONDALE, WALTER\nDEMOCRAT\n211\nNo\n\n\n1984\nREAGAN, RONALD\nOTHER\n2\nNo\n\n\n1984\nMONDALE, WALTER\nOTHER\n1\nNo\n\n\n1988\nBUSH, GEORGE H.W.\nREPUBLICAN\n291\nYes\n\n\n1988\nDUKAKIS, MICHAEL\nDEMOCRAT\n242\nNo\n\n\n1988\nBUSH, GEORGE H.W.\nOTHER\n1\nNo\n\n\n1988\nDUKAKIS, MICHAEL\nOTHER\n1\nNo\n\n\n1992\nCLINTON, BILL\nDEMOCRAT\n226\nYes\n\n\n1992\nBUSH, GEORGE H.W.\nREPUBLICAN\n203\nNo\n\n\n1992\nPEROT, ROSS\nOTHER\n103\nNo\n\n\n1992\nBUSH, GEORGE H.W.\nOTHER\n2\nNo\n\n\n1992\nBLANK VOTE/SCATTERING\nOTHER\n1\nNo\n\n\n1996\nCLINTON, BILL\nDEMOCRAT\n262\nYes\n\n\n1996\nDOLE, ROBERT\nREPUBLICAN\n223\nNo\n\n\n1996\nPEROT, ROSS\nOTHER\n42\nNo\n\n\n1996\nNA\nOTHER\n4\nNo\n\n\n1996\nBLANK VOTE/SCATTERING\nOTHER\n1\nNo\n\n\n1996\nCLINTON, BILL\nOTHER\n1\nNo\n\n\n1996\nDOLE, ROBERT\nOTHER\n1\nNo\n\n\n1996\nNADER, RALPH\nOTHER\n1\nNo\n\n\n2000\nGORE, AL\nDEMOCRAT\n263\nYes\n\n\n2000\nBUSH, GEORGE W.\nREPUBLICAN\n262\nNo\n\n\n2000\nNADER, RALPH\nOTHER\n6\nNo\n\n\n2000\nBLANK VOTE/SCATTERING\nOTHER\n1\nNo\n\n\n2000\nBUSH, GEORGE W.\nOTHER\n1\nNo\n\n\n2000\nNOT DESIGNATED\nOTHER\n1\nNo\n\n\n2000\nNA\nOTHER\n1\nNo\n\n\n2004\nBUSH, GEORGE W.\nREPUBLICAN\n278\nYes\n\n\n2004\nKERRY, JOHN\nDEMOCRAT\n255\nNo\n\n\n2004\nBUSH, GEORGE W.\nOTHER\n1\nNo\n\n\n2004\nKERRY, JOHN\nOTHER\n1\nNo\n\n\n2008\nOBAMA, BARACK H.\nDEMOCRAT\n285\nYes\n\n\n2008\nMCCAIN, JOHN\nREPUBLICAN\n247\nNo\n\n\n2008\nMCCAIN, JOHN\nOTHER\n2\nNo\n\n\n2008\nOBAMA, BARACK H.\nOTHER\n1\nNo\n\n\n2012\nOBAMA, BARACK H.\nDEMOCRAT\n271\nYes\n\n\n2012\nROMNEY, MITT\nREPUBLICAN\n261\nNo\n\n\n2012\nJOHNSON, GARY\nLIBERTARIAN\n1\nNo\n\n\n2012\nOBAMA, BARACK H.\nOTHER\n1\nNo\n\n\n2012\nROMNEY, MITT\nOTHER\n1\nNo\n\n\n2016\nCLINTON, HILLARY\nDEMOCRAT\n265\nYes\n\n\n2016\nTRUMP, DONALD J.\nREPUBLICAN\n257\nNo\n\n\n2016\nJOHNSON, GARY\nLIBERTARIAN\n8\nNo\n\n\n2016\nCLINTON, HILLARY\nOTHER\n1\nNo\n\n\n2016\nMCMULLIN, EVAN\nOTHER\n1\nNo\n\n\n2016\nSTEIN, JILL\nOTHER\n1\nNo\n\n\n2016\nTRUMP, DONALD J.\nOTHER\n1\nNo\n\n\n2016\nNA\nOTHER\n1\nNo\n\n\n2020\nBIDEN, JOSEPH R. JR\nDEMOCRAT\n273\nYes\n\n\n2020\nTRUMP, DONALD J.\nREPUBLICAN\n264\nNo\n\n\n2020\nJORGENSEN, JO\nLIBERTARIAN\n1\nNo"
  },
  {
    "objectID": "mp03.html#national-proportional",
    "href": "mp03.html#national-proportional",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "National Proportional",
    "text": "National Proportional\nThis system allocates ECVs based on the national popular vote, not state-by-state. So, each state’s contribution to the national total is proportional to the number of votes received by each candidate in the national election. If Candidate A wins 60% of the total national popular vote and Candidate B wins 40%, Candidate A would receive 60% of the total ECVs, and Candidate B would get 40%, regardless of how they performed in any individual state. This system would reduce the importance of individual states and the swing state effect, and might make the election outcomes more directly tied to the national popular vote.\n\n\nView Code\n# Find total ECV for each year \nelectoral_votes_available &lt;- ECV |&gt;\n  group_by(year) |&gt;\n  summarize(total_ecv = sum(ecv)) # sum ecv\n\nnation_wide_prop &lt;- PRESIDENT |&gt;\n  select(year, state, candidate, candidatevotes, party_simplified) |&gt;\n  group_by(year, candidate, party_simplified) |&gt;\n  summarize(candidate_total = sum(candidatevotes)) |&gt; # total votes nationwide per candidate per year\n  group_by(year) |&gt;\n  mutate(nation_total = sum(candidate_total)) |&gt;  # total votes nationwide per year\n  ungroup() |&gt;\n  mutate(prop_vote = (candidate_total / nation_total)) |&gt; # proportion of candidate votes to nationwide votes\n  select(-candidate_total, -nation_total) |&gt;\n  left_join(electoral_votes_available, join_by(year == year)) |&gt; # join with ECV\n  mutate(prop_ecv = round(prop_vote * total_ecv, digits = 0)) |&gt; # multiply proportion to total ecv that year\n  select(-prop_vote, -total_ecv) |&gt;\n  group_by(year)\n\n# Summarize the total allocated ECVs for each candidate\nnation_wide_summary &lt;- nation_wide_prop |&gt;\n  group_by(year) |&gt;\n  mutate(winner = if_else(prop_ecv == max(prop_ecv), \"Yes\", \"No\")) |&gt; \n  ungroup() |&gt;\n  filter(prop_ecv &gt; 0, !is.na(candidate)) |&gt;\n  select(year, candidate, prop_ecv, winner, party_simplified) |&gt;\n  arrange(year, desc(prop_ecv))\n\n\n\n\nView Code\nggplot(nation_wide_summary, aes(x = factor(year), y = prop_ecv, fill = party_simplified)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  #  keeps bars side-by-side\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\", \"LIBERTARIAN\" = \"beige\", \"OTHER\" = \"gray\")) +\n  theme_minimal() +\n  labs(\n    title = \"Total ECV Votes for Each Candidate in U.S. Presidential Elections\",\n    x = \"Year\",\n    y = \"Total ECV\",\n    fill = \"Party\"\n  )\n\n\n\n\n\n\n\n\n\nView Code\nnation_wide_summary |&gt; gt() |&gt;\n  tab_header(\n    title = \"Nation-Wide Proportional\"\n  ) |&gt;\n  cols_label( # display column names\n    year = \"Year\",\n    candidate = \"Candidate\",\n    party_simplified = \"Party\",\n    prop_ecv = \"Electoral Votes\",\n    winner = \"Winning Candidate\"\n  )\n\n\n\n\n\n\n\n\nNation-Wide Proportional\n\n\nYear\nCandidate\nElectoral Votes\nWinning Candidate\nParty\n\n\n\n\n1976\nCARTER, JIMMY\n267\nYes\nDEMOCRAT\n\n\n1976\nFORD, GERALD\n255\nNo\nREPUBLICAN\n\n\n1976\nMCCARTHY, EUGENE \"\"GENE\"\"\n4\nNo\nOTHER\n\n\n1976\nFORD, GERALD\n2\nNo\nOTHER\n\n\n1976\nANDERSON, THOMAS J.\n1\nNo\nOTHER\n\n\n1976\nCAMEJO, PETER\n1\nNo\nOTHER\n\n\n1976\nCARTER, JIMMY\n1\nNo\nOTHER\n\n\n1976\nMACBRIDE, ROGER\n1\nNo\nLIBERTARIAN\n\n\n1976\nMADDOX, LESTER\n1\nNo\nOTHER\n\n\n1976\nOTHER\n1\nNo\nOTHER\n\n\n1980\nREAGAN, RONALD\n270\nYes\nREPUBLICAN\n\n\n1980\nCARTER, JIMMY\n219\nNo\nDEMOCRAT\n\n\n1980\nANDERSON, JOHN B.\n35\nNo\nOTHER\n\n\n1980\nCLARK, EDWARD \"\"ED\"\"\n5\nNo\nLIBERTARIAN\n\n\n1980\nREAGAN, RONALD\n2\nNo\nOTHER\n\n\n1980\nCOMMONER, BARRY\n1\nNo\nOTHER\n\n\n1984\nREAGAN, RONALD\n313\nYes\nREPUBLICAN\n\n\n1984\nMONDALE, WALTER\n216\nNo\nDEMOCRAT\n\n\n1984\nREAGAN, RONALD\n2\nNo\nOTHER\n\n\n1984\nBERGLAND, DAVID\n1\nNo\nLIBERTARIAN\n\n\n1984\nMONDALE, WALTER\n1\nNo\nOTHER\n\n\n1988\nBUSH, GEORGE H.W.\n284\nYes\nREPUBLICAN\n\n\n1988\nDUKAKIS, MICHAEL\n244\nNo\nDEMOCRAT\n\n\n1988\nPAUL, RONALD \"\"RON\"\"\n2\nNo\nLIBERTARIAN\n\n\n1988\nBUSH, GEORGE H.W.\n1\nNo\nOTHER\n\n\n1988\nDUKAKIS, MICHAEL\n1\nNo\nOTHER\n\n\n1988\nFULANI, LENORA\n1\nNo\nOTHER\n\n\n1992\nCLINTON, BILL\n229\nYes\nDEMOCRAT\n\n\n1992\nBUSH, GEORGE H.W.\n198\nNo\nREPUBLICAN\n\n\n1992\nPEROT, ROSS\n101\nNo\nOTHER\n\n\n1992\nBUSH, GEORGE H.W.\n2\nNo\nOTHER\n\n\n1992\nBLANK VOTE/SCATTERING\n1\nNo\nOTHER\n\n\n1992\nMARROU, ANDRE\n1\nNo\nLIBERTARIAN\n\n\n1996\nCLINTON, BILL\n263\nYes\nDEMOCRAT\n\n\n1996\nDOLE, ROBERT\n216\nNo\nREPUBLICAN\n\n\n1996\nPEROT, ROSS\n42\nNo\nOTHER\n\n\n1996\nBROWNE, HARRY\n3\nNo\nLIBERTARIAN\n\n\n1996\nNADER, RALPH\n3\nNo\nOTHER\n\n\n1996\nBLANK VOTE/SCATTERING\n1\nNo\nOTHER\n\n\n1996\nCLINTON, BILL\n1\nNo\nOTHER\n\n\n1996\nDOLE, ROBERT\n1\nNo\nOTHER\n\n\n1996\nHAGELIN, JOHN\n1\nNo\nOTHER\n\n\n1996\nPHILLIPS, HOWARD\n1\nNo\nOTHER\n\n\n2000\nGORE, AL\n258\nYes\nDEMOCRAT\n\n\n2000\nBUSH, GEORGE W.\n255\nNo\nREPUBLICAN\n\n\n2000\nNADER, RALPH\n13\nNo\nOTHER\n\n\n2000\nBROWNE, HARRY\n2\nNo\nLIBERTARIAN\n\n\n2000\nBUCHANAN, PATRICK \"\"PAT\"\"\n2\nNo\nOTHER\n\n\n2000\nBLANK VOTE/SCATTERING\n1\nNo\nOTHER\n\n\n2000\nBUSH, GEORGE W.\n1\nNo\nOTHER\n\n\n2000\nGORE, AL\n1\nNo\nOTHER\n\n\n2000\nNOT DESIGNATED\n1\nNo\nOTHER\n\n\n2004\nBUSH, GEORGE W.\n271\nYes\nREPUBLICAN\n\n\n2004\nKERRY, JOHN\n258\nNo\nDEMOCRAT\n\n\n2004\nBADNARIK, MICHAEL\n2\nNo\nLIBERTARIAN\n\n\n2004\nNADER, RALPH\n2\nNo\nOTHER\n\n\n2004\nBUSH, GEORGE W.\n1\nNo\nOTHER\n\n\n2004\nCOBB, DAVID\n1\nNo\nOTHER\n\n\n2004\nKERRY, JOHN\n1\nNo\nOTHER\n\n\n2004\nOTHER\n1\nNo\nOTHER\n\n\n2004\nPEROUTKA, MICHAEL\n1\nNo\nOTHER\n\n\n2008\nOBAMA, BARACK H.\n282\nYes\nDEMOCRAT\n\n\n2008\nMCCAIN, JOHN\n243\nNo\nREPUBLICAN\n\n\n2008\nNADER, RALPH\n3\nNo\nOTHER\n\n\n2008\nBARR, BOB\n2\nNo\nLIBERTARIAN\n\n\n2008\nBALDWIN, CHARLES \"\"CHUCK\"\"\n1\nNo\nOTHER\n\n\n2008\nMCCAIN, JOHN\n1\nNo\nOTHER\n\n\n2008\nMCKINNEY, CYNTHIA\n1\nNo\nOTHER\n\n\n2008\nOBAMA, BARACK H.\n1\nNo\nOTHER\n\n\n2012\nOBAMA, BARACK H.\n272\nYes\nDEMOCRAT\n\n\n2012\nROMNEY, MITT\n251\nNo\nREPUBLICAN\n\n\n2012\nJOHNSON, GARY\n5\nNo\nLIBERTARIAN\n\n\n2012\nSTEIN, JILL\n2\nNo\nOTHER\n\n\n2012\nOBAMA, BARACK H.\n1\nNo\nOTHER\n\n\n2012\nROMNEY, MITT\n1\nNo\nOTHER\n\n\n2016\nCLINTON, HILLARY\n257\nYes\nDEMOCRAT\n\n\n2016\nTRUMP, DONALD J.\n245\nNo\nREPUBLICAN\n\n\n2016\nJOHNSON, GARY\n16\nNo\nLIBERTARIAN\n\n\n2016\nSTEIN, JILL\n5\nNo\nOTHER\n\n\n2016\nMCMULLIN, EVAN\n2\nNo\nOTHER\n\n\n2016\nBLANK VOTE\n1\nNo\nOTHER\n\n\n2016\nCASTLE, DARRELL L.\n1\nNo\nOTHER\n\n\n2016\nCLINTON, HILLARY\n1\nNo\nOTHER\n\n\n2016\nOTHER\n1\nNo\nOTHER\n\n\n2016\nSCATTERING\n1\nNo\nOTHER\n\n\n2016\nTRUMP, DONALD J.\n1\nNo\nOTHER\n\n\n2020\nBIDEN, JOSEPH R. JR\n276\nYes\nDEMOCRAT\n\n\n2020\nTRUMP, DONALD J.\n252\nNo\nREPUBLICAN\n\n\n2020\nJORGENSEN, JO\n6\nNo\nLIBERTARIAN\n\n\n2020\nHAWKINS, HOWIE\n1\nNo\nOTHER"
  },
  {
    "objectID": "mp03.html#fact-check-example-the-2000-u.s.-presidential-election",
    "href": "mp03.html#fact-check-example-the-2000-u.s.-presidential-election",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Fact Check Example: The 2000 U.S. Presidential Election",
    "text": "Fact Check Example: The 2000 U.S. Presidential Election\nThe 2000 U.S. presidential election between George W. Bush (Republican) and Al Gore (Democrat) provides a compelling case study of the Electoral College’s impact. Bush won the presidency despite losing the popular vote by approximately 500,000 votes. This resulted in widespread criticism of the electoral system.\nState-Wide Winner-Take-All: Bush: 271 ECVs (Winner) Gore: 266 ECVs (Loser) In this system, Bush wins, as he narrowly wins key battleground states, including Florida, despite Gore’s national popular vote lead.\nDistrict-Wide Winner-Take-All + State-Wide At-Large Votes: Bush: 278 ECVs (Winner) Gore: 260 ECVs (Loser) This hybrid system gives Bush a slightly larger margin due to the district-level distribution of votes, which tends to favor Republicans in many of the congressional districts.\nState-Wide Proportional: Gore: 290 ECVs (Winner) Bush: 248 ECVs (Loser) This system allocates ECVs proportionally based on the percentage of the popular vote. Gore wins more ECVs because his share of the popular vote is larger nationwide, leading to a more direct representation of voter preferences.\nNational Proportional: Gore: 286 ECVs (Winner) Bush: 252 ECVs (Loser) In a national proportional system, Gore’s larger share of the national vote translates to a clear victory, highlighting the disparity between the Electoral College and the popular vote outcome. In the 2000 election, the State-Wide Winner-Take-All system favored George W. Bush, despite Al Gore winning the popular vote. The National Proportional system would have resulted in a Gore victory, aligning the ECVs more closely with the popular vote. This highlights how winner-take-all methods can distort the will of the majority.\n\n\nView Code\n# State-Wide Winner-Take-All\nstate_wide_winner_only &lt;- state_wide_winner_take_all |&gt;\n  group_by(year) |&gt;\n  mutate(winner_party = if_else(total_ecv == max(total_ecv), party_simplified, NA_character_),\n         winning_candidate = if_else(total_ecv == max(total_ecv), candidate, NA_character_)) |&gt;\n  ungroup() |&gt;\n  select(year, party_simplified, winning_candidate, winner_party, total_ecv) |&gt;\n  filter(!is.na(winning_candidate))\n\n\n# District-Wide Winner-Take-All + State-Wide At Large Votes\ndistrict_wide_winner_only &lt;- ecv_combined |&gt;\n  group_by(year) |&gt;\n  mutate(winner_party = if_else(total_ecv == max(total_ecv), party, NA_character_),\n         winning_candidate = if_else(total_ecv == max(total_ecv), candidate, NA_character_)) |&gt;\n  ungroup() |&gt;\n  select(year, winning_candidate, winner_party, total_ecv) |&gt;\n  filter(!is.na(winning_candidate))\n\n# State-Wide Proportional\nstate_prop_winner_only &lt;- state_wide_totals |&gt;\n  group_by(year) |&gt;\n  mutate(winner_party = if_else(total_ecv == max(total_ecv), party_simplified, NA_character_),\n         winning_candidate = if_else(total_ecv == max(total_ecv), candidate, NA_character_)) |&gt;\n  ungroup() |&gt;\n  select(year, winning_candidate, winner_party, total_ecv) |&gt;\n  filter(!is.na(winning_candidate))\n\n# Nation-Wide Proportional\nnation_prop_winner_only &lt;- nation_wide_summary |&gt;\n  group_by(year) |&gt;\n  mutate(winner_party = if_else(prop_ecv == max(prop_ecv), party_simplified, NA_character_),\n         winning_candidate = if_else(prop_ecv == max(prop_ecv), candidate, NA_character_)) |&gt;\n  ungroup() |&gt;\n  select(year, winning_candidate, winner_party, prop_ecv) |&gt;\n  filter(!is.na(winning_candidate))\n\n# Join the four datasets by 'year'\nwinners_comparison &lt;- state_wide_winner_only |&gt;\n  left_join(district_wide_winner_only, by = \"year\", suffix = c(\"_state_wide\", \"_district_wide\")) |&gt;\n  left_join(state_prop_winner_only, by = \"year\", suffix = c(\"\", \"_state_prop\")) |&gt;\n  left_join(nation_prop_winner_only, by = \"year\", suffix = c(\"\", \"_nation_prop\"))\n\n# Select relevant columns for displaying the result\nwinners_comparison &lt;- winners_comparison |&gt;\n  select(-party_simplified)\n\n# Display the results for 2000\nwinners_comparison |&gt; \n  filter(year == 2000) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Winning Candidates by ECV Allocation Method (2000)\"\n  ) |&gt;\n  cols_label( # Display column names\n    year = \"Year\",\n    winning_candidate_state_wide = \"Winning Candidate\",\n    winner_party_state_wide = \"Winner Party\",\n    total_ecv_state_wide = \"Total ECV\",\n    winning_candidate_district_wide = \"Winning Candidate\",\n    winner_party_district_wide = \"Winner Party\",\n    total_ecv_district_wide = \"Total ECV\",\n    winning_candidate = \"Winning Candidate\",\n    winner_party = \"Winner Party\",\n    total_ecv = \"Total ECV\",\n    winning_candidate_nation_prop = \"Winning Candidate\",\n    winner_party_nation_prop = \"Winner Party\",\n    prop_ecv = \"Total ECV\"\n  ) |&gt;\n  tab_spanner(\n    label = \"State-Wide Winner-Take-All\",\n    columns = c(winning_candidate_state_wide, winner_party_state_wide, total_ecv_state_wide)\n  ) |&gt;\n  tab_spanner(\n    label = \"District-Wide Winner-Take-All + State-Wide At Large Votes\",\n    columns = c(winning_candidate_district_wide , winner_party_district_wide, total_ecv_district_wide)\n  ) |&gt;\n  tab_spanner(\n    label = \"State-Wide Proportional\",\n    columns = c(winning_candidate, winner_party, total_ecv)\n  ) |&gt;\n  tab_spanner(\n    label = \"Nation-Wide Proportional\",\n    columns = c(winning_candidate_nation_prop, winner_party_nation_prop, prop_ecv)\n  ) |&gt;\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_column_labels(columns = 1:13)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWinning Candidates by ECV Allocation Method (2000)\n\n\nYear\n\nState-Wide Winner-Take-All\n\n\nDistrict-Wide Winner-Take-All + State-Wide At Large Votes\n\n\nState-Wide Proportional\n\n\nNation-Wide Proportional\n\n\n\nWinning Candidate\nWinner Party\nTotal ECV\nWinning Candidate\nWinner Party\nTotal ECV\nWinning Candidate\nWinner Party\nTotal ECV\nWinning Candidate\nWinner Party\nTotal ECV\n\n\n\n\n2000\nBUSH, GEORGE W.\nREPUBLICAN\n271\nBUSH, GEORGE W.\nREPUBLICAN\n290\nGORE, AL\nDEMOCRAT\n263\nGORE, AL\nDEMOCRAT\n258\n\n\n\n\n\n\n\nThe proportional allocation methods also tend to assign a small number of ECV to non Republican or Democrat parties such as Other or Libertarian as seen below. However, since the number of ECV across the nation are relatively low in comparison, I will be filtering them out in the second plot below.\n\n\nView Code\nwinners_2000 &lt;- bind_rows( # bind different allocation methods together to plot in a facet\n  state_wide_winner_take_all |&gt; filter(year == 2000) |&gt;\n    mutate(allocation_method = \"State-Wide Winner-Take-All\"),\n  ecv_combined |&gt; filter(year == 2000) |&gt;\n    mutate(allocation_method = \"District-Wide Winner-Take-All + State-Wide At Large Votes\") |&gt;\n    rename(party_simplified = party),\n  state_wide_totals |&gt; filter(year == 2000) |&gt;\n    mutate(allocation_method = \"State-Wide Proportional\"),\n  nation_wide_summary |&gt; filter(year == 2000) |&gt;\n    mutate(allocation_method = \"Nation-Wide Proportional\") |&gt;\n    rename(total_ecv = prop_ecv)\n)\n\nggplot(winners_2000, aes(x = party_simplified, y = total_ecv, fill = party_simplified)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  geom_text(\n    aes(label = total_ecv), # label with ECV\n    position = position_stack(vjust = 0.5), color = \"black\", size = 3  # \n  )+\n  facet_wrap(~allocation_method, scales = \"free_y\") +  # Facet by allocation method\n  labs(\n    title = \"Total ECV Votes by Allocation Method (2000)\",\n    x = \"Party\",\n    y = \"Total ECV Votes\",\n    fill = \"Party\"\n  ) +\n  theme_minimal() +\n  scale_fill_manual(\n    values = c(\"REPUBLICAN\" = \"red\", \"DEMOCRAT\" = \"blue\", \"LIBERTARIAN\" = \"beige\", \"OTHER\" = \"gray\")  \n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1), # Rotate party names for better visibility\n    legend.position = \"none\" # Hide legend since it's already represented by colors\n  )\n\n\n\n\n\n\n\n\n\n\n\nView Code\nwinners_2000_na &lt;- winners_2000 |&gt;   filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\"))\n\nggplot(winners_2000_na, aes(x = party_simplified, y = total_ecv, fill = party_simplified)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  geom_text(\n    aes(label = total_ecv), # label with ECV\n    position = position_stack(vjust = 0.5), color = \"black\", size = 3  # \n  ) +\n  facet_wrap(~allocation_method, scales = \"free_y\") +  # Facet by allocation method\n  labs(\n    title = \"Total ECV Votes by Allocation Method (2000)\",\n    x = \"Party\",\n    y = \"Total ECV Votes\",\n    fill = \"Party\"\n  ) +\n  theme_minimal() +\n  scale_fill_manual(\n    values = c(\"REPUBLICAN\" = \"red\", \"DEMOCRAT\" = \"blue\")  \n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1), # Rotate party names for better visibility\n    legend.position = \"none\" # Hide legend since it's already represented by colors\n  )\n\n\n\n\n\n\n\n\n\n\nWinner-Take-All systems: In this system, narrow victories in key swing states have an outsized impact, making it possible for a candidate to lose the popular vote but win the Electoral College.\nProportional systems: These systems provide a more accurate reflection of the national popular vote and reduce the disproportionate weight given to small states or swing states.\n\n\n\nView Code\n# Display the results for all years\nwinners_comparison |&gt; \n  gt() |&gt;\n  tab_header(\n    title = \"Comparison of Winning Candidates and Parties by ECV Allocation System\"\n  ) |&gt;\n  cols_label( # Display column names\n    year = \"Year\",\n    winning_candidate_state_wide = \"Winning Candidate\",\n    winner_party_state_wide = \"Winner Party\",\n    total_ecv_state_wide = \"Total ECV\",\n    winning_candidate_district_wide = \"Winning Candidate\",\n    winner_party_district_wide = \"Winner Party\",\n    total_ecv_district_wide = \"Total ECV\",\n    winning_candidate = \"Winning Candidate\",\n    winner_party = \"Winner Party\",\n    total_ecv = \"Total ECV\",\n    winning_candidate_nation_prop = \"Winning Candidate\",\n    winner_party_nation_prop = \"Winner Party\",\n    prop_ecv = \"Total ECV\"\n  ) |&gt;\n  tab_spanner(\n    label = \"State-Wide Winner-Take-All\",\n    columns = c(winning_candidate_state_wide, winner_party_state_wide, total_ecv_state_wide)\n  ) |&gt;\n  tab_spanner(\n    label = \"District-Wide Winner-Take-All + State-Wide At Large Votes\",\n    columns = c(winning_candidate_district_wide , winner_party_district_wide, total_ecv_district_wide)\n  ) |&gt;\n  tab_spanner(\n    label = \"State-Wide Proportional\",\n    columns = c(winning_candidate, winner_party, total_ecv)\n  ) |&gt;\n  tab_spanner(\n    label = \"Nation-Wide Proportional\",\n    columns = c(winning_candidate_nation_prop, winner_party_nation_prop, prop_ecv)\n  ) |&gt;\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_column_labels(columns = 1:13)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of Winning Candidates and Parties by ECV Allocation System\n\n\nYear\n\nState-Wide Winner-Take-All\n\n\nDistrict-Wide Winner-Take-All + State-Wide At Large Votes\n\n\nState-Wide Proportional\n\n\nNation-Wide Proportional\n\n\n\nWinning Candidate\nWinner Party\nTotal ECV\nWinning Candidate\nWinner Party\nTotal ECV\nWinning Candidate\nWinner Party\nTotal ECV\nWinning Candidate\nWinner Party\nTotal ECV\n\n\n\n\n1976\nCARTER, JIMMY\nDEMOCRAT\n294\nCARTER, JIMMY\nDEMOCRAT\n362\nCARTER, JIMMY\nDEMOCRAT\n270\nCARTER, JIMMY\nDEMOCRAT\n267\n\n\n1980\nREAGAN, RONALD\nREPUBLICAN\n448\nREAGAN, RONALD\nREPUBLICAN\n287\nREAGAN, RONALD\nREPUBLICAN\n281\nREAGAN, RONALD\nREPUBLICAN\n270\n\n\n1984\nREAGAN, RONALD\nREPUBLICAN\n525\nREAGAN, RONALD\nREPUBLICAN\n283\nREAGAN, RONALD\nREPUBLICAN\n321\nREAGAN, RONALD\nREPUBLICAN\n313\n\n\n1988\nBUSH, GEORGE H.W.\nREPUBLICAN\n426\nDUKAKIS, MICHAEL\nDEMOCRAT\n292\nBUSH, GEORGE H.W.\nREPUBLICAN\n291\nBUSH, GEORGE H.W.\nREPUBLICAN\n284\n\n\n1992\nCLINTON, BILL\nDEMOCRAT\n367\nCLINTON, BILL\nDEMOCRAT\n329\nCLINTON, BILL\nDEMOCRAT\n226\nCLINTON, BILL\nDEMOCRAT\n229\n\n\n1996\nCLINTON, BILL\nDEMOCRAT\n376\nDOLE, ROBERT\nREPUBLICAN\n283\nCLINTON, BILL\nDEMOCRAT\n262\nCLINTON, BILL\nDEMOCRAT\n263\n\n\n2000\nBUSH, GEORGE W.\nREPUBLICAN\n271\nBUSH, GEORGE W.\nREPUBLICAN\n290\nGORE, AL\nDEMOCRAT\n263\nGORE, AL\nDEMOCRAT\n258\n\n\n2004\nBUSH, GEORGE W.\nREPUBLICAN\n286\nBUSH, GEORGE W.\nREPUBLICAN\n299\nBUSH, GEORGE W.\nREPUBLICAN\n278\nBUSH, GEORGE W.\nREPUBLICAN\n271\n\n\n2008\nOBAMA, BARACK H.\nDEMOCRAT\n361\nOBAMA, BARACK H.\nDEMOCRAT\n330\nOBAMA, BARACK H.\nDEMOCRAT\n285\nOBAMA, BARACK H.\nDEMOCRAT\n282\n\n\n2012\nOBAMA, BARACK H.\nDEMOCRAT\n329\nROMNEY, MITT\nREPUBLICAN\n286\nOBAMA, BARACK H.\nDEMOCRAT\n271\nOBAMA, BARACK H.\nDEMOCRAT\n272\n\n\n2016\nTRUMP, DONALD J.\nREPUBLICAN\n305\nTRUMP, DONALD J.\nREPUBLICAN\n313\nCLINTON, HILLARY\nDEMOCRAT\n265\nCLINTON, HILLARY\nDEMOCRAT\n257\n\n\n2020\nBIDEN, JOSEPH R. JR\nDEMOCRAT\n306\nBIDEN, JOSEPH R. JR\nDEMOCRAT\n275\nBIDEN, JOSEPH R. JR\nDEMOCRAT\n273\nBIDEN, JOSEPH R. JR\nDEMOCRAT\n276\n\n\n\n\n\n\n\n\n\nView Code\nwinners_all &lt;- bind_rows( # bind different allocation methods together to plot in a facet\n  state_wide_winner_only |&gt; \n    mutate(allocation_method = \"State-Wide Winner-Take-All\"),\n  district_wide_winner_only |&gt; \n    mutate(allocation_method = \"District-Wide Winner-Take-All + State-Wide At Large Votes\"),\n  state_prop_winner_only |&gt;\n    mutate(allocation_method = \"State-Wide Proportional\"),\n  nation_prop_winner_only |&gt;\n    mutate(allocation_method = \"Nation-Wide Proportional\") |&gt;\n    rename(total_ecv = prop_ecv)\n)\nggplot(winners_all, aes(x = year, y = total_ecv, fill = winner_party)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(\n    aes(label = total_ecv, y = total_ecv + 20), # label with ECV and offset labels above bars\n    position = position_dodge(width = 0.8), color = \"black\", size = 3  # \n  ) +\n  facet_wrap(~allocation_method, scales = \"free_y\") + # Facet by method, each year will have its own plot\n  scale_fill_manual(\n    values = c(\"REPUBLICAN\" = \"red\", \"DEMOCRAT\" = \"blue\") )+\n  labs(\n    title = \"Electoral College Votes by Method and Year\",\n    x = \"Year\",\n    y = \"Total ECV\",\n    fill = \"Party\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\nView Code\nlibrary(gganimate)\n\nggplot(winners_all, aes(x = year, y = total_ecv, fill = winner_party)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +  # Create bar plot with dodged bars for each party\n  geom_text(\n    aes(label = total_ecv, y = total_ecv + 20),  # Label ECV values with an offset to appear above bars\n    position = position_dodge(width = 0.8), color = \"black\", size = 3  # Text properties\n  ) +\n  scale_fill_manual(\n    values = c(\"REPUBLICAN\" = \"red\", \"DEMOCRAT\" = \"blue\")  # Party colors\n  ) +\n  labs(\n    title = \"Electoral College Votes by Method and Year: {current_frame}\",\n    x = \"Year\",\n    y = \"Total ECV\",\n    fill = \"Party\"\n  ) +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1),  # Rotate x-axis labels for better readability\n    legend.position = \"bottom\")  +  # Position the legend at the bottom \n  transition_manual(allocation_method) + # Animate by allocation_method\n  enter_fade() +  # Fade in new bars for the new allocation method\n  exit_fade()\n\n\n\n\n\n\n\n\n\nThe State-Wide Proportional and National Proportional allocation systems appear to be the fairest in terms of reflecting the popular vote. These systems ensure that each vote contributes to the outcome, reducing the disproportionate influence of small states and the swing state effect. On the other hand, the State-Wide Winner-Take-All and District-Wide Winner-Take-All systems can create a bias toward smaller states and swing states. Smaller states can disproportionately impact the outcome of the election."
  },
  {
    "objectID": "mp03.html#what-does-fairness-mean",
    "href": "mp03.html#what-does-fairness-mean",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "What Does “Fairness” Mean?",
    "text": "What Does “Fairness” Mean?\nWhen I think about fairness in the context of ECV allocation, it generally relates to how well the system:\n\nRepresents the popular vote: Does the distribution of ECVs accurately reflect the number of votes cast for each candidate? A system that over-represents or under-represents certain groups could be considered unfair.\nAccounts for each state: In systems like the current winner-take-all method, small states with fewer voters might have a disproportionately large influence in electing a president compared to larger states. In contrast, a proportional system might mitigate this imbalance.\nMinimizes “winner-take-all” advantages: A system where a candidate wins by just a small margin but takes all of a state’s ECVs could be seen as unfair because the losing candidate might have had broad support across the state, but doesn’t get any representation.\n\n\nTruthfulness Score\nClaim: claim under evaluation is: “The Electoral College system is biased and over-represents smaller states, giving them an unfair advantage in electing the president.” The scale I’ll use is a 5-point scale, ranging from 1 to 5, where:\n\nFalse – The claim is completely inaccurate, with no evidence to support it.\nMostly False – The claim contains significant inaccuracies or overgeneralizations, with some misleading aspects.\nHalf-True – The claim is partially accurate but misses key details or context that would provide a more complete picture.\nMostly True – The claim is largely accurate, with very minor misstatements or nuances that don’t change the overall truth.\nTrue – The claim is completely accurate, with no significant errors or misleading information.\n\nScore: 4 Under the State-Wide Winner-Take-All system, smaller states do indeed have a disproportionately large impact because of the fixed 2 Senate seats each state gets, regardless of population. This benefits candidates who win a disproportionate share of votes in less populous states.\nPew Research Center In September of 2024, the Pew published that the article: Majority of Americans Continue to Favor Moving Away from Electoral College. Following the 2000 and 2016 elections, where the winners of the popular vote received fewer Electoral College votes than their opponents, the Pew surveyed we surveyed 9,720 U.S. adults."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cindy Li",
    "section": "",
    "text": "Hi! I’m Cindy. Welcome to my page of projects!\nI am a Data Analyst based in NYC currently pursuing my Master’s in Business Analytics with a concentration in Data Analysis.\n\n\nA little about me…\n\nPreviously a Business Analyst at Bank of America\nBoston College Alumna (majored in Management Information Systems and Business Analytics, with a minor in Computer Science)\nLived in 3 countries\nSwiftie! Taylor Swift followed me on tumblr.\nI love to sew and am passionate about sustainable fashion and how technology can reduce the environmental/social impacts of the fashion industry.\n\n\n\nCurrently learning:\n\nR (Software Tools for Data Analysis)\nPython (Programming for Analytics)\nSQL (Principles of Database Management Systems)\nSalsa & bachata\n\n\nLet’s Connect!"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "",
    "text": "As a CUNY employee, you are presented with two retirement plan options: Teachers’ Retirement System (TRS) and Optional Retirement Plan (ORP). Both plans have distinct characteristics, and the best option depends on various factors, including your personal financial goals, investment preferences, and risk tolerance. Below, I will walk you through the key considerations for each plan and how they might work for you so that you can enjoy your retirement in the future."
  },
  {
    "objectID": "mp04.html#teachers-retirement-system-trs-overview",
    "href": "mp04.html#teachers-retirement-system-trs-overview",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "1. Teachers’ Retirement System (TRS) Overview",
    "text": "1. Teachers’ Retirement System (TRS) Overview\nThe TRS is a defined-benefit plan, meaning your retirement benefit is based on a formula related to your salary and years of service, rather than the market performance of your investments. The key features are:\n\nFixed Benefit: Your pension is a fixed amount that you receive monthly after retirement, based on your years of service and the final average salary (FAS).\nEmployer Takes the Risk: The CUNY system, as your employer, is responsible for ensuring that your pension is paid out, regardless of how the financial markets perform.\nInflation Adjustments: Your pension is typically adjusted for inflation every year, ensuring your purchasing power remains somewhat protected over time."
  },
  {
    "objectID": "mp04.html#optional-retirement-plan-orp-overview",
    "href": "mp04.html#optional-retirement-plan-orp-overview",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "2. Optional Retirement Plan (ORP) Overview",
    "text": "2. Optional Retirement Plan (ORP) Overview\nThe ORP is a defined-contribution plan, similar to a 401(k). In this plan, both you and CUNY make contributions to your individual retirement account, and the amount you have at retirement depends on several factors. Your ORP retirement benefit depends on:\n\nEmployee and Employer Contributions: These contributions are made throughout your career, generally a fixed percentage of your salary.\nInvestment Returns: The money is invested in mutual funds, and your retirement account grows based on the returns of those investments. Your return will depend on both the contributions and the market returns over your career. Given that the stock market can be volatile, your final portfolio value may fluctuate.\n\nRight off the bat, based on the information above, we can compare the two plans on a theoretical level.\n\nPredictability & Stability\n\nTRS: Offers a fixed, predictable monthly income that is guaranteed by CUNY. You don’t have to worry about market volatility or having to manage your own investments. This can be comforting if you prefer stability in retirement.\nORP: The benefit is not guaranteed. It depends on market performance, meaning that if markets perform well, your retirement savings will grow, but if they perform poorly, you may face lower than expected retirement income.\n\n\n\nInflation Protection\n\nTRS: Your pension will likely receive annual inflation adjustments, though they are capped. This means your income will keep up with inflation to some degree.\nORP: No built-in inflation protection. Your portfolio must grow at a rate that outpaces inflation, which may be challenging in a low-interest-rate environment.\n\n\n\nRisk Tolerance\n\nTRS: Low risk. The risk of market performance is carried by CUNY, not you.\nORP: Higher risk. Since it is based on market performance, your retirement income will be more volatile and dependent on your asset allocation and the financial markets.\n\n\n\nControl and Flexibility\n\nTRS: You have no control over your pension plan. Your monthly benefit is determined by a set formula.\nORP: You have full control over how your money is invested, which can lead to higher returns if you make the right choices. However, this also means you need to be actively involved in managing your retirement account.\n\n\n\nRetirement Income\n\nTRS: Provides a steady, predictable income for life, which can be helpful for budgeting in retirement.\nORP: Your retirement income is variable and can increase or decrease depending on the performance of your investments."
  },
  {
    "objectID": "mp04.html#calculation-of-trs",
    "href": "mp04.html#calculation-of-trs",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "Calculation of TRS",
    "text": "Calculation of TRS\nTRS: traditional pension plan: after retirement, the employer pays employees a fraction of their salary until death. aka “defined-benefit plan”\n\nbenefit is fixed - employer takes the market risk\nIf the market underperforms expectations, CUNY has to “pony up” and make up the gap - If the market overperforms expectations, CUNY pockets the excess balance.\nYour pension will not be based on the contributions you paid into the system.3\n\nCalculate Wage Growth\n\n\nCode\nWAGE &lt;- WAGE |&gt; mutate(wage_growth_percent = (wage - lag(wage)) / lag(wage)) \ninitial_salary &lt;- 50000  # Hypothetical starting salary\nstart_date &lt;- \"2004-10-01\"\nend_date &lt;- \"2024-10-01\" # 20 years\nwage_20yrs &lt;- WAGE |&gt; \n  filter(date &gt;= as.Date(start_date), !is.na(date))\n\nsalary_data &lt;- wage_20yrs |&gt;\n  mutate(\n    salary = initial_salary * cumprod(1 + lag(wage_growth_percent, default = 0))\n  )\n\n\n\n\nCode\n# Visualizing salary growth over the next 20 years\nggplot(salary_data, aes(x = date, y = salary)) +\n  geom_line(color = \"darkgreen\", size = 1) +\n  labs(\n    title = \"Salary Growth Over 20 Years\",\n    x = \"Year\",\n    y = \"Salary ($)\",\n    subtitle = paste(\"Starting Salary:\", initial_salary, \"with 20 Years of Wage Growth\")\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nContribution Based on Last 3 Years of Salary The TRS pension benefit is based on your final average salary (FAS), which is determined by the average salary over the last three years of employment.\n\n\nCode\nsalary_3yrs &lt;- salary_data |&gt; filter(date &gt;= \"2021-10-01\")\nyears_of_service = 20\nfinal_average_salary = mean(salary_3yrs$salary)\nretirement_benefit &lt;- function(n, fas) {\n  if (n &lt; 20) return (0.0167 * fas * n)\n  else if (n == 20) return(0.0175 * fas * n)\n  else return((0.35 + 0.02 * (n - 20)) * fas)\n}\n\ntrs_benefit = retirement_benefit(years_of_service, final_average_salary)\ncat(\"TRS Benefit: \", trs_benefit, \"\\n\")\n\n\nTRS Benefit:  31645.28 \n\n\nTRS First Month Retirement Benefit The first month retirement benefit for TRS is calculated by dividing the total annual pension by 12 months.\n\n\nCode\n# Monthly TRS Benefit (assuming TRS benefit is paid monthly)\ntrs_monthly_benefit &lt;- trs_benefit / 12\ncat(\"First Month Benefit: \", trs_monthly_benefit, \"\\n\")\n\n\nFirst Month Benefit:  2637.107"
  },
  {
    "objectID": "mp04.html#calculation-of-orp",
    "href": "mp04.html#calculation-of-orp",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "Calculation of ORP",
    "text": "Calculation of ORP\nORP: closer to a 401k plan, employee and employer make contribution to retirement account that is invested in mutual funds. aka “defined-contribution plan” - only contribution is fixed - depends on market performance\nContribution Rate The employee’s contribution is based on their salary, and the employer contributes a fixed percentage based on years of service. We will be following the Fidelity Freedom Fund distribution of funds for your contributions. The allocation of funds between equities (both domestic and international), bonds, and short-term debt changes as you age.\n\n\nCode\norp_contribution_rate &lt;- function(salary) {\n  if (salary &lt;= 45000) return(0.03)\n  else if (salary &lt;= 55000) return(0.035)\n  else if (salary &lt;= 75000) return(0.045)\n  else if (salary &lt;= 100000) return(0.0575)\n  else return(0.06)\n}\nemployee_contrib_rate &lt;- function(salary) {\n  return(salary * orp_contribution_rate(salary))\n}\n\n# US Equities\nus_equities_rate &lt;- function(age) {\n  if (age &gt;= 25 && age &lt;= 49) {\n    return(0.54)\n  } else if (age &gt;= 50 && age &lt;= 59) {\n    return(0.47)\n  } else if (age &gt;= 60 && age &lt;= 74) {\n    return(0.34)\n  } else if (age &gt;= 75) {\n    return(0.19)\n  } else {\n    return(NA)\n  }\n}\n\n# International Equities\ninternational_equities_rate &lt;- function(age) {\n  if (age &gt;= 25 && age &lt;= 49) {\n    return(0.36)\n  } else if (age &gt;= 50 && age &lt;= 59) {\n    return(0.32)\n  } else if (age &gt;= 60 && age &lt;= 74) {\n    return(0.23)\n  } else if (age &gt;= 75) {\n    return(0.13)\n  } else {\n    return(NA)\n  }\n}\n\n# Bonds\nbonds_rate &lt;- function(age) {\n  if (age &gt;= 25 && age &lt;= 49) {\n    return(0.10)\n  } else if (age &gt;= 50 && age &lt;= 59) {\n    return(0.21)\n  } else if (age &gt;= 60 && age &lt;= 74) {\n    return(0.43)\n  } else if (age &gt;= 75) {\n    return(0.62)\n  } else {\n    return(NA_real_) \n  }\n}\n\n# Short Term Debt\nshort_term_debt_rate &lt;- function(age) {\n  if (age &gt;= 75) {\n    return(0.06)  # Short-term debt only applies to age 75+\n  } else {\n    return(0.00)  # No short-term debt allocation for ages below 75\n  }\n}\n\n# Fixed Employer Contribution at 7 years\nemployer_contrib_rate &lt;- function(years) {\n  if (years &lt;= 7) return(0.08)\n  else return(0.10)\n}\n\n\nCalculate for ORP This function simulates the growth of your ORP account based on contributions, salary increases, and asset allocation.\n\n\nCode\ncalculate_portfolio_value &lt;- function(start_salary, years_worked, retirement_age, current_age) {\n  total_contrib &lt;- 0\n  portfolio_value &lt;- 0\n  current_salary &lt;- start_salary\n  year &lt;- 1\n  portfolio_values &lt;- data.frame(Year = numeric(0), Portfolio_Value = numeric(0))\n  \n  # Simulate annual contributions and growth until retirement\n  for (age in current_age:(retirement_age - 1)) {\n    employee_contrib &lt;- employee_contrib_rate(current_salary)\n    employer_contrib &lt;- current_salary*employer_contrib_rate(year)\n    year &lt;- year + 1\n    total_contrib &lt;- employee_contrib + employer_contrib\n    portfolio_value &lt;- portfolio_value + total_contrib\n    # Asset allocation based on age\n    us_equities_pct &lt;- us_equities_rate(age)\n    international_pct &lt;- international_equities_rate(age)\n    bonds_pct &lt;- bonds_rate(age)\n    short_term_pct &lt;- short_term_debt_rate(age)\n    # convert to average annual returns\n    us_equities_return &lt;- (1 + long_run_avg_2004$equity_return_SPY_avg)^12 - 1\n    international_return &lt;- (1 + long_run_avg_2004$equity_return_ACWI_avg)^12 - 1\n    bonds_return &lt;- (1 + long_run_avg_2004$bond_return_avg / 100)^12 - 1\n    short_term_return &lt;- (1 + long_run_avg_2004$debt_return_avg / 100)^12 - 1\n\n    # Calculate portfolio growth based on allocation\n    portfolio_value &lt;- portfolio_value * (1 + us_equities_pct * us_equities_return +\n                                            international_pct * international_return +\n                                            bonds_pct * bonds_return +\n                                            short_term_pct * short_term_return)\n    \n    # Simulate salary increase\n    current_salary &lt;- current_salary * (1 + long_run_averages$wage_growth_avg + long_run_averages$inflation_avg)\n    portfolio_values &lt;- rbind(portfolio_values, data.frame(Year = age, Portfolio_Value = portfolio_value))\n  }\n  return(portfolio_values)\n}\n\nstart_age = 40\nretirement_age = 65\nportfolio_value_at_retirement &lt;- calculate_portfolio_value(initial_salary, years_of_service, retirement_age, start_age)\n\n\n\n\nCode\n# Monthly TRS Benefit (assuming TRS benefit is paid monthly)\nggplot(portfolio_value_at_retirement, aes(x = Year, y = Portfolio_Value)) +\n  geom_line(color = \"blue\", size = 1) +\n  labs(\n    title = \"Portfolio Value Growth Over Time\",\n    x = \"Age\",\n    y = \"Portfolio Value ($)\",\n    subtitle = paste(\"Initial Salary: $\", initial_salary, \"Years of Service: \", years_of_service)\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::comma)"
  },
  {
    "objectID": "mp04.html#comparison-of-trs-and-orp-first-month-retirement-benefits",
    "href": "mp04.html#comparison-of-trs-and-orp-first-month-retirement-benefits",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "Comparison of TRS and ORP First Month Retirement Benefits",
    "text": "Comparison of TRS and ORP First Month Retirement Benefits\nHere we compare the first month’s retirement benefit from both plans. TRS First Month Retirement Benefit The first month retirement benefit for TRS is calculated by dividing the total annual pension by 12 months. ORP First Month Retirement Benefit After retirement, the ORP monthly benefit can be calculated based on a 4% withdrawal rate, which is typical for sustainable withdrawals.\n\n\nCode\n# 2. **ORP: Calculate Portfolio Value at Retirement**\n# the first month’s payout is typically a 4% withdrawal rate\n# Sustainable withdrawal rate (e.g., 4% annually, which is 0.33% monthly)\nfinal_portfolio_value &lt;- portfolio_value_at_retirement$Portfolio_Value[nrow(portfolio_value_at_retirement)]\norp_monthly_withdrawal &lt;- final_portfolio_value * 0.04 / 12\n\ncomparison_table_gt &lt;- tibble(\n  Plan = c(\"TRS\", \"ORP\"),\n  First_Month_Retirement_Value = c(trs_monthly_benefit, orp_monthly_withdrawal)\n) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Comparison of TRS and ORP for First Month of Retirement\"\n  ) |&gt;\n  cols_label(\n    Plan = \"Retirement Plan\",\n    First_Month_Retirement_Value = \"First_Month_Retirement_Value\"\n  ) |&gt;\n  fmt_currency(\n    columns = vars(First_Month_Retirement_Value),\n    currency = \"USD\",\n    use_seps = TRUE,\n    decimals = 0\n  )\ncomparison_table_gt\n\n\n\n\n\n\n\n\nComparison of TRS and ORP for First Month of Retirement\n\n\nRetirement Plan\nFirst_Month_Retirement_Value\n\n\n\n\nTRS\n$2,637\n\n\nORP\n$2,430"
  },
  {
    "objectID": "mp04.html#trs-withdrawals-after-retirement",
    "href": "mp04.html#trs-withdrawals-after-retirement",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "TRS Withdrawals After Retirement",
    "text": "TRS Withdrawals After Retirement\nThe TRS pension offers a steady, inflation-adjusted income during retirement. This income increases each year based on inflation, but the increase is capped at 3% annually. The adjustment is effective every September, using the Consumer Price Index (CPI) of the previous 12 months to calculate the inflation rate. Inflation Adjustement To simulate the TRS benefit after retirement, we adjust the pension payments based on inflation. The adjustment is capped at 3% annually. The inflation adjustement is effective each September and the CPI used is the aggregate monthly CPI of the previous 12 months.\n\n\nCode\n# Inflation adjustment function\ninflation_adjustment &lt;- function(cpi) {\n  adjustment &lt;- ceiling(cpi * 0.5) / 100\n  if (adjustment &lt; 0.01) return(0.01)  \n  if (adjustment &gt; 0.03) return(0.03) \n  return(adjustment)\n}\n\n\nThe following code simulates the pension payments over a 20-year retirement period. The pension increases annually based on the average inflation rate of the previous 12 months, adjusted according to the rules described above. The result provides insight into how TRS can help preserve purchasing power over the long term.\n\n\nCode\n# TRS\n# Set the number of retirement years\n\n# TRS Monthly Benefit Calculation\n# Function to calculate inflation adjustment based on CPI\ninflation_adjustment &lt;- function(CPI) {\n  adjustment &lt;- round(CPI * 0.5, 1)  # 50% of CPI increase, rounded to nearest 0.1%\n  if (adjustment &lt; 0.01) adjustment &lt;- 0.01  # Floor at 1%\n  if (adjustment &gt; 0.03) adjustment &lt;- 0.03  # Cap at 3%\n  return(adjustment)\n}\n\n# Simulating pension benefit with inflation adjustments\nsimulate_trs_with_inflation &lt;- function(initial_pension, inflation_data, retirement_date, end_date) {\n  retirement_benefit &lt;- numeric(total_years * 12) \n  retirement_benefit[1] &lt;- initial_pension \n  current_pension &lt;- initial_pension\n  for (current_month in 2:length(retirement_benefit)) {\n    current_date &lt;- seq(retirement_date, length.out = length(retirement_benefit), by = \"month\")[current_month]\n    # The inflation adjustment is effective each September\n    if (format(current_date, \"%m\") == \"09\") {\n      inflation_data_recent &lt;- inflation_data[inflation_data$date &lt; current_date,]\n      last_12_months_inflation &lt;- tail(inflation_data_recent$inflation_rate, 12)\n      avg_inflation &lt;- mean(last_12_months_inflation)\n      adjustment &lt;- inflation_adjustment(avg_inflation)\n      current_pension &lt;- current_pension * (1 + adjustment)\n    }\n    retirement_benefit[current_month] &lt;- current_pension\n  }\n  return(retirement_benefit)\n}\n\n# using past infation rates due to data availability\n# TRS benefit is assumed to be a fixed percentage of the employee's final salary\nmonthly_trs_pension &lt;- trs_benefit / 12 \nretirement_date &lt;- as.Date(\"2020-10-01\")\nend_date &lt;- as.Date(\"2024-10-01\")\ntotal_years &lt;- 20  \ntrs_income_stream &lt;- simulate_trs_with_inflation(monthly_trs_pension, INFLATION, retirement_date, end_date)\n\n# Print the total amount received over 20 years\ntotal_received &lt;- sum(trs_income_stream)\ncat(\"Total amount received over 20 years: $\", round(total_received, 2), \"\\n\")\n\n\nTotal amount received over 20 years: $ 697378.2 \n\n\n\n\nCode\ntrs_income_df &lt;- data.frame(\n  Month = seq(retirement_date, by = \"month\", length.out = total_years*12),\n  Pension_Benefit = trs_income_stream\n)\n\ntrs_income_df$cumulative_received &lt;- cumsum(trs_income_df$Pension_Benefit)\n\nggplot(trs_income_df, aes(x = Month)) + \n  geom_line(aes(y = Pension_Benefit, color = \"Pension Benefit\"), size = 1) +\n  scale_y_continuous(\n    name = \"Pension Benefit ($)\") +\n  labs(title = \"TRS Monthly Benefit\",\n       x = \"Date\", \n       y = \"Monthly Pension Benefit ($)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "mp04.html#orp-withdrawals-after-retirement",
    "href": "mp04.html#orp-withdrawals-after-retirement",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "ORP Withdrawals After Retirement",
    "text": "ORP Withdrawals After Retirement\nWith the ORP, the employee builds a retirement portfolio over their working years and begins drawing down funds after retirement. The amount withdrawn each year is typically a percentage of the portfolio value, with a common recommendation being 4% annually. In addition to withdrawals, the remaining portfolio continues to grow during retirement at an assumed annual return of 3%. This growth helps maintain the portfolio balance and ensures that funds last throughout retirement. The following code simulates the withdrawals from the ORP portfolio over a 20-year retirement period. The initial portfolio value is based on the employee’s salary, years of service, and contributions. The code assumes an annual withdrawal of 4% and a 3% annual growth rate.\n\n\nCode\nwithdraw_after_retirement &lt;- function(portfolio_values, withdrawal_rate = 0.04, years_retired = 20) {\n  monthly_withdrawal_rate &lt;- withdrawal_rate / 12\n  monthly_growth_rate &lt;- 1.03^(1/12)\n  \n  portfolio_value_at_retirement &lt;- tail(portfolio_values$Portfolio_Value, 1)\n  remaining_balance &lt;- portfolio_value_at_retirement\n  withdrawals &lt;- numeric(years_retired)\n  portfolio_balance &lt;- numeric(years_retired)\n  \n  for (year in 1:years_retired) {\n    withdrawals[year] &lt;- remaining_balance * withdrawal_rate\n    remaining_balance &lt;- remaining_balance - withdrawals[year]\n    remaining_balance &lt;- remaining_balance * 1.03\n\n    portfolio_balance[year] &lt;- remaining_balance\n    if (remaining_balance &lt; 0) {\n      withdrawals[year] &lt;- withdrawals[year] + remaining_balance\n      break\n    }\n  }\n  return(data.frame(Year = 1:years_retired, Withdrawals = withdrawals, Portfolio_Balance = portfolio_balance))\n}\n\n\n\n\nCode\nstart_age = 40\nretirement_age = 65\nlife_expectancy = 85\nportfolio_value_at_retirement &lt;- calculate_portfolio_value(initial_salary, years_of_service, retirement_age, start_age)\nwithdrawals_data &lt;- withdraw_after_retirement(portfolio_value_at_retirement)\n\nggplot(withdrawals_data, aes(x = Year)) + \n  geom_line(aes(y = Withdrawals, color = \"Annual Withdrawals\"), size = 1) +\n  scale_y_continuous(\n    name = \"Amount ($)\",\n    sec.axis = sec_axis(~ ., name = \"Portfolio Balance ($)\")\n  ) +\n  labs(title = \"Annual ORP Withdrawals Over Retirement\",\n       x = \"Years After Retirement\",\n       y = \"Annual Withdrawals ($)\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Annual Withdrawals\" = \"blue\", \"Portfolio Balance\" = \"red\"))\n\n\n\n\n\n\n\n\n\nCode\n# Print results\ncat(\"Portfolio value at retirement: $\", round(tail(portfolio_value_at_retirement$Portfolio_Value, 1), 2), \"\\n\")\n\n\nPortfolio value at retirement: $ 728872.6 \n\n\nCode\ncat(\"Total withdrawals over retirement: $\", round(sum(withdrawals_data$Withdrawals), 2), \"\\n\")\n\n\nTotal withdrawals over retirement: $ 525034.1"
  },
  {
    "objectID": "mp04.html#comparison-of-withdrawals-after-retirement",
    "href": "mp04.html#comparison-of-withdrawals-after-retirement",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "Comparison of Withdrawals After Retirement",
    "text": "Comparison of Withdrawals After Retirement\nSince ORP is withdrawn annually, to compare it with the monthly TRS withdrawals, below, I am displaying ORP withdrawals on a month to month basis. Monthly Income/Withhdrawal (TRS vs ORP)\n\n\nCode\nyears_retired = 20\nwithdrawals_data_monthly &lt;- data.frame(\nMonth = seq.Date(from = as.Date(\"2020-01-01\"), \n                   by = \"month\", \n                   length.out = years_retired * 12),\n  Withdrawals = rep(withdrawals_data$Withdrawals / 12, each = 12), \n  Portfolio_Balance = rep(withdrawals_data$Portfolio_Balance, each = 12) \n)\ncombined_df &lt;- merge(withdrawals_data_monthly, trs_income_df, by = \"Month\")\nggplot(combined_df, aes(x = Month)) + \n  geom_line(aes(y = Withdrawals, color = \"Monthly Withdrawals\"), size = 1) +\n  geom_line(aes(y = Pension_Benefit, color = \"Monthly Pension Benefit\"), size = 1) +\n  scale_y_continuous(\n    name = \"Amount ($)\") +\n  labs(title = \"Monthly ORP Withdrawals vs TRS Pension Benefit\",\n       x = \"Months After Retirement\",\n       y = \"Monthly Withdrawals / Pension Benefit ($)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_color_manual(values = c(\"Monthly Withdrawals\" = \"blue\", \n                                \"Monthly Pension Benefit\" = \"brown\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\navg_trs_income &lt;- mean(trs_income_df$Pension_Benefit)\navg_orp_income &lt;- mean(withdrawals_data$Withdrawals)/12\n\naverage_income_table &lt;- tibble(\n  Retirement_Plan = c(\"TRS\", \"ORP\"),\n  Average_Monthly_Income = c(round(avg_trs_income, 2), round(avg_orp_income, 2))\n)\n\naverage_income_table_gt &lt;- average_income_table |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Average Monthly Income for TRS and ORP\"\n  ) |&gt;\n  cols_label(\n    Retirement_Plan = \"Retirement Plan\",\n    Average_Monthly_Income = \"Average Monthly Income ($)\"\n  ) |&gt;\n  fmt_currency(\n    columns = vars(Average_Monthly_Income),\n    currency = \"USD\",\n    use_seps = TRUE,\n    decimals = 2\n  )\n\naverage_income_table_gt\n\n\n\n\n\n\n\n\nAverage Monthly Income for TRS and ORP\n\n\nRetirement Plan\nAverage Monthly Income ($)\n\n\n\n\nTRS\n$2,905.74\n\n\nORP\n$2,187.64\n\n\n\n\n\n\n\nMaximum and Minimum Gap in Monthly Income (TRS vs ORP) The maximum gap represents the largest difference where TRS income was higher than ORP income. The minimum gap represents the largest difference where ORP income was higher than TRS income (a negative value indicates ORP is higher).\n\n\nCode\n# Calculate monthly income gap between TRS and ORP for each year\ntrs_monthly_income &lt;- trs_income_df$Pension_Benefit\norp_monthly_income &lt;- withdrawals_data$Withdrawals\n\n# Ensure both vectors are of the same length (use the same years for both plans)\ncommon_years &lt;- min(length(trs_monthly_income), length(orp_monthly_income))\nmonthly_gap &lt;- abs(trs_monthly_income[1:common_years] - orp_monthly_income[1:common_years])\n\n# Maximum and minimum gap\nmax_gap &lt;- max(monthly_gap)\nmin_gap &lt;- min(monthly_gap)\n\ncat(\"Maximum gap in monthly income between TRS and ORP: $\", round(max_gap, 2), \"\\n\")\n\n\nMaximum gap in monthly income between TRS and ORP: $ 26517.8 \n\n\nCode\ncat(\"Minimum gap in monthly income between TRS and ORP: $\", round(min_gap, 2), \"\\n\")\n\n\nMinimum gap in monthly income between TRS and ORP: $ 20874.67"
  },
  {
    "objectID": "mp04.html#trs-simulation",
    "href": "mp04.html#trs-simulation",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "TRS Simulation",
    "text": "TRS Simulation\nKey Assumptions: - Salary growth: Simulated using historical wage growth data. - Inflation: Applied annually to adjust pension payments. - Inflation-adjusted benefits: We simulate TRS benefits with a 3% maximum annual inflation increase.\n\n\nCode\nset.seed(123)  # For reproducibility\nn_simulations &lt;- 200\n\n# TRS Simulation\nsimulate_trs_monte_carlo &lt;- function(n_simulations, initial_salary, years_worked, wage_growth_data, inflation_data) {\n  wage_growth_data &lt;- na.omit(wage_growth_data)\n  inflation_data &lt;- na.omit(inflation_data)\n  \n  results &lt;- numeric(n_simulations) \n  \n  for (sim in 1:n_simulations) {\n    salary &lt;- initial_salary\n    salaries &lt;- numeric(years_worked)\n    retirement_benefit &lt;- numeric(years_worked * 12) \n    # Salary growth\n    for (i in 1:years_worked) {\n      growth_rate &lt;- sample(wage_growth_data$wage_growth_percent, 1) \n      inflation_rate &lt;- sample(inflation_data$inflation_rate, 1) \n      salary &lt;- salary * (1 + growth_rate + inflation_rate)\n      salaries[i] &lt;- salary \n    }\n    # Calculate Final Average Salary (FAS) based on the last 3 years of salary\n    final_average_salary &lt;- mean(tail(salaries, 3))\n    \n    trs_benefit &lt;- retirement_benefit(years_worked, final_average_salary)\n\n    current_pension &lt;- trs_benefit\n    retirement_benefit_monthly &lt;- trs_benefit / 12 \n    retirement_benefit[1] &lt;- trs_benefit\n    for (current_month in 2:(years_worked * 12)) {\n      current_year &lt;- ceiling(current_month / 12)\n      current_month_in_year &lt;- current_month %% 12\n      if (current_month_in_year == 0) current_month_in_year &lt;- 12   \n      if (current_month_in_year == 9) {  # The inflation adjustment is effective each September\n        sampled_inflation &lt;- sample(inflation_data$inflation_rate, 12, replace = TRUE)\n        mean_inflation &lt;- mean(sampled_inflation)\n        adjustment &lt;- inflation_adjustment(mean_inflation)\n        current_pension &lt;- current_pension * (1 + adjustment)\n      }\n      retirement_benefit[current_month] &lt;- current_pension\n    }\n    results[sim] &lt;- sum(retirement_benefit)\n  }\n\n  return(results)\n}\nstarting_salary &lt;- 50000 \nyears_worked &lt;- 20\nn_simulations &lt;- 200\n\n# Run the simulation\ntrs_results &lt;- simulate_trs_monte_carlo(n_simulations, starting_salary, years_worked, WAGE, INFLATION)\n\nmean_trs_benefit &lt;- mean(trs_results)\nmedian_trs_benefit &lt;- median(trs_results)\nquantiles_trs_benefit &lt;- quantile(trs_results, c(0.25, 0.75))\n\n# Print the results\ncat(\"Mean TRS Benefit: $\", mean_trs_benefit, \"\\n\")\n\n\nMean TRS Benefit: $ 5223770 \n\n\nCode\ncat(\"Median TRS Benefit: $\", median_trs_benefit, \"\\n\")\n\n\nMedian TRS Benefit: $ 5223068 \n\n\nCode\ncat(\"25th and 75th Percentiles of TRS Benefit: $\", quantiles_trs_benefit, \"\\n\")\n\n\n25th and 75th Percentiles of TRS Benefit: $ 5164768 5285646 \n\n\n\n\nCode\nmean_result &lt;- mean(trs_results)\nsd_result &lt;- sd(trs_results)\n\noptions(scipen = 999)\ntrs_results_df &lt;- data.frame(trs_benefit = trs_results)\n\nggplot(trs_results_df, aes(x = trs_benefit)) +\n  geom_histogram(bins = 25, fill = \"lightblue\", color = \"darkblue\", alpha = 0.7) + \n  geom_vline(aes(xintercept = mean(trs_benefit)), color = \"blue\", linetype = \"dashed\", size = 1.2) +\n  geom_vline(aes(xintercept = mean(trs_benefit) - sd(trs_benefit)), color = \"blue\", linetype = \"dotted\", size = 1.2) +\n  geom_vline(aes(xintercept = mean(trs_benefit) + sd(trs_benefit)), color = \"blue\", linetype = \"dotted\", size = 1.2) +\n  geom_text(aes(x = mean(trs_benefit), y = 0.2, label = paste(\"Mean\")), \n            color = \"black\", vjust = -45, size = 3) +\n  geom_text(aes(x = mean(trs_benefit) - sd(trs_benefit), y = 0.2, \n                label = paste(\"- 1 SD\")), \n            color = \"black\", vjust = -45, size = 3) +\n  geom_text(aes(x = mean(trs_benefit) + sd(trs_benefit), y = 0.2, \n                label = paste(\"+ 1 SD\")), \n            color = \"black\", vjust = -45, size = 3) +\n  labs(\n    title = \"Distribution of Simulations for TRS Benefit\",\n    x = \"TRS Benefit ($)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "mp04.html#orp-contribution-simulation",
    "href": "mp04.html#orp-contribution-simulation",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "ORP Contribution Simulation",
    "text": "ORP Contribution Simulation\nKey Assumptions: - Employee and Employer Contributions: Contributions are based on salary and a specified contribution rate. - Asset Allocation: The portfolio grows based on a mix of equities, bonds, and other assets. This mix changes based on the employee’s age. - Investment Returns: Simulated using historical returns from a variety of asset classes.\n\n\nCode\n# ORP Simulation\nsimulate_orp &lt;- function(n_simulations, starting_salary, years_worked, starting_age,\n                            wage_growth_data, inflation_data, equity_return_SPY_data, equity_return_ACWI_data,\n                            bond_return_data, debt_return_data) {\n  wage_growth_data &lt;- na.omit(wage_growth_data)\n  inflation_data &lt;- na.omit(inflation_data)\n  equity_return_SPY_data &lt;- na.omit(equity_return_SPY_data)\n  equity_return_ACWI_data &lt;- na.omit(equity_return_ACWI_data)\n  bond_return_data &lt;- na.omit(bond_return_data)\n  debt_return_data &lt;- na.omit(debt_return_data)\n  bond_return_data &lt;- bond_return_data |&gt; filter(date &gt;= \"2000-10-01\") # limit historical data to sample from \n  bond_return_data$bond_return &lt;- bond_return_data$bond_return/100\n  debt_return_data$debt_return &lt;- debt_return_data$debt_return/100\n\n  results &lt;- numeric(n_simulations)\n\n  for (i in 1:n_simulations) {\n    portfolio_value &lt;- 0\n    current_salary &lt;- starting_salary\n    total_contrib &lt;- 0\n    year &lt;- 1\n    \n    for (age in starting_age:(retirement_age - 1)) {\n      # Resample returns for this year (with replacement)\n      wage_growth &lt;- sample(wage_growth_data$wage_growth_percent, 1, replace = TRUE)\n      inflation_rate &lt;- sample(inflation_data$inflation_rate, 1, replace = TRUE)\n      equity_return_SPY &lt;- sample(equity_return_SPY_data$equity_return_SPY, 1, replace = TRUE)\n      equity_return_ACWI &lt;- sample(equity_return_ACWI_data$equity_return_ACWI, 1, replace = TRUE)\n      bond_return &lt;- sample(bond_return_data$bond_return, 1, replace = TRUE) \n      debt_return &lt;- sample(debt_return_data$debt_return, 1, replace = TRUE)\n     \n       # Calculate the contribution for this year\n      employee_contrib &lt;- employee_contrib_rate(current_salary)\n      employer_contrib &lt;- current_salary * employer_contrib_rate(year)\n      total_contrib &lt;- employee_contrib + employer_contrib\n      portfolio_value &lt;- portfolio_value + total_contrib\n      \n      # convert to average annual returns\n      equity_return_SPY &lt;- (1 + equity_return_SPY) ^ 12 - 1\n      equity_return_ACWI &lt;- (1 + equity_return_ACWI) ^ 12 - 1\n      bond_return &lt;- ((1 + bond_return) ^ 12) - 1\n      debt_return &lt;- ((1 + debt_return)) ^ 12 - 1\n      \n      # Asset allocation based on age\n      us_equities_pct &lt;- us_equities_rate(age)\n      international_pct &lt;- international_equities_rate(age)\n      bonds_pct &lt;- bonds_rate(age)\n      short_term_pct &lt;- short_term_debt_rate(age)\n      \n      portfolio_value &lt;- portfolio_value * (1 + us_equities_pct * equity_return_SPY +\n                                            international_pct * equity_return_ACWI +\n                                            bonds_pct * bond_return +\n                                            short_term_pct * debt_return)\n      current_salary &lt;- current_salary * (1 + wage_growth + inflation_rate)\n      year &lt;- year + 1\n    }\n    results[i] &lt;- portfolio_value\n  }\n  return(results)\n}\n\nstarting_salary &lt;- 50000\nyears_worked &lt;- 20\nstarting_age &lt;- 45\n\n# Run the Monte Carlo simulation\norp_results &lt;- simulate_orp(n_simulations, starting_salary, years_worked, starting_age,\n                           WAGE, INFLATION, SPY, ACWI, BOND, DEBT)\nmean_orp_benefit &lt;- mean(orp_results)\nmedian_orp_benefit &lt;- median(orp_results)\nquantiles_orp_benefit &lt;- quantile(orp_results, c(0.25, 0.75))\n\ncat(\"Mean OPR Benefit: $\", mean_orp_benefit, \"\\n\")\n\n\nMean OPR Benefit: $ 583829.8 \n\n\nCode\ncat(\"Median OPR Benefit: $\", median_orp_benefit, \"\\n\")\n\n\nMedian OPR Benefit: $ 565783.7 \n\n\nCode\ncat(\"25th and 75th Percentiles of OPR Benefit: $\", quantiles_orp_benefit, \"\\n\")\n\n\n25th and 75th Percentiles of OPR Benefit: $ 501773.6 655749"
  },
  {
    "objectID": "mp04.html#orp-withdrawal-simulation",
    "href": "mp04.html#orp-withdrawal-simulation",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "ORP Withdrawal Simulation",
    "text": "ORP Withdrawal Simulation\n\n\nCode\nsimulate_orp_withdrawal &lt;- function(orp_results, retirement_age, life_expectancy, withdrawal_rate, equity_return_SPY_data, equity_return_ACWI_data, bond_return_data, debt_return_data) {\n  remaining_balances &lt;- numeric(length(orp_results))  # Store the remaining balances for each simulation\n  for (i in 1:length(orp_results)) {\n    remaining_balance &lt;- orp_results[i] \n    age_at_retirement &lt;- retirement_age\n    years_in_retirement &lt;- life_expectancy - age_at_retirement\n    # Simulate withdrawals over retirement years\n    for (year in 1:years_in_retirement) {\n      age_in_retirement &lt;- age_at_retirement + year\n      # Withdraw a fixed percentage of the portfolio value\n      annual_withdrawal &lt;- remaining_balance * withdrawal_rate\n      remaining_balance &lt;- remaining_balance - annual_withdrawal\n      # Sample returns for this year\n      equity_return_SPY &lt;- sample(equity_return_SPY_data$equity_return_SPY, 1)\n      equity_return_ACWI &lt;- sample(equity_return_ACWI_data$equity_return_ACWI, 1)\n      bond_return &lt;- sample(bond_return_data$bond_return, 1) /100\n      debt_return &lt;- sample(debt_return_data$debt_return, 1) /100\n      \n      # Assume fixed portfolio allocations during retirement (could be modified)\n      us_equities_pct &lt;- us_equities_rate(age_in_retirement)\n      international_pct &lt;- international_equities_rate(age_in_retirement)\n      bonds_pct &lt;- bonds_rate(age_in_retirement)\n      debt_pct &lt;- short_term_debt_rate(age_in_retirement)\n      \n      remaining_balance &lt;- remaining_balance * (1 + us_equities_pct * equity_return_SPY + \n                                                international_pct * equity_return_ACWI +\n                                                bonds_pct * bond_return + \n                                                debt_pct * debt_return)\n    }\n    \n    # Store the result for this simulation (1 = exhausted, 0 = not exhausted)\n    remaining_balances[i] &lt;- remaining_balance\n  }\n  return(remaining_balances)\n}\n\n# Define your assumptions for the simulation\nwithdrawal_rate &lt;- 0.04  # 4% withdrawal rate\nlife_expectancy &lt;- 85  # Assume the person lives to 85 years old\nretirement_age &lt;- 65  # Person retires at age 65\nmonthly_orp &lt;- orp_results/(life_expectancy- retirement_age)/12\n\n\n# Run the simulation for exhaustion probability\nsimulate_orp_withdrawal &lt;- simulate_orp_withdrawal(orp_results, retirement_age, life_expectancy, withdrawal_rate, SPY, ACWI, BOND, DEBT)\n\n\n\n\nCode\nmean_result &lt;- mean(orp_results)\nsd_result &lt;- sd(orp_results)\n\noptions(scipen = 999)\norp_results_df &lt;- data.frame(orp_benefit = orp_results)\n\nggplot(orp_results_df, aes(x = orp_benefit)) +\n  geom_histogram(bins = 25, fill = \"lightblue\", color = \"darkblue\", alpha = 0.7) + \n  geom_vline(aes(xintercept = mean(orp_benefit)), color = \"blue\", linetype = \"dashed\", size = 1.2) +\n  geom_vline(aes(xintercept = mean(orp_benefit) - sd(orp_benefit)), color = \"blue\", linetype = \"dotted\", size = 1.2) +\n  geom_vline(aes(xintercept = mean(orp_benefit) + sd(orp_benefit)), color = \"blue\", linetype = \"dotted\", size = 1.2) +\n  geom_text(aes(x = mean(orp_benefit), y = 0.2, label = paste(\"Mean\")), \n            color = \"black\", vjust = -45, size = 3) +\n  geom_text(aes(x = mean(orp_benefit) - sd(orp_benefit), y = 0.2, \n                label = paste(\"- 1 SD\")), \n            color = \"black\", vjust = -45, size = 3) +\n  geom_text(aes(x = mean(orp_benefit) + sd(orp_benefit), y = 0.2, \n                label = paste(\"+ 1 SD\")), \n            color = \"black\", vjust = -45, size = 3) +\n  labs(\n    title = \"Distribution of Simulations for ORP Portfolio Value\",\n    x = \"ORP Portfolio Value ($)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\nORP Depletion Probability The simulations indicate that there is no risk of your ORP balance depleting during retirement. This is a strong positive, suggesting that your ORP contributions, along with the returns on your investments, are sufficient to sustain withdrawals throughout retirement. The ORP offers a degree of flexibility, allowing you to withdraw funds as needed, but the fact that the simulations show no depletion is reassuring.\n\n\nCode\n# Calculate the probability of exhaustion\nprob_exhaustion &lt;- mean(simulate_orp_withdrawal == 0)\ncat(\"Probability of exhaustion: \", prob_exhaustion * 100, \"%\\n\")\n\n\nProbability of exhaustion:  NA %\n\n\nChance of Higher ORP Income Comparing the ORP income against the TRS pension shows that there’s a 60% chance that the ORP will provide higher monthly income than the TRS pension over the course of retirement. The TRS plan offers a guaranteed pension that will provide a fixed income based on your salary and years of service, offering stability and predictability. If you prefer certainty and lower risk, the TRS pension might be a better choice, though you may sacrifice the opportunity for higher returns seen in the ORP. To reach this insight, I performed the simulations while storing each month’s income rather than the final annual results for both TRS and ORP.\n\n\nCode\nset.seed(123)  # For reproducibility\nn_simulations &lt;- 200\n\nmonthly_trs_monte_carlo &lt;- function(n_simulations, initial_salary, years_worked, wage_growth_data, inflation_data) {\n  wage_growth_data &lt;- na.omit(wage_growth_data)\n  inflation_data &lt;- na.omit(inflation_data)\n  results &lt;- matrix(NA, n_simulations, years_worked * 12)\n\n  for (sim in 1:n_simulations) {\n    salary &lt;- initial_salary\n    salaries &lt;- numeric(years_worked)\n    retirement_benefit &lt;- numeric(years_worked * 12) \n    \n    # Simulate salary growth over the years\n    for (i in 1:years_worked) {\n      growth_rate &lt;- sample(wage_growth_data$wage_growth_percent, 1) \n      inflation_rate &lt;- sample(inflation_data$inflation_rate, 1) \n      salary &lt;- salary * (1 + growth_rate + inflation_rate)\n      salaries[i] &lt;- salary\n    }\n    final_average_salary &lt;- mean(tail(salaries, 3))\n   \n    # Compute TRS benefit based on FAS\n    trs_benefit_yearly &lt;- retirement_benefit(years_worked, final_average_salary)\n    current_pension &lt;- trs_benefit_yearly / 12\n    retirement_benefit[1] &lt;- current_pension\n    \n    # Simulate monthly adjustments during retirement (inflation adjustments)\n    for (current_month in 2:(years_worked * 12)) {\n      current_year &lt;- ceiling(current_month / 12)\n      current_month_in_year &lt;- current_month %% 12\n      if (current_month_in_year == 0) current_month_in_year &lt;- 12   \n      \n      # Apply inflation adjustment every September (month 9)\n      if (current_month_in_year == 9) {\n        sampled_inflation &lt;- sample(inflation_data$inflation_rate, 12, replace = TRUE)\n        mean_inflation &lt;- mean(sampled_inflation)\n        adjustment &lt;- inflation_adjustment(mean_inflation)\n        current_pension &lt;- current_pension * (1 + adjustment)\n      }\n      retirement_benefit[current_month] &lt;- current_pension\n    }\n    results[sim, ] &lt;- retirement_benefit\n  }\n  \n  return(results)\n}\n\nstarting_salary &lt;- 50000\nyears_worked &lt;- 20\nn_simulations &lt;- 200\n\nmonthly_trs_results &lt;- monthly_trs_monte_carlo(n_simulations, starting_salary, years_worked, WAGE, INFLATION)\n\nmonths &lt;- rep(1:(years_worked * 12), n_simulations)\nsimulations &lt;- rep(1:n_simulations, each = years_worked * 12)  \n\n# Create data frame\ntrs_monthly_monte_carlo_df &lt;- data.frame(\n  Simulation = simulations,\n  Month = months,\n  RetirementBenefit = as.vector(monthly_trs_results)\n)\n\n\n\n\nCode\nsimulate_monthly_orp_withdrawal &lt;- function(orp_results, retirement_age, life_expectancy, withdrawal_rate, \n                                           equity_return_SPY_data, equity_return_ACWI_data, \n                                           bond_return_data, debt_return_data) {\n  months_in_retirement &lt;- (life_expectancy - retirement_age) * 12\n  monthly_withdrawals &lt;- matrix(NA, nrow = length(orp_results), ncol = months_in_retirement)\n  \n  # Run simulations for each ORP result\n  for (i in 1:length(orp_results)) {\n    remaining_balance &lt;- orp_results[i]  \n    age_at_retirement &lt;- retirement_age\n    \n    # Calculate monthly withdrawal amount based on the initial balance and withdrawal rate\n    monthly_withdrawal &lt;- (remaining_balance * withdrawal_rate) / 12 \n    \n    # Simulate withdrawals and returns for each month\n    for (month in 1:months_in_retirement) {\n      monthly_withdrawals[i, month] &lt;- monthly_withdrawal\n      \n      # Sample monthly returns from the annual returns data\n      equity_return_SPY &lt;- sample(equity_return_SPY_data$equity_return_SPY, 1) / 12\n      equity_return_ACWI &lt;- sample(equity_return_ACWI_data$equity_return_ACWI, 1) / 12\n      bond_return &lt;- sample(bond_return_data$bond_return, 1) / 100 / 12  \n      debt_return &lt;- sample(debt_return_data$debt_return, 1) / 100 / 12\n      \n      # Portfolio allocations (can be modified to change with age)\n      us_equities_pct &lt;- us_equities_rate(age_at_retirement + floor(month / 12)) \n      international_pct &lt;- international_equities_rate(age_at_retirement + floor(month / 12))\n      bonds_pct &lt;- bonds_rate(age_at_retirement + floor(month / 12))\n      debt_pct &lt;- short_term_debt_rate(age_at_retirement + floor(month / 12))\n      \n      # Calculate the monthly return based on portfolio allocations\n      remaining_balance &lt;- remaining_balance * (1 + us_equities_pct * equity_return_SPY + \n                                                international_pct * equity_return_ACWI +\n                                                bonds_pct * bond_return + \n                                                debt_pct * debt_return)\n      \n      # Subtract monthly withdrawal from remaining balance\n      remaining_balance &lt;- remaining_balance - monthly_withdrawal\n    }\n  }\n  return(monthly_withdrawals) \n}\n\nwithdrawal_rate &lt;- 0.04 \nlife_expectancy &lt;- 85  \nretirement_age &lt;- 65 \n\nmonthly_balances &lt;- simulate_monthly_orp_withdrawal(orp_results, retirement_age, life_expectancy, withdrawal_rate, SPY, ACWI, BOND, DEBT)\n\n# Create the correct months and simulations vectors:\nmonths &lt;- rep(1:((life_expectancy - retirement_age) * 12), length(orp_results)) \nsimulations &lt;- rep(1:length(orp_results), each = (life_expectancy - retirement_age) * 12) \n\n# Create the ORP monthly data frame\norp_monthly_monte_carlo_df &lt;- data.frame(\n  Simulation = simulations,\n  Month = months,\n  Withdrawal = as.vector(monthly_balances)\n)\n\n\n\n\nCode\ncombined_monte_carlo_df &lt;- full_join(trs_monthly_monte_carlo_df, \n                                     orp_monthly_monte_carlo_df, \n                                     by = c(\"Simulation\", \"Month\"))\n\nanalyze_simulation &lt;- function(simulation_data, num_simulations) {\n  simulation_data &lt;- simulation_data |&gt;\n    mutate(\n      orp_benefit = as.numeric(Withdrawal),\n      trs_benefit = as.numeric(RetirementBenefit),\n      orp_greater = orp_benefit &gt; trs_benefit\n    )\n  \n  # Calculate the probability that ORP benefit exceeds TRS benefit\n  income_exceeds_trs_probability &lt;- simulation_data |&gt;\n    summarize(probability = mean(orp_greater, na.rm = TRUE)) |&gt;\n    pull(probability)\n  \n  # Calculate the depletion probability: proportion of simulations where ORP balance reaches 0\n  depletion_probability &lt;- simulation_data |&gt;\n    filter(orp_benefit == 0) |&gt;\n    summarize(probability = n_distinct(Simulation) / num_simulations) |&gt;\n    pull(probability)\n  \n  result_table &lt;- data.frame(\n    Metric = c(\"Probability of ORP Depletion\", \"Probability ORP Income Exceeds TRS\"),\n    Value = c(depletion_probability, income_exceeds_trs_probability)\n  )\n  return(result_table)\n}\n\n\nmonte_carlo_analysis &lt;- analyze_simulation(combined_monte_carlo_df, n_simulations)\nmonte_carlo_analysis |&gt;\n    gt() |&gt;\n    tab_header(title = \"Simulation Analysis Results\") |&gt;\n    cols_label(Metric = \"Metric\", Value = \"Probability\") |&gt;\n    fmt_number(columns = vars(Value), decimals = 2, use_seps = TRUE) |&gt;\n    tab_spanner(label = \"Analysis Results\", columns = vars(Metric, Value)) |&gt;\n    tab_style(style = list(cell_text(weight = \"bold\")),\n              locations = cells_column_labels(columns = everything())\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation Analysis Results\n\n\n\nAnalysis Results\n\n\n\nMetric\nProbability\n\n\n\n\nProbability of ORP Depletion\n0.00\n\n\nProbability ORP Income Exceeds TRS\n0.58"
  },
  {
    "objectID": "mp04.html#decision-factors-for-you",
    "href": "mp04.html#decision-factors-for-you",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "Decision Factors for You",
    "text": "Decision Factors for You\n\nHow long do you plan to stay at CUNY?\nAre you comfortable with market fluctuations?\nDo you value control over your investments?\nWhat is your current and expected future salary?\n\n\nWhen Should You Consider TRS?\n\nIf you want certainty in retirement and prefer not to worry about market fluctuations.\nIf you have a long career with CUNY, since TRS benefits become more favorable the longer you work (especially after 20+ years of service).\nIf you want steady, inflation-adjusted income in retirement.\n\n\n\nWhen Should You Consider ORP?\n\nIf you have a shorter tenure at CUNY or if you’re considering leaving CUNY at some point and want to take your retirement savings with you.\nIf you’re comfortable with market risk and want more control over your investments.\nIf you believe you can achieve higher returns through investment growth compared to a fixed pension benefit."
  },
  {
    "objectID": "mp04.html#footnotes",
    "href": "mp04.html#footnotes",
    "title": "Choosing Your CUNY Retirement Plan: TRS or ORP",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://fred.stlouisfed.org/↩︎\nhttps://www.alphavantage.co/documentation/↩︎\nNew York State & Local Retirement System Member Contribution↩︎"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Proposing a Successful Film",
    "section": "",
    "text": "This project uses data made available by Internet Movie Database (IMDb). While I did intially use the full data set in my analysis, I switched over to using a sub-sampled pre-processed data because my computer was struggling to handle the full data.\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(gt)\n\nget_imdb_file &lt;- function(fname){\n  fname_ext &lt;- paste0(fname, \".csv.zip\")\n  as.data.frame(readr::read_csv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS &lt;- get_imdb_file(\"name_basics_small\")\nTITLE_BASICS     &lt;- get_imdb_file(\"title_basics_small\")\nTITLE_EPISODES &lt;- get_imdb_file(\"title_episodes_small\")\nTITLE_RATINGS &lt;- get_imdb_file(\"title_ratings_small\")\nTITLE_CREW &lt;- get_imdb_file(\"title_crew_small\")\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title_principals_small\")\n\nBelow, I am converting the birth year and death year to numeric data types.\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  mutate(birthYear = as.numeric(birthYear), # numeric birth years\n         deathYear = as.numeric(deathYear))\n\nWith that, the data I am using has been narrowed down significantly to a smaller and manageable sub sample.\n\n\n\n\nDisplay Code\nlibrary(DT)\nlibrary(dplyr)\n\nsample_n(NAME_BASICS, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nDisplay Code\nsample_n(TITLE_BASICS, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nDisplay Code\nsample_n(TITLE_CREW, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nDisplay Code\nsample_n(TITLE_EPISODES, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nDisplay Code\nsample_n(TITLE_RATINGS, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nDisplay Code\nsample_n(TITLE_PRINCIPALS, 1000) |&gt;\n  DT::datatable()"
  },
  {
    "objectID": "mp02.html#data",
    "href": "mp02.html#data",
    "title": "Proposing a Successful Film",
    "section": "",
    "text": "This project uses data made available by Internet Movie Database (IMDb). While I did intially use the full data set in my analysis, I switched over to using a sub-sampled pre-processed data because my computer was struggling to handle the full data.\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(gt)\n\nget_imdb_file &lt;- function(fname){\n  fname_ext &lt;- paste0(fname, \".csv.zip\")\n  as.data.frame(readr::read_csv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS &lt;- get_imdb_file(\"name_basics_small\")\nTITLE_BASICS     &lt;- get_imdb_file(\"title_basics_small\")\nTITLE_EPISODES &lt;- get_imdb_file(\"title_episodes_small\")\nTITLE_RATINGS &lt;- get_imdb_file(\"title_ratings_small\")\nTITLE_CREW &lt;- get_imdb_file(\"title_crew_small\")\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title_principals_small\")\n\nBelow, I am converting the birth year and death year to numeric data types.\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  mutate(birthYear = as.numeric(birthYear), # numeric birth years\n         deathYear = as.numeric(deathYear))\n\nWith that, the data I am using has been narrowed down significantly to a smaller and manageable sub sample.\n\n\n\n\nDisplay Code\nlibrary(DT)\nlibrary(dplyr)\n\nsample_n(NAME_BASICS, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nDisplay Code\nsample_n(TITLE_BASICS, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nDisplay Code\nsample_n(TITLE_CREW, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nDisplay Code\nsample_n(TITLE_EPISODES, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nDisplay Code\nsample_n(TITLE_RATINGS, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nDisplay Code\nsample_n(TITLE_PRINCIPALS, 1000) |&gt;\n  DT::datatable()"
  },
  {
    "objectID": "mp02.html#preliminary-exploration",
    "href": "mp02.html#preliminary-exploration",
    "title": "Proposing a Successful Film",
    "section": "Preliminary Exploration",
    "text": "Preliminary Exploration\n1. How many movies are in our data set? How many TV series? How many TV episodes?\n\n\nDisplay Code\nlibrary(gt)\nTITLE_BASICS |&gt;\n  group_by(titleType) |&gt;\n  filter(titleType %in% c(\"movie\", \"tvSeries\", \"tvEpisode\" )) |&gt;\n  summarise(count = n()) |&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Number of Title Types\"\n  ) |&gt;\n  cols_label( # display column names\n    titleType = \"Type\",\n    count = \"Count\"\n  )\n\n\n\n\n\n\n\n\nNumber of Title Types\n\n\nType\nCount\n\n\n\n\nmovie\n131662\n\n\ntvEpisode\n155722\n\n\ntvSeries\n29789\n\n\n\n\n\n\n\n2. Who is the oldest living person in our data set?\n\n\nDisplay Code\nNAME_BASICS |&gt; \n  filter(is.na(deathYear)) |&gt; # filter for those without a deathYear\n  arrange(birthYear) |&gt; \n  slice(1) |&gt;\n  gt() |&gt; # create a display table\n  cols_label( # display column names\n    primaryName = \"Name\",\n    birthYear = \"Birth Year\"\n  )\n\n\n\n\n\n\n\n\nnconst\nName\nBirth Year\ndeathYear\nprimaryProfession\nknownForTitles\n\n\n\n\nnm5671597\nRobert De Visée\n1655\nNA\ncomposer,soundtrack\ntt2219674,tt1743724,tt0441074,tt14426058\n\n\n\n\n\n\n\nThe results of this first query says Robert De Visee but Robert was born in 1655, which does not make sense at all. A quick Google search says that the oldest person alive in 2024 is 116. Given that, let us filter for people born after 1914.\n\n\nDisplay Code\nNAME_BASICS |&gt; \n  filter(is.na(deathYear)) |&gt; # filter for those without a deathYear\n  filter(birthYear&gt;=1914) |&gt;\n  arrange(birthYear) |&gt; \n  slice(1)\n\n\n     nconst    primaryName birthYear deathYear primaryProfession\n1 nm0029349 Antonio Anelli      1914        NA             actor\n                           knownForTitles\n1 tt0072364,tt0068416,tt0065460,tt0068973\n\n\nLet’s perform a sanity check by confirming online. The internet says Antonio Anelli died on 12 May 1977.\n\n\nDisplay Code\nNAME_BASICS |&gt; \n  filter(is.na(deathYear)) |&gt; # filter for those without a deathYear\n  filter(birthYear&gt;=1916) |&gt;\n  arrange(birthYear) |&gt; \n  slice(1)\n\n\n     nconst primaryName birthYear deathYear                   primaryProfession\n1 nm0048527   Ivy Baker      1916        NA costume_department,costume_designer\n                           knownForTitles\n1 tt0041959,tt0054518,tt0066344,tt0042522\n\n\nIvy Baker born in 1916 and her bio on IMDb does not have a death year.\nTo be safe, let’s check for a year before 1916 and look at 1915.\n\n\nDisplay Code\nNAME_BASICS |&gt; \n  filter(is.na(deathYear)) |&gt; # filter for those without a deathYear\n  filter(birthYear&gt;=1915) |&gt;\n  arrange(birthYear) |&gt; \n  slice(1)\n\n\n     nconst    primaryName birthYear deathYear     primaryProfession\n1 nm0015296 Akhtar-Ul-Iman      1915        NA writer,actor,director\n                           knownForTitles\n1 tt0059893,tt0175450,tt0060689,tt0158587\n\n\nThe result says Akhtar-Ul-Iman but according to Wikipedia, he died on March 9, 1996.\n3. There is one TV Episode in this data set with a perfect 10/10 rating and 200,000 IMDb ratings.\n\n\nDisplay Code\nTITLE_RATINGS |&gt; \n  left_join(TITLE_EPISODES, by = \"tconst\") |&gt; # join using tconst\n  filter(averageRating == 10.0 & numVotes &gt;= 200000) |&gt; # filter by averageRating and numVotes\n  left_join(TITLE_BASICS, by = \"tconst\") |&gt; # join using tconst\n  left_join(TITLE_BASICS, join_by(\"parentTconst\" == \"tconst\")) |&gt;\n  select(primaryTitle.y, primaryTitle.x, averageRating, numVotes) |&gt; # select just the title, average rating, and number of votes\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Perfect TV Episode\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryTitle.y = \"TV Show\",\n    primaryTitle.x = \"Episode\",\n    averageRating = \"Average Rating\",\n    numVotes = \"Number of Votes\"\n  )\n\n\n\n\n\n\n\n\nPerfect TV Episode\n\n\nTV Show\nEpisode\nAverage Rating\nNumber of Votes\n\n\n\n\nBreaking Bad\nOzymandias\n10\n227589\n\n\n\n\n\n\n\n4. What four projects is the actor Mark Hammill most known for?\n\n\nDisplay Code\nlibrary(tidyr)\nNAME_BASICS |&gt; \n  filter(primaryName == \"Mark Hamill\") |&gt; \n  select(knownForTitles) |&gt; \n  separate_rows(knownForTitles, sep = \",\") |&gt;  # Split by comma to make each knownForTitle a row\n  left_join(TITLE_BASICS, by = c(\"knownForTitles\" = \"tconst\")) |&gt; # join to TITLE_BASICS on tconst\n  select(primaryTitle) |&gt; \n  head(4) |&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Titles Mark Hammil is Known For\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryTitle = \"Title\"\n  )\n\n\n\n\n\n\n\n\nTitles Mark Hammil is Known For\n\n\nTitle\n\n\n\n\nStar Wars: Episode IV - A New Hope\n\n\nStar Wars: Episode VIII - The Last Jedi\n\n\nStar Wars: Episode V - The Empire Strikes Back\n\n\nStar Wars: Episode VI - Return of the Jedi\n\n\n\n\n\n\n\n5. What TV series, with more than 12 episodes, has the highest average rating?\n\n\nDisplay Code\n# Find the highest-rated TV series with more than 12 episodes\n# tt15613780        9.7           318 Craft Games\n\nTITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  right_join(TITLE_EPISODES, by = c(\"tconst\" = \"parentTconst\")) |&gt;\n  group_by(tconst) |&gt;\n  mutate(episode_count = n()) |&gt;\n  ungroup() |&gt;\n  filter(episode_count &gt; 12) |&gt;\n  left_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  arrange(desc(averageRating)) |&gt;\n  select(primaryTitle, averageRating, episode_count) |&gt;\n  head(1) |&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Highest-Rated TV Series with More Than 12 Episodes\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryTitle = \"TV Series\",\n    averageRating = \"Average Rating\",\n    episode_count = \"Number of Episodes\"\n  )\n\n\n\n\n\n\n\n\nHighest-Rated TV Series with More Than 12 Episodes\n\n\nTV Series\nAverage Rating\nNumber of Episodes\n\n\n\n\nCraft Games\n9.7\n318\n\n\n\n\n\n\n\n6. Is it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons? The TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality.\nFirst, I’m finding the tconst for Happy Days.\n\n\nDisplay Code\nTITLE_BASICS |&gt;\n  filter(originalTitle == \"Happy Days\") |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  filter(startYear == \"1974\") |&gt;\n  select(tconst) # find the tconst for Happy Days\n\n\n     tconst\n1 tt0070992\n\n\nWe can see below that seasons 1 through 4 do well in terms of average rating. Aside from season 11, the seasons after season 5 are all in the bottom half of ratings. Meanwhile, season 3 has the highest rating of 7.7.\n\n\nDisplay Code\nTITLE_EPISODES |&gt; \n  filter(parentTconst == \"tt0070992\") |&gt; \n  left_join(TITLE_RATINGS, join_by(\"tconst\" )) |&gt; \n  group_by(seasonNumber) |&gt; \n  summarize(avg_rating = mean(averageRating, na.rm = TRUE)) |&gt; \n  mutate(seasonNumber = as.numeric(seasonNumber)) |&gt;\n  select(seasonNumber, avg_rating) |&gt;\n  arrange(desc(avg_rating)) |&gt; # arrange by average rating in descending order\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Ratings for Happy Days by Season\"\n  ) |&gt;\n  cols_label( # display column names\n    seasonNumber = \"Season\",\n    avg_rating = \"Average Rating\"\n  )\n\n\n\n\n\n\n\n\nRatings for Happy Days by Season\n\n\nSeason\nAverage Rating\n\n\n\n\n3\n7.700000\n\n\n2\n7.691304\n\n\n1\n7.581250\n\n\n4\n7.428000\n\n\n11\n7.333333\n\n\n6\n7.018750\n\n\n5\n7.000000\n\n\n10\n6.700000\n\n\n9\n6.400000\n\n\n7\n6.333333\n\n\n8\n5.400000\n\n\n\n\n\n\n\nIn the plot below, we see that there is a indeed a dip in ratings in the later seasons of the show.\n\n\nDisplay Code\nseason_ratings &lt;- TITLE_EPISODES |&gt; \n  filter(parentTconst == \"tt0070992\") |&gt; \n  left_join(TITLE_RATINGS, join_by(\"tconst\" )) |&gt; \n  group_by(seasonNumber) |&gt; \n  summarize(avg_rating = mean(averageRating, na.rm = TRUE)) |&gt; \n  mutate(seasonNumber = as.numeric(seasonNumber)) |&gt;\n  select(seasonNumber, avg_rating) |&gt;\n  arrange(desc(seasonNumber)) # arrange by season in descending order\n\nlibrary(ggplot2)\n# Bar chart of average ratings by season \nggplot(season_ratings, aes(x = seasonNumber, y = avg_rating)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Average Ratings of Happy Days by Season\",\n       x = \"Season Number\",\n       y = \"Average Rating\") +\n  ylim(0,10) +\n  scale_x_continuous(breaks = seq(1, 11, by = 1)) +\n  geom_text(aes(label=round(avg_rating, digits = 1)), vjust=0)"
  },
  {
    "objectID": "mp02.html#threshold-for-a-solid-title",
    "href": "mp02.html#threshold-for-a-solid-title",
    "title": "Proposing a Successful Film",
    "section": "Threshold for a Solid Title",
    "text": "Threshold for a Solid Title\nTo come up with a numerical threshold for a project to be a ‘success’ and determine a value such that movies above are all “solid” or better, I am going to use summary statistics. The 3rd Q is 0.6 and so I thinkk a score greater than 0.6 is “solid” and above the average movie.\n\n\nDisplay Code\nggplot(TITLE_RATINGS, aes(x=success_score)) + geom_histogram()\n\n\n\n\n\n\n\n\n\nDisplay Code\nsummary(TITLE_RATINGS$success_score)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.2046  0.4987  0.5517  0.5484  0.6000  0.9653"
  },
  {
    "objectID": "mp02.html#examining-success-by-genre-and-decade",
    "href": "mp02.html#examining-success-by-genre-and-decade",
    "title": "Proposing a Successful Film",
    "section": "Examining Success by Genre and Decade",
    "text": "Examining Success by Genre and Decade\nIn this section, we explore the interplay between film genres and their success across different decades, aiming to uncover trends that can inform a future project. By analyzing the performance of various genres over time, we can identify promising opportunities for an upcoming film.\n\n\nDisplay Code\n# highest average score\n# Biography, Adventure, Animation\nTITLE_BASICS |&gt;\n  left_join(TITLE_RATINGS, by = \"tconst\") |&gt;  # Corrected join syntax\n  separate_longer_delim(genres, \",\") |&gt; \n  group_by(genres) |&gt;  # Group by genres\n  summarise(avg_score = mean(success_score, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  arrange(desc(avg_score)) |&gt;\n  head(5)  |&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Genres with Top 10 Average Success Scores\"\n  ) |&gt;\n  cols_label( # display column names\n    genres = \"Genre\",\n    avg_score = \"Average Success Score\"\n  )\n\n\n\n\n\n\n\n\nGenres with Top 10 Average Success Scores\n\n\nGenre\nAverage Success Score\n\n\n\n\nBiography\n0.5739485\n\n\nAdventure\n0.5732288\n\n\nAnimation\n0.5721809\n\n\nCrime\n0.5692845\n\n\nHistory\n0.5671059\n\n\n\n\n\n\n\nThe average success scores for the various genres seem fairly close to each other. Instead, we can count the number of success scores greater than the threshold of 0.6 for each genre.\n\n\nDisplay Code\n# Drama, Comedy, Action\nTITLE_BASICS |&gt;\n  left_join(TITLE_RATINGS, by = \"tconst\") |&gt;  # Corrected join syntax\n  filter(success_score &gt;= 0.6) |&gt; # greater than threshold\n  separate_longer_delim(genres, \",\") |&gt; \n  group_by(genres) |&gt;  # Group by genres\n  summarise(success_count = n()) |&gt;\n  ungroup() |&gt;\n  arrange(desc(success_count)) |&gt;\n  head(5) |&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Genres with Top 10 Average Success Scores\"\n  ) |&gt;\n  cols_label( # display column names\n    genres = \"Genre\",\n    success_count = \"Number of Success Scores &gt; 0.60\"\n  )\n\n\n\n\n\n\n\n\nGenres with Top 10 Average Success Scores\n\n\nGenre\nNumber of Success Scores &gt; 0.60\n\n\n\n\nDrama\n52577\n\n\nComedy\n30306\n\n\nAction\n24394\n\n\nAdventure\n19408\n\n\nCrime\n19341\n\n\n\n\n\n\n\nWhat was the genre with the most “successes” in each decade? What genre consistently has the most “successes”?\n\n\nDisplay Code\n# Success by genre and decade\nsuccess_by_decade &lt;- TITLE_BASICS |&gt;\n  left_join(TITLE_RATINGS, by = \"tconst\") |&gt; \n  filter(success_score &gt;= 0.6) |&gt; \n  mutate(decade = floor(as.integer(startYear) / 10) * 10) |&gt;  # Create a decade column\n  separate_longer_delim(genres, \",\") |&gt; \n  group_by(decade, genres) |&gt; \n  summarise(success_count = n()) |&gt; \n  ungroup() |&gt;\n  arrange(decade, desc(success_count)) \n\n# Find the genre with the most successes in each decade\ntop_genre_per_decade &lt;- success_by_decade |&gt;\n  group_by(decade) |&gt; \n  slice_max(success_count, n = 1) # select top performing\n\n# Count how many times each genre is the highest in each decade\ntop_genre_per_decade |&gt;\n  group_by(genres) |&gt; \n  summarise(highest_count = n()) |&gt; \n  arrange(desc(highest_count)) |&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Number of Times Genres Had the Most Successes of a Decade\"\n  ) |&gt;\n  cols_label( # display column names\n    genres = \"Genre\",\n    highest_count = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\nNumber of Times Genres Had the Most Successes of a Decade\n\n\nGenre\nFrequency\n\n\n\n\nDrama\n12\n\n\nShort\n3\n\n\nComedy\n2\n\n\nDocumentary\n2\n\n\nSport\n1\n\n\n\n\n\n\n\nWhat genre used to reliably produced “successes” and has fallen out of favor?\n\n\nDisplay Code\nlibrary(ggrepel)\nlibrary(ggplot2)\n\ngenre_success &lt;- TITLE_BASICS |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  filter(success_score &gt; 0.6) |&gt;  # Set your success threshold\n  filter(!is.na(startYear) & !is.na(success_score)) |&gt;\n  separate_longer_delim(genres, \",\") |&gt; \n  group_by(genres, startYear) |&gt;\n  summarise(success_count = n())\n\ngenre_success |&gt;\n  ggplot(aes(x = startYear, y = success_count, color = genres)) +\n  geom_line(size = 0.5) +       # Use lines to connect points\n  geom_point(size = 1) +      # Add points for each data point\n  labs(\n    title = \"Count of Successful Movies by Genre Over Time\",\n    x = \"Year\",\n    y = \"Count of Successful Movies\",\n    color = \"Genre\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nWhat genre has produced the most “successes” since 2010? It seems that many films just get counted as Dramas with 32923 Dramas.\n\n\nDisplay Code\nTITLE_BASICS |&gt;\n  left_join(TITLE_RATINGS, by = \"tconst\") |&gt; \n  filter(success_score &gt;= 0.6, startYear &gt;= 2010) |&gt; # filter by 2010\n  separate_longer_delim(genres, \",\") |&gt; \n  group_by(genres) |&gt; \n  summarise(success_count = n()) |&gt; \n  ungroup() |&gt;\n  arrange(desc(success_count)) |&gt;\n  head(5) |&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Success Count by Genre\"\n  ) |&gt;\n  cols_label( # display column names\n    genres = \"Genre\",\n    success_count = \"Success Count\"\n  )\n\n\n\n\n\n\n\n\nSuccess Count by Genre\n\n\nGenre\nSuccess Count\n\n\n\n\nDrama\n32923\n\n\nComedy\n16142\n\n\nAction\n15961\n\n\nCrime\n12626\n\n\nAdventure\n11665\n\n\n\n\n\n\n\nDisplay Code\nrecent_top_genres &lt;- TITLE_BASICS |&gt;\n  left_join(TITLE_RATINGS, by = \"tconst\") |&gt; \n  filter(success_score &gt;= 0.6, startYear &gt;= 2010) |&gt; \n  separate_longer_delim(genres, \",\") |&gt; \n  group_by(genres) |&gt; \n  summarise(success_count = n()) |&gt; \n  ungroup() |&gt;\n  arrange(desc(success_count)) |&gt;\n  head(5)\n\nrecent_top_genres |&gt;\n  ggplot(aes(x = genres, y = success_count, fill = genres)) +\n  geom_bar(stat=\"identity\") +\n  labs(\n    title = \"Count of Successful Movies by Genre Over Time\",\n    x = \"Genre\",\n    y = \"Count of Successful Movies\",\n    color = \"Genre\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nWithin the Drama genre, what were the most successful movies? It turns out the top movies we saw before all fall within the Drama genre as well.\n\n\nDisplay Code\nTITLE_RATINGS |&gt; \n  arrange(desc(success_score)) |&gt;  \n  left_join(TITLE_BASICS, join_by(\"tconst\" )) |&gt; \n  filter(titleType == 'movie') |&gt; \n  separate_longer_delim(genres, \",\") |&gt; \n  filter(genres == \"Drama\") |&gt;\n  select(primaryTitle, success_score, averageRating, numVotes) |&gt;\n  head(10) |&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Most Successful Drama Movies\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryTitle = \"Title\",\n    success_score = \"Success Score\",\n    averageRating = \"Average Rating\",\n    numVotes = \"Number of Votes\"\n  )\n\n\n\n\n\n\n\n\nMost Successful Drama Movies\n\n\nTitle\nSuccess Score\nAverage Rating\nNumber of Votes\n\n\n\n\nThe Shawshank Redemption\n0.9648769\n9.3\n2942823\n\n\nThe Dark Knight\n0.9495972\n9.0\n2922922\n\n\nThe Godfather\n0.9477853\n9.2\n2051186\n\n\nThe Lord of the Rings: The Return of the King\n0.9371353\n9.0\n2013824\n\n\nPulp Fiction\n0.9359758\n8.9\n2260017\n\n\nFight Club\n0.9326142\n8.8\n2374722\n\n\nThe Lord of the Rings: The Fellowship of the Ring\n0.9326021\n8.9\n2043202\n\n\nForrest Gump\n0.9315685\n8.8\n2301630\n\n\nSchindler's List\n0.9267397\n9.0\n1475891\n\n\nThe Godfather Part II\n0.9246497\n9.0\n1386499\n\n\n\n\n\n\n\nWhat about the second most successful genre, Comedy?\n\n\nDisplay Code\nTITLE_RATINGS |&gt; \n  arrange(desc(success_score)) |&gt;  \n  left_join(TITLE_BASICS, join_by(\"tconst\" )) |&gt; \n  filter(titleType == 'movie') |&gt; \n  separate_longer_delim(genres, \",\") |&gt; \n  filter(genres == \"Comedy\") |&gt;\n  select(primaryTitle, success_score, averageRating, numVotes) |&gt;\n  head(5) |&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Most Successful Comedy Movies\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryTitle = \"Title\",\n    success_score = \"Success Score\",\n    averageRating = \"Average Rating\",\n    numVotes = \"Number of Votes\"\n  )\n\n\n\n\n\n\n\n\nMost Successful Comedy Movies\n\n\nTitle\nSuccess Score\nAverage Rating\nNumber of Votes\n\n\n\n\nDjango Unchained\n0.9069468\n8.5\n1729019\n\n\nBack to the Future\n0.8982525\n8.5\n1333279\n\n\nThe Wolf of Wall Street\n0.8897217\n8.2\n1620302\n\n\nThe Intouchables\n0.8867587\n8.5\n945572\n\n\nLife Is Beautiful\n0.8842202\n8.6\n754383"
  },
  {
    "objectID": "mp02.html#successful-personnel-in-the-genre",
    "href": "mp02.html#successful-personnel-in-the-genre",
    "title": "Proposing a Successful Film",
    "section": "Successful Personnel in the Genre",
    "text": "Successful Personnel in the Genre\nAs I develop my team, I want to consider the benefits of pairing an older, established actor—someone with over 20 successful titles—to lend credibility and experience, alongside an up-and-coming actor who can bring fresh energy and appeal to a younger audience. This combination can create a dynamic synergy that enhances the project’s overall impact.\nWhen working with NAME_BASICS, I can only have alive personnel on my team. Here, I am creating a new data frame called ALIVE_NAME_BASICS.\n\n\nDisplay Code\nALIVE_NAME_BASICS &lt;- NAME_BASICS |&gt;\n  filter(birthYear &gt;= 1916, is.na(deathYear)) # Can only have alive personnel on my team\nsample_n(ALIVE_NAME_BASICS, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\nI will aso identify titles with a success score greater than 0.6 in SUCCESSFUL_TITLES.\n\n\nDisplay Code\n#Successful Movies Dataset\nSUCCESSFUL_TITLES &lt;- TITLE_BASICS |&gt;\n  left_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  filter(success_score &gt;= 0.6)\nsample_n(SUCCESSFUL_TITLES, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\nEstablished Drama Actors & Actresses\n\n\nDisplay Code\n# actor\nTITLE_PRINCIPALS |&gt;\n  filter(category == \"actor\") |&gt;\n  filter(tconst %in% SUCCESSFUL_TITLES$tconst) |&gt;\n  select(tconst, nconst) |&gt;\n  inner_join(ALIVE_NAME_BASICS, by = \"nconst\") |&gt;\n  inner_join(SUCCESSFUL_TITLES, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\") |&gt;\n  separate_longer_delim(genres, \",\") |&gt; \n  filter(genres == \"Drama\") |&gt;\n  group_by(primaryName) |&gt;\n  summarise(\n    success_count = n(),\n    mean_success_score = mean(success_score, na.rm = TRUE)  # Ensure NA values are ignored\n  ) |&gt;\n  filter(success_count &gt;= 20) |&gt;\n  arrange(desc(mean_success_score), desc(success_count)) |&gt;\n  head(5)|&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Top 5 Drama Actors\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryName = \"Actor Name\",\n    success_count = \"Success Count\",\n    mean_success_score = \"Mean Success Score\"\n  )\n\n\n\n\n\n\n\n\nTop 5 Drama Actors\n\n\nActor Name\nSuccess Count\nMean Success Score\n\n\n\n\nBrad Pitt\n31\n0.7833171\n\n\nLeonardo DiCaprio\n28\n0.7801242\n\n\nChristian Bale\n38\n0.7641907\n\n\nRyan Gosling\n21\n0.7620046\n\n\nTom Hanks\n38\n0.7595574\n\n\n\n\n\n\n\n\n\nDisplay Code\n# actor\nTITLE_PRINCIPALS |&gt;\n  filter(category == \"actress\") |&gt;\n  filter(tconst %in% SUCCESSFUL_TITLES$tconst) |&gt;\n  select(tconst, nconst) |&gt;\n  inner_join(ALIVE_NAME_BASICS, by = \"nconst\") |&gt;\n  inner_join(SUCCESSFUL_TITLES, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\") |&gt;\n  separate_longer_delim(genres, \",\") |&gt; \n  filter(genres == \"Drama\") |&gt;\n  group_by(primaryName) |&gt;\n  summarise(\n    success_count = n(),\n    mean_success_score = mean(success_score, na.rm = TRUE)  # Ensure NA values are ignored\n  ) |&gt;\n  filter(success_count &gt;= 20) |&gt;\n  arrange(desc(mean_success_score), desc(success_count)) |&gt;\n  head(5)|&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Top 5 Drama Actresses\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryName = \"Actor Name\",\n    success_count = \"Success Count\",\n    mean_success_score = \"Mean Success Score\"\n  )\n\n\n\n\n\n\n\n\nTop 5 Drama Actresses\n\n\nActor Name\nSuccess Count\nMean Success Score\n\n\n\n\nScarlett Johansson\n30\n0.7471914\n\n\nFrances McDormand\n24\n0.7253185\n\n\nEmily Blunt\n23\n0.7252131\n\n\nJessica Chastain\n24\n0.7219137\n\n\nNatalie Portman\n26\n0.7205248"
  },
  {
    "objectID": "mp02.html#up-and-coming-actors-actresses",
    "href": "mp02.html#up-and-coming-actors-actresses",
    "title": "Proposing a Successful Film",
    "section": "Up and Coming Actors & Actresses",
    "text": "Up and Coming Actors & Actresses\nTo find up and coming actors, I tried a couple of ways to define the criteria for “up and coming”. What if we consider mean success score? We could find top scoring actors but with low success counts.\n\n\nDisplay Code\n# actor\nTITLE_PRINCIPALS |&gt;\n  filter(category == \"actor\") |&gt;\n  filter(tconst %in% SUCCESSFUL_TITLES$tconst) |&gt;\n  select(tconst, nconst) |&gt;\n  inner_join(ALIVE_NAME_BASICS, by = \"nconst\") |&gt;\n  inner_join(SUCCESSFUL_TITLES, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\") |&gt;\n  separate_longer_delim(genres, \",\") |&gt; \n  filter(genres == \"Drama\") |&gt;\n  group_by(primaryName) |&gt;\n  summarise(\n    success_count = n(),\n    mean_success_score = mean(success_score, na.rm = TRUE)  # Ensure NA values are ignored\n  ) |&gt;\n  arrange(desc(mean_success_score), desc(success_count)) |&gt;\n  head(5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top Scoring Actors with Low Success Counts\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryName = \"Actor Name\",\n    success_count = \"Success Count\",\n    mean_success_score = \"Mean Success Score\"\n  )\n\n\n\n\n\n\n\n\nTop Scoring Actors with Low Success Counts\n\n\nActor Name\nSuccess Count\nMean Success Score\n\n\n\n\nMichael Conner Humphreys\n1\n0.9315685\n\n\nJohn Bach\n2\n0.9303864\n\n\nSala Baker\n4\n0.9303610\n\n\nJonathan Sagall\n1\n0.9267397\n\n\nShmuel Levy\n1\n0.9267397\n\n\n\n\n\n\n\nMichael Conner Humphreys has only been in 1 film where he played young Forrest Gump. This does not seem like the best way to define up and coming. I do want someone with a higher success count. What if I limit it to between 5 and 10 successful titles.\n\n\nDisplay Code\nTITLE_PRINCIPALS |&gt;\n  filter(category == \"actor\") |&gt;\n  filter(tconst %in% SUCCESSFUL_TITLES$tconst) |&gt;\n  select(tconst, nconst) |&gt;\n  inner_join(ALIVE_NAME_BASICS, by = \"nconst\") |&gt;\n  inner_join(SUCCESSFUL_TITLES, by = \"tconst\") |&gt;\n  separate_longer_delim(genres, \",\") |&gt; \n  filter(genres == \"Drama\") |&gt;\n  filter(titleType == \"movie\", startYear &gt;=2020) |&gt;\n  group_by(primaryName) |&gt;\n  summarise(\n    success_count = n(),\n    mean_success_score = mean(success_score, na.rm = TRUE)  # Ensure NA values are ignored\n  ) |&gt;\n  filter(success_count &gt;=5 & success_count &lt;=10) |&gt; # 5 to 10 successful titles\n  arrange(desc(mean_success_score), desc(success_count)) |&gt;\n  head(10) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Actors with High Success Scores and Between 5-10 Successful Titles Since 2020\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryName = \"Actress Name\",\n    success_count = \"Success Count\",\n    mean_success_score = \"Mean Success Score\"\n  )\n\n\n\n\n\n\n\n\nActors with High Success Scores and Between 5-10 Successful Titles Since 2020\n\n\nActress Name\nSuccess Count\nMean Success Score\n\n\n\n\nTimothée Chalamet\n5\n0.8000488\n\n\nColin Farrell\n6\n0.7626842\n\n\nSahil Vaid\n5\n0.7508455\n\n\nJeffrey Wright\n5\n0.7449358\n\n\nMark Rylance\n5\n0.7401355\n\n\nAchyuth Kumar\n7\n0.7290892\n\n\nTom Hanks\n7\n0.7270197\n\n\nAjay Devgn\n9\n0.7232701\n\n\nRao Ramesh\n6\n0.7230589\n\n\nJohnny Flynn\n5\n0.7157819\n\n\n\n\n\n\n\nTom Hanks… I don’t think I could consider him an up and coming actor. Let us adjust again. It seems like the startYear lowers the success count. It only counts for recent movies and so established actors show up too because their earlier successes are not counted. Does up and coming mean young or just that their first movie was recent? I think older personell can be up and coming if they are just coming into the scene. Let’s try looking at the minimum start year of their first titles and actors with a number of successes.\n\n\nDisplay Code\nTITLE_PRINCIPALS |&gt;\n  filter(category == \"actor\") |&gt;\n  select(tconst, nconst) |&gt;\n  right_join(ALIVE_NAME_BASICS, by = \"nconst\") |&gt;\n  left_join(TITLE_BASICS, by = \"tconst\") |&gt;\n  left_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  separate_longer_delim(genres, \",\") |&gt; \n  filter(genres == \"Drama\", titleType == \"movie\") |&gt;\n  select(primaryName, primaryTitle, startYear, genres, success_score) |&gt;\n  filter(success_score &gt;= 0.6) |&gt;  # Filter by success score\n  group_by(primaryName) |&gt;  # group by actor\n  summarise(\n    debut_year = min(startYear, na.rm = TRUE),  # Compute first movie they did here\n    success_count = n(), # count succeses \n    mean_success_score = mean(success_score, na.rm = TRUE), # find mean success score\n    .groups = 'drop'\n  ) |&gt;\n  filter(debut_year &gt;= 2015, success_count &gt;= 5 & success_count &lt;= 10) |&gt;\n  arrange(desc(mean_success_score), desc(success_count)) |&gt;\n  head(5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Up and Coming Actors\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryName = \"Name\",\n    debut_year = \"Year of First Movie\",\n    success_count = \"Success Count\",\n    mean_success_score = \"Mean Success Score\"\n  )\n\n\n\n\n\n\n\n\nUp and Coming Actors\n\n\nName\nYear of First Movie\nSuccess Count\nMean Success Score\n\n\n\n\nAnthony Ramos\n2018\n5\n0.7734428\n\n\nNoah Jupe\n2017\n7\n0.7565397\n\n\nBarry Keoghan\n2017\n8\n0.7535545\n\n\nDave Bautista\n2015\n7\n0.7352465\n\n\nLeslie Odom Jr.\n2019\n5\n0.7348193\n\n\n\n\n\n\n\nSelecting Anthony Ramos, Noah Jupe, and Barry Keoghan from this list, I would consider them lesser known actors who were a part of successful films."
  },
  {
    "objectID": "mp02.html#combinations-of-personnel",
    "href": "mp02.html#combinations-of-personnel",
    "title": "Proposing a Successful Film",
    "section": "Combinations of Personnel",
    "text": "Combinations of Personnel\nActor/director pairs who have been successful together\n\n\nDisplay Code\n# Filter actors and directors first\nactors_df &lt;- TITLE_PRINCIPALS |&gt;\n  filter(category %in% c(\"actor\", \"actress\")) |&gt;\n  select(tconst, nconst)\n\ndirectors_df &lt;- TITLE_PRINCIPALS |&gt;\n  filter(category == \"director\") |&gt;\n  select(tconst, nconst)\n\n# Join actors and directors to create combinations\nactor_director_pairs &lt;- actors_df |&gt;\n  inner_join(directors_df, by = \"tconst\", suffix = c(\"_actor\", \"_director\"), relationship = \"many-to-many\")\n\nactor_director_names &lt;- actor_director_pairs |&gt;\n  inner_join(ALIVE_NAME_BASICS, by = c(\"nconst_actor\" = \"nconst\"), relationship = \"many-to-many\") |&gt; # join to names\n  rename(actor_name = primaryName) |&gt;\n  inner_join(ALIVE_NAME_BASICS, by = c(\"nconst_director\" = \"nconst\"), relationship = \"many-to-many\") |&gt; # join to names\n  rename(director_name = primaryName) |&gt;\n  inner_join(TITLE_BASICS, by = \"tconst\") |&gt; # join with titles\n  inner_join(TITLE_RATINGS, by = \"tconst\") # join with ratings\n\nactor_director_names |&gt; \n  group_by(actor_name, director_name) |&gt; # group by actor_name and director_name combinations\n  summarise(\n    success_count = n(), # count the number of times they've worked together\n    mean_success_score = mean(success_score, na.rm = TRUE)) |&gt; # find the mean success score\n  filter(mean_success_score &gt; 0.60) |&gt;\n  select(actor_name, director_name, success_count, mean_success_score) |&gt;\n  arrange(desc(success_count), desc(mean_success_score)) |&gt;\n  head(10)\n\n\n# A tibble: 10 × 4\n# Groups:   actor_name [10]\n   actor_name       director_name        success_count mean_success_score\n   &lt;chr&gt;            &lt;chr&gt;                        &lt;int&gt;              &lt;dbl&gt;\n 1 Trey Parker      Trey Parker                    888              0.669\n 2 Matt Stone       Trey Parker                    866              0.670\n 3 Masako Nozawa    Daisuke Nishio                 466              0.612\n 4 Hank Azaria      Mark Kirkland                  350              0.615\n 5 Dan Castellaneta Mark Kirkland                  346              0.614\n 6 Mona Marshall    Trey Parker                    338              0.674\n 7 Harry Shearer    Mark Kirkland                  336              0.615\n 8 Sam Marin        John Davis Infantino           301              0.685\n 9 Nancy Cartwright Mark Kirkland                  281              0.614\n10 William Salyers  John Davis Infantino           274              0.688\n\n\nI am getting mostly voice actors in the result above. My guess is this is due to episodes and tv series having a higher count. It may help to filter out the TV episodes. We can see how directors of TV series that work with the same actors multiple times may skew the success count.\n\n\nDisplay Code\nTITLE_BASICS |&gt;\n  group_by(titleType) |&gt;\n  summarize(count = n())\n\n\n# A tibble: 10 × 2\n   titleType     count\n   &lt;chr&gt;         &lt;int&gt;\n 1 movie        131662\n 2 short         16656\n 3 tvEpisode    155722\n 4 tvMiniSeries   5907\n 5 tvMovie       15007\n 6 tvSeries      29789\n 7 tvShort         410\n 8 tvSpecial      3045\n 9 video          9332\n10 videoGame      4668\n\n\nTop 10 Actor/Actress and Director Pairs in Drama Genre\n\n\nDisplay Code\nactor_director_names |&gt; \n  filter(titleType == \"movie\") |&gt; # filter for movies so results are not skewed by number of episodes\n  separate_longer_delim(genres, \",\") |&gt;\n  filter(genres == \"Drama\") |&gt; # filter by Drama\n  group_by(actor_name, director_name) |&gt; # group by actor_name and director_name combinations\n  summarise(\n    success_count = n(), # count the number of times they've worked together\n    mean_success_score = mean(success_score, na.rm = TRUE)) |&gt; # find the mean success score\n  filter(mean_success_score &gt; 0.60, success_count &gt; 5) |&gt;\n  arrange(desc(mean_success_score), desc(success_count)) |&gt;\n  head(10)\n\n\n# A tibble: 10 × 4\n# Groups:   actor_name [10]\n   actor_name          director_name     success_count mean_success_score\n   &lt;chr&gt;               &lt;chr&gt;                     &lt;int&gt;              &lt;dbl&gt;\n 1 Christian Bale      Christopher Nolan             7              0.913\n 2 Robert De Niro      Martin Scorsese               8              0.809\n 3 Ethan Hawke         Richard Linklater             7              0.766\n 4 Prabhas             S.S. Rajamouli                6              0.757\n 5 Frank Welker        Don Bluth                     6              0.757\n 6 Penélope Cruz       Pedro Almodóvar               6              0.739\n 7 Tony Leung Chiu-wai Kar-Wai Wong                  7              0.735\n 8 Woody Allen         Woody Allen                   7              0.729\n 9 Clint Eastwood      Clint Eastwood               18              0.728\n10 Megumi Hayashibara  Kazuya Tsurumaki             14              0.723\n\n\nWhat if we look at a specific director to see what actors and actresses they work well with. For example, if I wanted Steven Spielberg. Who has he worked well with in the past?\n\n\nDisplay Code\nactor_director_names |&gt; \n  filter(titleType == \"movie\") |&gt; # filter for movies so results are not skewed by number of episodes\n  filter(director_name == \"Steven Spielberg\") |&gt;\n  group_by(actor_name, director_name) |&gt; # group by actor_name and director_name combinations\n  summarise(\n    success_count = n(), # count the number of times they've worked together\n    mean_success_score = mean(success_score, na.rm = TRUE)) |&gt; # find the mean success score\n  select(actor_name, success_count, mean_success_score) |&gt;\n  arrange(desc(success_count), desc(mean_success_score)) |&gt;\n  head(5)\n\n\n# A tibble: 5 × 3\n# Groups:   actor_name [5]\n  actor_name    success_count mean_success_score\n  &lt;chr&gt;                 &lt;int&gt;              &lt;dbl&gt;\n1 Tom Hanks                 5              0.832\n2 Harrison Ford             4              0.830\n3 Mark Rylance              4              0.781\n4 Dan Aykroyd               4              0.669\n5 Simon Pegg                3              0.801\n\n\nNow let’s look at people the director has worked with at least twice.\n\n\nDisplay Code\nactor_director_names |&gt; \n  filter(titleType == \"movie\") |&gt; # filter for movies so results are not skewed by number of episodes\n  filter(director_name == \"Steven Spielberg\") |&gt;\n  group_by(actor_name, director_name) |&gt; # group by actor_name and director_name combinations\n  summarise(\n    success_count = n(), # count the number of times they've worked together\n    mean_success_score = mean(success_score, na.rm = TRUE)) |&gt; # find the mean success score\n  filter(success_count &gt;= 2) |&gt; # worked with multiple times (at least twice)\n  select(actor_name, success_count, mean_success_score) |&gt;\n  arrange(desc(mean_success_score), desc(success_count)) |&gt;\n  head(5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Actors & Actresses Steven Spielberg Worked With\"\n  ) |&gt;\n  cols_label( # display column names\n    actor_name = \"Actor/Actress\",\n    success_count = \"Success Count\",\n    mean_success_score = \"Mean Success Score\"\n  )  \n\n\n\n\n\n\n\n\nActors & Actresses Steven Spielberg Worked With\n\n\nSuccess Count\nMean Success Score\n\n\n\n\nGiovanni Ribisi\n\n\n2\n0.9076895\n\n\nVic Tablian\n\n\n2\n0.8852299\n\n\nJohn Rhys-Davies\n\n\n2\n0.8761031\n\n\nCaroline Goodall\n\n\n2\n0.8435231\n\n\nTom Hanks\n\n\n5\n0.8319899"
  },
  {
    "objectID": "mp02.html#nostalgia-and-remakes",
    "href": "mp02.html#nostalgia-and-remakes",
    "title": "Proposing a Successful Film",
    "section": "Nostalgia and Remakes",
    "text": "Nostalgia and Remakes\nThe classic movie I propose to remake is The Princess Bride. Remaking The Princess Bride, a classic with an 8 IMDb rating, taps into a well-loved story while allowing us to introduce fresh talent, including original cast cameos, to engage both longtime fans and new audiences. The original has a large number of IMDb ratings, a high average rating, and has not been remade in the past 25 years. A Princess Bride Home Movie was created during COVID, a miniseries shot on phones from home, but a full film remake would be very successful. \n\n\nDisplay Code\nTITLE_BASICS |&gt;\n  filter(primaryTitle == \"The Princess Bride\")|&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  select(primaryTitle, averageRating, numVotes, success_score) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"The Princess Bride\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryTitle = \"Title\",\n    averageRating = \"Average Rating\",\n    numVotes = \"Number of Votes\",\n    success_score = \"Success Score\"\n  )\n\n\n\n\n\n\n\n\nThe Princess Bride\n\n\nTitle\nAverage Rating\nNumber of Votes\nSuccess Score\n\n\n\n\nThe Princess Bride\n8\n456797\n0.8373338\n\n\n\n\n\n\n\nSuccessful Genres The Princess Bride is a film filled with Drama, Action, and Comedy, three of the top performing genres.\n\n\nDisplay Code\nrecent_top_genres &lt;- TITLE_BASICS |&gt;\n  left_join(TITLE_RATINGS, by = \"tconst\") |&gt; \n  filter(success_score &gt;= 0.6) |&gt; \n  separate_longer_delim(genres, \",\") |&gt; \n  filter(genres %in% c(\"Drama\", \"Comedy\", \"Action\")) |&gt;\n  group_by(genres, startYear) |&gt;  # Group by genres and startYear\n  summarise(success_count = n(), .groups = 'drop') |&gt; \n  ungroup() |&gt;\n  arrange(startYear, desc(success_count))\n\n\n\n\nDisplay Code\nggplot(recent_top_genres, aes(x = startYear, y = success_count, fill = genres)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Growth of Successful Movies by Genre Over Time\",\n    x = \"Year\",\n    y = \"Count of Successful Movies\",\n    fill = \"Genre\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  facet_wrap(~ genres, scales = \"free_y\")  # Create separate plots for each genre\n\n\n\n\n\n\n\n\n\nKey Personnel Let’s confirm whether key actors, directors, or writers from the original are still alive.\n\n\nDisplay Code\nTITLE_PRINCIPALS |&gt;\n  filter(tconst == \"tt0093779\") |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  filter(category %in% c(\"actor\", \"actress\", \"director\", \"writer\")) |&gt;\n  select(primaryName, category, characters, deathYear) |&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Key Personnel\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryName = \"Name\",\n    category = \"Role\",\n    characters = \"Characters\",\n    deathYear = \"Death Year\"\n  )\n\n\n\n\n\n\n\n\nKey Personnel\n\n\nName\nRole\nCharacters\nDeath Year\n\n\n\n\nCary Elwes\nactor\n[\"Westley\"]\nNA\n\n\nMandy Patinkin\nactor\n[\"Inigo Montoya\"]\nNA\n\n\nRobin Wright\nactress\n[\"The Princess Bride\"]\nNA\n\n\nChristopher Guest\nactor\n[\"Count Rugen\"]\nNA\n\n\nWallace Shawn\nactor\n[\"Vizzini\"]\nNA\n\n\nAndré René Roussimoff\nactor\n[\"Fezzik\"]\n1993\n\n\nFred Savage\nactor\n[\"The Grandson\"]\nNA\n\n\nPeter Falk\nactor\n[\"The Grandfather\"]\n2011\n\n\nPeter Cook\nactor\n[\"The Impressive Clergyman\"]\n1995\n\n\nRob Reiner\ndirector\n\\N\nNA\n\n\nWilliam Goldman\nwriter\n\\N\n2018\n\n\n\n\n\n\n\nProven Director &lt;a\nRenowned for his ability to capture magic in storytelling, Steven Spielberg is ideal for reimagining this beloved classic. With Steven Spielberg, we can get a balance of heartfelt storytelling and commercial success, making this project a compelling remake. His success in genres such as Drama and Action fit well with The Princess Bride.\n\n\nDisplay Code\nNAME_BASICS |&gt;\n  filter(primaryName == 'Steven Spielberg') |&gt;\n  separate_longer_delim(knownForTitles, \",\") |&gt; \n  left_join(TITLE_BASICS, by = c(\"knownForTitles\" = \"tconst\")) |&gt; \n  left_join(TITLE_RATINGS, by = c(\"knownForTitles\" = \"tconst\")) |&gt;\n  select(primaryName, primaryTitle, success_score, averageRating, numVotes) |&gt;\n  arrange(desc(success_score)) |&gt;\n  gt() |&gt; # create a display table\n  tab_header(\n    title = \"Steven Spielberg    Project Scores\"\n  ) |&gt;\n  cols_label( # display column names\n    primaryName = \"Name\",\n    primaryTitle = \"Title\",\n    success_score = \"Success Score\",\n    averageRating = \"Average Rating\",\n    numVotes = \"Number of Votes\"\n  )\n\n\n\n\n\n\n\n\nSteven Spielberg Project Scores\n\n\nName\nTitle\nSuccess Score\nAverage Rating\nNumber of Votes\n\n\n\n\nSteven Spielberg\nSchindler's List\n0.9267397\n9.0\n1475891\n\n\nSteven Spielberg\nSaving Private Ryan\n0.9076895\n8.6\n1521594\n\n\nSteven Spielberg\nRaiders of the Lost Ark\n0.8852299\n8.4\n1049518\n\n\nSteven Spielberg\nE.T. the Extra-Terrestrial\n0.8313398\n7.9\n443655\n\n\n\n\n\n\n\nSteven Spielberg has consistently directed successful movies for over 5 decades.\n\n\nDisplay Code\nspielberg_success &lt;- TITLE_PRINCIPALS |&gt;\nfilter(nconst %in% NAME_BASICS$nconst[NAME_BASICS$primaryName == \"Steven Spielberg\"]) |&gt;\n  inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  select(primaryTitle, startYear, success_score) |&gt;\n  distinct() |&gt;  # Remove duplicates\n  mutate(successful = ifelse(success_score &gt;= 0.6, 1, 0)) |&gt;\n  group_by(startYear) |&gt;\n  summarise(success_count = sum(successful), .groups = 'drop') |&gt;\n  filter(success_count &gt; 0) |&gt;  # Only keep years with successful films\n  arrange(startYear)\n\n\n\n\nDisplay Code\n# Plotting\nggplot(spielberg_success, aes(x = startYear, y = success_count)) +\n  geom_line(color = \"blue\", size = .5) +\n  geom_point(color = \"blue\", size = 1) +\n  labs(title = \"Number of Successful Films Directed by Steven Spielberg Over Time\",\n       x = \"Year\",\n       y = \"Number of Successful Films\") +\n  scale_y_continuous(limits = c(0, NA)) +  # Set y-axis to start at 0\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCasting The original actress, Robin Wright, who played The Princess Bride has previously worked with Tom Hanks in the successful film Forrest Gump. However, it might be a good idea to cast some up and coming actors and actresses as leads to attract newer audience. We can keep Wallace Shawn as Vizzini for fans and a nod towards the original classic. By casting Mille Bobby Brown as The Princess Bride and Noah Jupe as Westley. Noah is known for his performances in A Quiet Place and Honey Boy, Jupe brings a fresh energy and charm suitable for the role of Westley. Timothée Chalamet could be cast as Inigo Montoya. His performances in Dune: Part Two and Part One make him an excellent choice for the iconic role of Inigo, seeking vengeance for his father."
  },
  {
    "objectID": "mp02.html#elevator-pitch",
    "href": "mp02.html#elevator-pitch",
    "title": "Proposing a Successful Film",
    "section": "Elevator Pitch",
    "text": "Elevator Pitch\nFrom the visionary mind of Steven Spielberg comes a timeless tale of adventure and romance, The Princess Bride. Featuring the talented Noah Jupe as Westley and the dynamic Millie Bobby Brown as Buttercup, this re-imagining will delight both old fans and new. With Timothée Chalamet as the iconic Inigo Montoya and Wallace Shawn reprising his role as Vizzini, alongside the incredible Julianne Moore, this film is a story of love, bravery, and the magic of storytelling—coming soon to theaters near you!\nThis remake promises to capture the heart of the original while inviting audiences into a fresh, enchanting cinematic experience."
  },
  {
    "objectID": "NYCPSFunding.html#nyc-school-codes",
    "href": "NYCPSFunding.html#nyc-school-codes",
    "title": "New York City Public Schools Funding & Spending",
    "section": "NYC School Codes",
    "text": "NYC School Codes\nA New York City public school’s school code is a six-digit number that identifies the school: - First two digits: Represent the school district\n\nThird character: Indicates the borough where the school is located\nFinal three digits: Unique within the borough\n\nThe borough codes are:\n\nM: Manhattan\nX: Bronx\nR: Staten Island\nK: Brooklyn\nQ: Queens"
  },
  {
    "objectID": "NYCPSFunding.html#foundation-aid",
    "href": "NYCPSFunding.html#foundation-aid",
    "title": "New York City Public Schools Funding & Spending",
    "section": "Foundation Aid",
    "text": "Foundation Aid\nNew York follows the public school funding formula — known as “Foundation Aid”. The goal is to ensure adequate funding for all school districts. Broadly speaking, the Foundation Aid formula multiplies the number of students in a district by the amount of funding needed in that district. The formula estimates the total current-year enrollment and assigns heigher weights to students with disabilities and students enrolled in special education programs. Districts with higher enrollment or more pupils in poverty would see higher Foundation Aid allocations. While, lower poverty districts would see lower Foundation Aid, but not below $500 per pupil.3\n\n\nCode\nif (!require(\"readr\")) install.packages(\"readr\")\nif (!require(\"sf\")) install.packages(\"sf\")\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nif (!require(\"tidyr\")) install.packages(\"tidyr\")\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"DT\")) install.packages(\"DT\")\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"gt\")) install.packages(\"gt\")\nif (!require(\"plotly\")) install.packages(\"plotly\")\nif (!require(\"purrr\")) install.packages(\"purrr\")\nif (!require(\"httr2\")) install.packages(\"httr2\")\nif (!require(\"gganimate\")) install.packages(\"gganimate\")\nif (!require(\"rvest\")) install.packages(\"rvest\")\nif (!require(\"readxl\")) install.packages(\"readxl\")\nif (!require(\"scales\")) install.packages(\"scales\")\nif (!require(\"reshape2\")) install.packages(\"reshape2\")\nif (!require(\"viridis\")) install.packages(\"viridis\")\nif (!require(\"corrplot\")) install.packages(\"corrplot\")\nif (!require(\"caret\")) install.packages(\"caret\")\nif (!require(\"ggpmisc\")) install.packages(\"ggpmisc\")\n\nlibrary(readr)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(plotly)\nlibrary(purrr)\nlibrary(httr2)\nlibrary(gganimate)\nlibrary(rvest)\nlibrary(readxl)\nlibrary(scales)\nlibrary(reshape2)\nlibrary(viridis)\nlibrary(corrplot)\nlibrary(caret)\nlibrary(ggpmisc)\n\n\n\nEconomically Disadvantaged\nThe ECON Status in the data set is marked by the percentage of students who are economically disadvantaged. New York uses reduced-price lunch eligibility as a basis for measuring student poverty. First implemented in 2007, Foundation Aid uses decades-old data to calculate district needs, like relying on figures from the 2000 Census to measure student poverty. Other factors that impact district spending, including the number of students living in temporary housing, don’t weigh into the current formula at all. 4"
  },
  {
    "objectID": "NYCPSFunding.html#academic-performance",
    "href": "NYCPSFunding.html#academic-performance",
    "title": "New York City Public Schools Funding & Spending",
    "section": "Academic Performance",
    "text": "Academic Performance\nInfoHub Data From the InfoHub site, I am loading the following data files:\n\nGRAD_RATES\nDEMOGRAPHICS\nREGENTS\nREGENTS_CS\n\n\n\nCode\n# Function to Get Data Files from NYCED URL\nget_nycps_file &lt;- function(fname, sheet){\n  BASE_URL &lt;- \"https://infohub.nyced.org/docs/default-source/default-document-library/\"\n  fname_ext &lt;- paste0(fname, \".xlsx\")\n  if(!file.exists(fname_ext)){\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL, \n                  destfile = fname_ext)\n  }\n  as.data.frame(read_excel(fname_ext, sheet = sheet))\n}\n\nGRAD_RATES &lt;- get_nycps_file(\"2023-graduation-rates-public-school\", \"All\")\nDEMOGRAPHICS &lt;- get_nycps_file(\"demographic-snapshot-2019-20-to-2023-24-public\", \"School\")\nREGENTS &lt;- get_nycps_file(\"2014-15-to-2022-23-nyc-regents-overall-and-by-category---public\", \"All Students\")\n\n\nTo clean the data from, I am converting characters to the numeric data type, replacing ‘s’ with NAs, and calculating averages for graduation rates.\n\n\nCode\n# Regents & Grad Rates\n# Replace 's' and 'na' with NA for all columns in both datasets\nREGENTS &lt;- REGENTS |&gt; mutate(across(where(is.character), ~ na_if(na_if(., 's'), 'na')))\nREGENTS &lt;- REGENTS |&gt; mutate(across(c(`Mean Score`, `Number Scoring Below 65`, `Percent Scoring Below 65`, \n                                      `Number Scoring 65 or Above`, `Percent Scoring 65 or Above`,\n                                      `Number Scoring 80 or Above`, `Percent Scoring 80 or Above`,\n                                      `Number meeting CUNY proficiency requirmenets`, \n                                      `Percent meeting CUNY proficiency requirmenets`), ~ as.numeric(.)))\nREGENTS &lt;- REGENTS |&gt; rename(DBN = `School DBN`)\nGRAD_RATES &lt;- GRAD_RATES |&gt; mutate(across(where(is.character), ~ na_if(na_if(., 's'), 'na')))\nGRAD_RATES &lt;- GRAD_RATES |&gt; mutate(across(c(`# Grads`, `% Grads`, `# Total Regents`, \n                                      `% Total Regents of Cohort`, `% Total Regents of Grads`,\n                                      `# Advanced Regents`, `% Advanced Regents of Cohort`,\n                                      `% Advanced Regents of Grads`, `# Regents without Advanced`,\n                                      `% Regents without Advanced of Cohort`, `% Regents without Advanced of Grads`,\n                                      `# Local`, `% Local of Cohort`, `% Local of Grads`, `# Still Enrolled`,\n                                      `% Still Enrolled`, `# Dropout`, `% Dropout`), ~ as.numeric(.)))\n\n# Calculate the averages for the relevant columns\ngrad_rates_avg &lt;- GRAD_RATES |&gt;\n  group_by(DBN, `Cohort Year`) |&gt;\n  summarize( # Group by school and calculate the average for relevant columns\n    avg_total_cohort = mean(`# Total Cohort`, na.rm = TRUE),\n    avg_grads = mean(`# Grads`, na.rm = TRUE),\n    avg_pct_grads = mean(`% Grads`, na.rm = TRUE),\n    avg_total_regents = mean(`# Total Regents`, na.rm = TRUE),\n    avg_pct_regents_cohort = mean(`% Total Regents of Cohort`, na.rm = TRUE),\n    avg_pct_regents_grads = mean(`% Total Regents of Grads`, na.rm = TRUE),\n    avg_advanced_regents = mean(`# Advanced Regents`, na.rm = TRUE),\n    avg_pct_advanced_regents_cohort = mean(`% Advanced Regents of Cohort`, na.rm = TRUE),\n    avg_pct_advanced_regents_grads = mean(`% Advanced Regents of Grads`, na.rm = TRUE),\n    avg_regents_without_advanced = mean(`# Regents without Advanced`, na.rm = TRUE),\n    avg_pct_regents_without_advanced_cohort = mean(`% Regents without Advanced of Cohort`, na.rm = TRUE),\n    avg_pct_regents_without_advanced_grads = mean(`% Regents without Advanced of Grads`, na.rm = TRUE),\n    avg_local = mean(`# Local`, na.rm = TRUE),\n    avg_pct_local_cohort = mean(`% Local of Cohort`, na.rm = TRUE),\n    avg_pct_local_grads = mean(`% Local of Grads`, na.rm = TRUE),\n    avg_still_enrolled = mean(`# Still Enrolled`, na.rm = TRUE),\n    avg_pct_still_enrolled = mean(`% Still Enrolled`, na.rm = TRUE),\n    avg_dropout = mean(`# Dropout`, na.rm = TRUE),\n    avg_pct_dropout = mean(`% Dropout`, na.rm = TRUE)\n  )\n\n\nCity of New York Data\n\nMATH_06_23_ALL\nELA_06_23_ALL\nECON_MATH_ELA_13_23\n\nHere I’ve created a function to pull exam result data from the City of New York.\n\n\nCode\n# Function to Get City of New York Data Files from cityofnewyork URL\nget_cony_file &lt;- function(fname, zname, FILE_URL) {\n  if(!file.exists(fname)){\n    if(!file.exists(zname)){ \n      download.file(FILE_URL, destfile = zname) \n    }\n    td &lt;- tempdir()\n    zip_contents &lt;- unzip(zname, exdir = td)\n    file_path &lt;- file.path(td, fname) \n  }\n  as.data.frame(read.csv(file_path))\n}\n\n\nBelow, I use the function to download data for ELA scores for 2006-2012 and 2013-2019, ELA scores with Economic status for 2013-2017, ELA scores on a district level for 2006-2011 and 2013-2018.\n\n\nCode\n# define URL and zip file name\nELA_URL_2006 &lt;- \"https://data.cityofnewyork.us/api/views/9uqq-k6t8/files/6cb21873-4c19-42ab-aa31-13573b47ed45?download=true&filename=2006-2012%20ELA%20Test%20Results.zip\"\nELA_2006 &lt;- \"2006-2012_ELA_Test_Results.zip\" # zip folder name\n# there is no Economic data file in the zip for 2006-2012\n\nELA_2006_12_ALL &lt;- get_cony_file(\"2006-2011_English_Language_Arts__ELA__Test_Results_by_Grade_-_School_level_-_All_Students.csv\", ELA_2006, ELA_URL_2006)\n\n# ELA 2013-2019\nELA_URL_2013_19 &lt;- \"https://data.cityofnewyork.us/api/views/iebs-5yhr/files/a6ff1ea4-4895-4372-a2d6-bbd0b17e89a4?download=true&filename=2013-2019%20ELA%20Test%20Results.zip\"\nELA_2013_19 &lt;- \"2013-2019_ELA_Test_Results.zip\" # zip folder name\nELA_2013_17_ECON &lt;- get_cony_file(\"2013-2017_School_ELA_Results_-_Economic.csv\", ELA_2013_19, ELA_URL_2013_19)\n\ndistrict_ELA_1318 &lt;- get_cony_file(\"2013-2018_District_ELA_Results.csv\", ELA_2013_19, ELA_URL_2013_19)\ndistrict_ELA_0611 &lt;- get_cony_file(\"2006-2011_English_Language_Arts__ELA__Test_Results_by_Grade_-_District_-_All_Students.csv\", ELA_2006, ELA_URL_2006)\n\n\nHere, I define a function for ELA scores from 2013-2023. ELA 2013-2023 ALL\n\n\nCode\n# ELA 2013-2023 Function\nget_ela_file &lt;- function(fname, zname, sheet, FILE_URL){\n  if(!file.exists(fname)){\n    if(!file.exists(zname)){ \n      download.file(FILE_URL, destfile = zname, mode = \"wb\")\n    }\n    td &lt;- tempdir()\n    zip_contents &lt;- unzip(zname, exdir = td) \n    file_path &lt;- file.path(td, fname)\n  }\n  as.data.frame(read_excel(file_path, sheet = sheet))\n}\n# increase timeout time for zip download\noptions(timeout = 1000)\n\nELA_URL_2013_23 &lt;- \"https://data.cityofnewyork.us/api/views/iebs-5yhr/files/33712362-a710-4640-a740-f6dc2c8ac991?download=true&filename=2013-2023%20ELA%20Test%20Results%20(Excel%20files).zip\"\nELA_2013_23 &lt;- \"2013-2023_ELA_Test_Results__Excel_files_.zip\" # zip folder name\nELA_2013_23_ALL &lt;- get_ela_file(\"school-ela-results-2013-2023-(public).xlsx\", ELA_2013_23, \"All\", ELA_URL_2013_23)\n\n\nNext, I’m renaming the columns of ELA_2006_2012_ALL so that the column names are consistent across the ELA score data sets. This will help with binding the datasets and my analysis later on.\n\n\nCode\n# Rename columns\nELA_2006_12_ALL &lt;- ELA_2006_12_ALL |&gt;\n  rename(\n    `Demographic` = `Category`,              \n    `Num.Level.1` = `Level.1..`,\n    `Pct.Level.1` = `Level.1...1`,\n    `Num.Level.2` = `Level.2..`,\n    `Pct.Level.2` = `Level.2...1`,\n    `Num.Level.3` = `Level.3..`,\n    `Pct.Level.3` = `Level.3...1`,\n    `Num.Level.4` = `Level.4..`,\n    `Pct.Level.4` = `Level.4...1`,\n    `Num.Level.3.and.4` = `Level.3.4..`,\n    `Pct.Level.3.and.4` = `Level.3.4...1`  \n  )\nELA_2013_23_ALL &lt;- ELA_2013_23_ALL |&gt;\n  rename(\n    `Demographic` = `Category`,              \n    `Number.Tested` = `Number Tested`,\n    `Mean.Scale.Score` = `Mean Scale Score`,\n    `Num.Level.1` = `# Level 1`,\n    `Pct.Level.1` = `% Level 1`,\n    `Num.Level.2` = `# Level 2`,\n    `Pct.Level.2` = `% Level 2`,\n    `Num.Level.3` = `# Level 3`,\n    `Pct.Level.3` = `% Level 3`,\n    `Num.Level.4` = `# Level 4`,\n    `Pct.Level.4` = `% Level 4`,\n    `Num.Level.3.and.4` = `# Level 3+4`,\n    `Pct.Level.3.and.4` = `% Level 3+4`  \n  )  \n\n\nAdditionally, I am ensuring that columns such as the number, percent, and mean are numeric instead of characters. This will introduce NA where the value is ‘s’. After this, I can bind the ELA 2006-2012 and 2013-2023 data sets together.\n\n\nCode\nELA_2013_23_ALL &lt;- ELA_2013_23_ALL |&gt;\n  mutate(across(\n    .cols = starts_with('Num') | starts_with('Pct') | starts_with('Mean'), \n    .fns = ~ as.numeric(.),        \n    .names = '{.col}'                        \n  )) # introduces NA where value was \"s\" \n\nELA_2006_12_ALL &lt;- ELA_2006_12_ALL |&gt;\n  mutate(`School Name` = NA)\n\nELA_2006_23_ALL &lt;- bind_rows(\n  ELA_2006_12_ALL,\n  ELA_2013_23_ALL)\n\n\nBelow, I am loading in the Math 2006-2012, Math 2013-2023, and district level Math 2013-2023 data.\n\n\nCode\n# Math 2006 - 2012 download from github link\nget_mathall_file &lt;- function(fname){\n  BASE_URL &lt;- \"https://raw.githubusercontent.com/cndy17/STA9750-2024-FALL/main/2006_-_2012__Math_Test_Results__-_All_Students\"\n  fname_ext &lt;- paste0(fname, \".csv\")\n  if(!file.exists(fname_ext)){\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL, \n                  destfile = fname_ext)\n  }\n  as.data.frame(readr::read_csv(fname_ext, lazy=FALSE))\n}\n\nMATH_2006_12_ALL &lt;- get_mathall_file(\"2006_-_2012__Math_Test_Results__-_All_Students\")\n\n# Math 2013- 2023\nget_math1323_file &lt;- function(fname, zname, sheet, FILE_URL){\n  if(!file.exists(fname)){\n    if(!file.exists(zname)){ \n      download.file(FILE_URL, destfile = zname, mode = \"wb\")\n    }\n    td &lt;- tempdir()\n    zip_contents &lt;- unzip(zname, exdir = td) \n    file_path &lt;- file.path(td, fname)\n  }\n  as.data.frame(read_excel(file_path, sheet = sheet))\n}\nMATH_URL_2013_23 &lt;-\"https://data.cityofnewyork.us/api/views/74kb-55u9/files/ca3b8584-9b64-472c-ace4-683e795c12e2?download=true&filename=2013-2023%20Math%20Test%20Results%20(Excel%20files).zip\"\nMATH_2013_23 &lt;- \"2013-2023_Math_Test_Results__Excel_files_.zip\"\nMATH_2013_23_ALL &lt;- get_math1323_file(\"04_school-math-results-2013-2023-(public).xlsx\", MATH_2013_23, \"All\", MATH_URL_2013_23)\n\n\ndistrict_MATH_1323&lt;- get_math1323_file(\"03_district-math-results-2013-2023-(public).xlsx\", MATH_2013_23, \"All\", MATH_URL_2013_23)\n\n\nTo clean the math data, like with the ELA data, I am renaming the columns to ensure consistency across the data sets. I am also binding the Math 2006-2012 and 2013-2023 data sets together and converting to the appropriate data types.\n\n\nCode\n# Bind 2006-2012 with 2013-2023\n# Math\nMATH_2006_12_ALL &lt;- MATH_2006_12_ALL |&gt;\n  mutate(`School Name` = NA) # add column to match 2013-23 df\n\nMATH_2013_23_ALL &lt;- MATH_2013_23_ALL |&gt;\n  rename(\n    `Grade` = `Grade`,                 \n    `Demographic` = `Category`,              \n    `Number.Tested` = `Number Tested`,\n    `Mean.Scale.Score` = `Mean Scale Score`,\n    `Num.Level.1` = `# Level 1`,\n    `Pct.Level.1` = `% Level 1`,\n    `Num.Level.2` = `# Level 2`,\n    `Pct.Level.2` = `% Level 2`,\n    `Num.Level.3` = `# Level 3`,\n    `Pct.Level.3` = `% Level 3`,\n    `Num.Level.4` = `# Level 4`,\n    `Pct.Level.4` = `% Level 4`,\n    `Num.Level.3.and.4` = `# Level 3+4`,\n    `Pct.Level.3.and.4` = `% Level 3+4`  \n  )\n\nMATH_2006_23_ALL &lt;- bind_rows(\n  MATH_2006_12_ALL,\n  MATH_2013_23_ALL) # remember School Name is NA from 2006-12\n\n# convert character columns to numeric\nMATH_2006_23_ALL &lt;- MATH_2006_23_ALL |&gt;\n  mutate(across(\n    .cols = starts_with('Num') | starts_with('Pct') | starts_with('Mean'), \n    .fns = ~ as.numeric(.),        \n    .names = '{.col}'                            \n  ))\n\n\nLooking through the ELA and Math datasets for all students, I noticed that the Mean.Scale.Score for rows where Grade is “All Grades” is NA. However, we can find the Mean.Scale.Score for “All Grades” manually by calculating the average of the Mean.Scale.Score for each DBN and Year.\n\n\nCode\n# Calculate the averages for the relevant columns (for Math and ELA)\n# For MATH (Calculating means by DBN and Year)\nMATH_06_23_ALL_avg &lt;- MATH_2006_23_ALL |&gt;\n  filter(Grade != 'All Grades') |&gt;\n  group_by(DBN, Year) |&gt;\n  summarise(\n    Pct.Level.1 = mean(`Pct.Level.1`, na.rm = TRUE),\n    Pct.Level.2 = mean(`Pct.Level.2`, na.rm = TRUE),\n    Pct.Level.3 = mean(`Pct.Level.3`, na.rm = TRUE),\n    Mean.Scale.Score = mean(`Mean.Scale.Score`, na.rm = TRUE),\n    Number.Tested = sum(`Number.Tested`, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\n# Add the \"All Grades\" row for Math\nMATH_06_23_ALL_with_avg &lt;- bind_rows(MATH_2006_23_ALL, MATH_06_23_ALL_avg) |&gt;\n  mutate(Grade = if_else(Grade == \"All Grades\", \"All Grades\", Grade))\n\n# Replace NA values in the \"All Grades\" row with the computed averages\nMATH_06_23_ALL &lt;- MATH_06_23_ALL_with_avg |&gt;\n  mutate(\n    Pct.Level.1 = ifelse(is.na(Pct.Level.1) & Grade == \"All Grades\", mean(Pct.Level.1, na.rm = TRUE), Pct.Level.1),\n    Pct.Level.2 = ifelse(is.na(Pct.Level.2) & Grade == \"All Grades\", mean(Pct.Level.2, na.rm = TRUE), Pct.Level.2),\n    Pct.Level.3 = ifelse(is.na(Pct.Level.3) & Grade == \"All Grades\", mean(Pct.Level.3, na.rm = TRUE), Pct.Level.3),\n    Mean.Scale.Score = ifelse(is.na(Mean.Scale.Score) & Grade == \"All Grades\", mean(Mean.Scale.Score, na.rm = TRUE), Mean.Scale.Score),\n    Number.Tested = ifelse(is.na(Number.Tested) & Grade == \"All Grades\", sum(Number.Tested, na.rm = TRUE), Number.Tested)\n  )\n\n# For ELA (Calculating means by DBN and Year)\nELA_06_23_ALL_avg &lt;- ELA_2006_23_ALL |&gt;\n  filter(Grade != 'All Grades') |&gt;\n  group_by(DBN, Year) |&gt;\n  summarise(\n    Pct.Level.1 = mean(`Pct.Level.1`, na.rm = TRUE),\n    Pct.Level.2 = mean(`Pct.Level.2`, na.rm = TRUE),\n    Pct.Level.3 = mean(`Pct.Level.3`, na.rm = TRUE),\n    Mean.Scale.Score = mean(`Mean.Scale.Score`, na.rm = TRUE),\n    Number.Tested = sum(`Number.Tested`, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\n# Add the \"All Grades\" row for ELA\nELA_06_23_ALL_with_avg &lt;- bind_rows(ELA_2006_23_ALL, ELA_06_23_ALL_avg) |&gt;\n  mutate(Grade = if_else(Grade == \"All Grades\", \"All Grades\", Grade))\n\n# Replace NA values in the \"All Grades\" row with the computed averages for ELA\nELA_06_23_ALL&lt;- ELA_06_23_ALL_with_avg |&gt;\n  mutate(\n    Pct.Level.1 = ifelse(is.na(Pct.Level.1) & Grade == \"All Grades\", mean(Pct.Level.1, na.rm = TRUE), Pct.Level.1),\n    Pct.Level.2 = ifelse(is.na(Pct.Level.2) & Grade == \"All Grades\", mean(Pct.Level.2, na.rm = TRUE), Pct.Level.2),\n    Pct.Level.3 = ifelse(is.na(Pct.Level.3) & Grade == \"All Grades\", mean(Pct.Level.3, na.rm = TRUE), Pct.Level.3),\n    Mean.Scale.Score = ifelse(is.na(Mean.Scale.Score) & Grade == \"All Grades\", mean(Mean.Scale.Score, na.rm = TRUE), Mean.Scale.Score),\n    Number.Tested = ifelse(is.na(Number.Tested) & Grade == \"All Grades\", sum(Number.Tested, na.rm = TRUE), Number.Tested)\n  )\n\nMATH_06_23_ALL &lt;- MATH_06_23_ALL |&gt;\n  filter(`Grade` == \"All Grades\")\n\nELA_06_23_ALL &lt;- ELA_06_23_ALL |&gt;\n  filter(`Grade` == \"All Grades\")\n\n\nFor the MLA and ELA scores broken down by Economic Status, again, I am downloading the data and renaming the columns to ensure consistency across the data sets. I am also joining the Math and ELA data sets together and converting the columns to the appropriate data types.\n\n\nCode\nELA_2013_23_ECON &lt;- get_ela_file(\"school-ela-results-2013-2023-(public).xlsx\", ELA_2013_23, \"Econ Status\", ELA_URL_2013_23)\nMATH_2013_23_ECON &lt;- get_math1323_file(\"04_school-math-results-2013-2023-(public).xlsx\", MATH_2013_23, \"Econ Status\", MATH_URL_2013_23)\n\nELA_2013_23_ECON &lt;- ELA_2013_23_ECON |&gt;\n  rename(\n    `Demographic` = `Category`,              \n    `Number.Tested` = `Number Tested`,\n    `Mean.Scale.Score` = `Mean Scale Score`,\n    `Num.Level.1` = `# Level 1`,\n    `Pct.Level.1` = `% Level 1`,\n    `Num.Level.2` = `# Level 2`,\n    `Pct.Level.2` = `% Level 2`,\n    `Num.Level.3` = `# Level 3`,\n    `Pct.Level.3` = `% Level 3`,\n    `Num.Level.4` = `# Level 4`,\n    `Pct.Level.4` = `% Level 4`,\n    `Num.Level.3.and.4` = `# Level 3+4`,\n    `Pct.Level.3.and.4` = `% Level 3+4`  \n  )  \n\nMATH_2013_23_ECON &lt;- MATH_2013_23_ECON |&gt;\n  rename(\n    `Demographic` = `Category`,              \n    `Number.Tested` = `Number Tested`,\n    `Mean.Scale.Score` = `Mean Scale Score`,\n    `Num.Level.1` = `# Level 1`,\n    `Pct.Level.1` = `% Level 1`,\n    `Num.Level.2` = `# Level 2`,\n    `Pct.Level.2` = `% Level 2`,\n    `Num.Level.3` = `# Level 3`,\n    `Pct.Level.3` = `% Level 3`,\n    `Num.Level.4` = `# Level 4`,\n    `Pct.Level.4` = `% Level 4`,\n    `Num.Level.3.and.4` = `# Level 3+4`,\n    `Pct.Level.3.and.4` = `% Level 3+4`  \n  )  \n\n# Full Join Math and ELA results with Economic Demographics\nMATH_ELA_2013_23_ECON &lt;- MATH_2013_23_ECON |&gt;\n  full_join(ELA_2013_23_ECON, by = c(\"DBN\", \"Year\", \"Grade\", \"Demographic\", \"School Name\"), \n            suffix = c(\".math\", \".ela\"))\n\n# Convert character to numeric data type\nMATH_ELA_2013_23_ECON &lt;- MATH_ELA_2013_23_ECON |&gt;\n  mutate(across(\n    .cols = starts_with('Num') | starts_with('Pct') | starts_with('Mean'), \n    .fns = ~ as.numeric(.),        \n    .names = '{.col}'                         \n  )) # introduces NA where value was \"s\" \n\nECON_MATH_ELA_13_23 &lt;- MATH_ELA_2013_23_ECON |&gt;\n  filter(`Grade` == \"All Grades\")\n\n\nELA and MATH Test Score Distribution By Student Economic Demographic\n\n\nCode\n# Faceted Math Score Plot by Demographic\nggplot(ECON_MATH_ELA_13_23, aes(x = `Mean.Scale.Score.math`)) +\n  geom_histogram(binwidth = 5, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Distribution of Math Scores by Economic Demographic\", x = \"Math Score\", y = \"Frequency\") +\n  theme_minimal() +\n  facet_wrap(~ Demographic, scales = \"free_y\")  # Facet by Demographic with independent y-axis scales\n\n\n\n\n\n\n\n\n\nCode\nggplot(ECON_MATH_ELA_13_23, aes(x = `Mean.Scale.Score.ela`)) +\n  geom_histogram(binwidth = 5, fill = \"lightcoral\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Distribution of ELA Scores by Economic Demographic\", x = \"ELA Score\", y = \"Frequency\") +\n  theme_minimal() +\n  facet_wrap(~ Demographic, scales = \"free_y\")"
  },
  {
    "objectID": "NYCPSFunding.html#school-funding",
    "href": "NYCPSFunding.html#school-funding",
    "title": "New York City Public Schools Funding & Spending",
    "section": "School Funding",
    "text": "School Funding\nSchool Funding Transparency Data\n\nfunding_C_201823\n\nNYC publishes School Funding Transparency Data for 2017-2019.\n\n\nCode\n# Downloading School Funding Transparency Data 2017-2019\nget_201719_funds &lt;- function(zname, fname, sheet){\n  BASE_URL &lt;- \"https://nycenet.edu/documents/NYS/\"\n  zname_ext &lt;- paste0(zname, \".zip\")\n  FILE_URL &lt;- paste0(BASE_URL, zname_ext)\n  if(!file.exists(zname_ext)) {\n    download.file(FILE_URL, destfile = zname_ext, mode = \"wb\")\n  }\n  td &lt;- tempdir()\n  unzip(zname_ext, exdir = td)\n  xlsx_file &lt;- file.path(td, paste0(fname, \".xlsx\"))\n  as.data.frame(read_excel(xlsx_file, sheet = sheet))\n  if (!file.exists(xlsx_file)) {\n    unzip(zname_ext, exdir = td)\n  }\n  # Read the specified sheet from the Excel file, skipping the first 5 rows because they are blank and setting the 6th row as the header\n  as.data.frame(read_excel(xlsx_file, sheet = sheet, skip = 6))\n}\n\n# Downloading School Funding Transparency Data 2019-2023\nget_funding_file &lt;- function(zname, fname, sheet){\n  BASE_URL &lt;- \"https://infohub.nyced.org/docs/default-source/default-document-library/\"\n  zname_ext &lt;- paste0(zname, \".zip\")\n  FILE_URL &lt;- paste0(BASE_URL, zname_ext)\n  if(!file.exists(zname_ext)) {\n    download.file(FILE_URL, destfile = zname_ext, mode = \"wb\")\n  }\n  td &lt;- tempdir()\n  unzip(zname_ext, exdir = td)\n  xlsx_file &lt;- file.path(td, paste0(fname, \".xlsx\"))\n  as.data.frame(read_excel(xlsx_file, sheet = sheet))\n  if (!file.exists(xlsx_file)) {\n      unzip(zname_ext, exdir = td)\n    }\n  # Read the specified sheet from the Excel file, skipping the first 5 rows because they are blank and setting the 6th row as the header\n as.data.frame(read_excel(xlsx_file, sheet = sheet, skip = 6))\n}\n\n# use functions to get data files\nfunding_201718_C &lt;- get_201719_funds(\"NewYorkCitySchoolTransparency201718\", \"NewYorkCitySchoolTransparency201718 A-E web\", \"Part-C\")\nfunding_201819_C &lt;- get_201719_funds(\"NewYorkCitySchoolTransparency201819\", \"NewYorkCitySchoolTransparency201819 A-E web\", \"Part-C\")\nfunding_201920_C &lt;- get_funding_file(\"newyorkcityschooltransparency201920\", \"NewYorkCitySchoolTransparency201920 A-E web\", \"Part C\")\nfunding_202021_C &lt;- get_funding_file(\"newyorkcityschooltransparency202021\", \"NewYorkCitySchoolTransparency202021 A-E web\", \"Part C\")\nfunding_202122_C &lt;- get_funding_file(\"newyorkcityschooltransparency202122\", \"NewYorkCitySchoolTransparency202122 A-E web\", \"Part C\")\nfunding_202223_C &lt;- get_funding_file(\"newyorkcityschooltransparency202223\", \"NewYorkCitySchoolTransparency202223\", \"Part C\")\n\n# combine into one dataset for Part C\nfunding_C_201823 &lt;- bind_rows(\n  funding_201718_C |&gt;  mutate(Year = 2018),\n  funding_201819_C |&gt;  mutate(Year = 2019),\n  funding_201920_C |&gt;  mutate(Year = 2020),\n  funding_202021_C |&gt;  mutate(Year = 2021),\n  funding_202122_C |&gt;  mutate(Year = 2022),\n  funding_202223_C |&gt;  mutate(Year = 2023),\n)\nfunding_C_201823 &lt;- funding_C_201823 |&gt;\n  rename(DBN = `Local School Code`)\n\n\nCharter Schools Moving on to charter schools. Charter schools are publicly funded but operate independently. While the range of scores remains similar, charter schools tend to have lower median scores compared to district-run schools. While they need to meet certain performance goals, they receive less per-pupil funding than district schools, which can impact their ability to offer additional resources and support for students. To look at charter school performance, we will use the following datasets:\n\nREGENTS_CS\nELA_2013_17_CS\nMATH_2013_17_CS\nAll_CS_combined\n\n\n\nCode\n# list of charter schools: https://www.nysed.gov/charter-schools/charter-schools-directory \ncs_url &lt;- \"https://www.nysed.gov/charter-schools/charter-schools-directory\"\ncs &lt;- read_html(\"https://www.nysed.gov/charter-schools/charter-schools-directory\")\ncharter_schools &lt;- cs |&gt; \n  html_elements(\".rtecenter:nth-child(1)\") \ncharter_schools_text &lt;- charter_schools |&gt;\n  html_text()\ncharter_schools_df &lt;- data.frame(\n  school_name = charter_schools_text,\n  stringsAsFactors = FALSE\n)\n\nREGENTS_CS &lt;- REGENTS |&gt; filter(str_detect(`School Name`, 'Charter'))\n\n\nELA and Math scores for Charter Schools is available through the City of New York. I will download the data and clean it up using the functions defined previously.\n\n\nCode\n# Load Charter School Data\nELA_2006_12_CS &lt;- get_cony_file(\"2006-2012_English_Language_Arts__ELA__Test_Results_-_Charter_Schools.csv\", ELA_2006, ELA_URL_2006)\nELA_2013_17_CS &lt;- get_cony_file(\"2013-2017_Charter_School_ELA_Results_-_All.csv\", ELA_2013_19, ELA_URL_2013_19)\n\n# CharterSchool for 2012-2017\nget_math_file &lt;- function(fname, zname, FILE_URL) {\n  if(!file.exists(fname)){\n    if(!file.exists(zname)){ \n      download.file(FILE_URL, destfile = zname) \n    }\n    td &lt;- tempdir()\n    folder_path &lt;- \"2006-2012 Math Test Results\"\n    zip_contents &lt;- unzip(zname, exdir = td)\n    file_path &lt;- file.path(td, folder_path, fname) \n  }\n  as.data.frame(read.csv(file_path))\n}\n\nget_math_file_1317 &lt;- function(fname, zname, FILE_URL) {\n  if(!file.exists(fname)){\n    if(!file.exists(zname)){ \n      download.file(FILE_URL, destfile = zname) \n    }\n    td &lt;- tempdir()\n    folder_path &lt;- \"2013-2017 Math Test Results\"\n    zip_contents &lt;- unzip(zname, exdir = td)\n    file_path &lt;- file.path(td, folder_path, fname) \n  }\n  as.data.frame(read.csv(file_path))\n}\n\nMATH_URL_2006_12 &lt;- \"https://data.cityofnewyork.us/api/views/e5c5-ieuv/files/93fd3270-8d2e-47ac-878d-04d96e411168?download=true&filename=2006-2012%20Math%20Test%20Results.zip\"\nMATH_2006_12 &lt;- \"2006-2012_Math_Test_Results.zip\"\nMATH_2006_12_CS &lt;- get_math_file(\"2006_-_2012_Math_Test_Results_-_Charter_Schools.csv\", MATH_2006_12, MATH_URL_2006_12)\n\nMATH_URL_2013_17 &lt;- \"https://data.cityofnewyork.us/api/views/74kb-55u9/files/8c17d960-19d2-4c68-b18a-f02c20f3e32e?download=true&filename=2013-2017%20Math%20Test%20Results.zip\"\nMATH_2013_17 &lt;- \"2013-2017_Math_Test_Results.zip\"\nMATH_2013_17_CS &lt;- get_math_file_1317(\"2013-2017_Charter_School_Math_Results_-_All.csv\", MATH_2013_17, MATH_URL_2013_17)\n\n\nTo clean the Charter School data, I will rename the columns to ensure consistency across the data sets. I will also convert the columns to the appropriate data types as we did with the other data sets.\n\n\nCode\n# only have CS until 2017\nrename_2013_17_CS &lt;- function(dataframe) {\n  dataframe &lt;- dataframe|&gt;\n  rename(\n    `Demographic` = `Category`,    \n    `Num.Level.1` = `Level1_N`,\n    `Pct.Level.1` = `Level1_.`,\n    `Num.Level.2` = `Level2_N`,\n    `Pct.Level.2` = `Level2_.`,\n    `Num.Level.3` = `Level3_N`,\n    `Pct.Level.3` = `Level3_.`,\n    `Num.Level.4` = `Level4_N`,\n    `Pct.Level.4` = `Level4_.`,\n    `Num.Level.3.and.4` = `Level3.4_N`,\n    `Pct.Level.3.and.4` = `Level3.4_.`  \n  ) \n  dataframe &lt;- dataframe |&gt;\n    mutate(across(\n      .cols = starts_with('Pct') | starts_with('Num') | starts_with('Mean'), \n      .fns = ~ as.numeric(.),        \n      .names = '{.col}'                            \n    ))\n  return(dataframe)\n}\nELA_2013_17_CS &lt;- rename_2013_17_CS(ELA_2013_17_CS)\nMATH_2013_17_CS &lt;- rename_2013_17_CS(MATH_2013_17_CS)\n\n# convert data types\ncharter_numeric &lt;- function(dataframe){\n  dataframe &lt;- dataframe |&gt;\n    mutate(across(\n      .cols = starts_with('Pct') | starts_with('Num') | starts_with('Mean'), \n      .fns = ~ as.numeric(.),        \n      .names = '{.col}'                            \n    ))\n  return(dataframe)\n}\nELA_2006_12_CS &lt;- charter_numeric(ELA_2006_12_CS)\n\n\nHere, I combine the separate years of charter school data sets for ELA and Math. I then assign a Subject column to then combine Math and ELA scores for charter schools into one dataframe.\n\n\nCode\nELA_2006_12_CS &lt;- ELA_2006_12_CS |&gt; mutate(`School.Name` = NA)\nELA_combined &lt;- rbind(ELA_2006_12_CS, ELA_2013_17_CS)\n\n# Combine the datasets into one for Math\nMATH_2006_12_CS &lt;- MATH_2006_12_CS |&gt; mutate(`School.Name` = NA)\nMATH_combined &lt;- rbind(MATH_2006_12_CS, MATH_2013_17_CS)\n\nELA_combined &lt;- ELA_combined  |&gt; mutate(across(c(`Mean.Scale.Score`, `Num.Level.1`, `Pct.Level.1`, \n                                      `Num.Level.2`, `Pct.Level.2`, `Num.Level.3`, `Pct.Level.3`, `Num.Level.4`, `Pct.Level.4`, `Num.Level.3.and.4`, `Pct.Level.3.and.4`), ~ as.numeric(.)))\n\nMATH_combined &lt;- MATH_combined  |&gt; mutate(across(c(`Mean.Scale.Score`, `Num.Level.1`, `Pct.Level.1`, \n                                      `Num.Level.2`, `Pct.Level.2`, `Num.Level.3`, `Pct.Level.3`, `Num.Level.4`, `Pct.Level.4`, `Num.Level.3.and.4`, `Pct.Level.3.and.4`), ~ as.numeric(.)))\n\n\n# For MATH (Calculating means by DBN and Year)\nMATH_combined_avg &lt;- MATH_combined |&gt;\n  filter(Grade != 'All Grades') |&gt;\n  group_by(DBN, Year) |&gt;\n  summarise(\n    Pct.Level.1 = mean(`Pct.Level.1`, na.rm = TRUE),\n    Pct.Level.2 = mean(`Pct.Level.2`, na.rm = TRUE),\n    Pct.Level.3 = mean(`Pct.Level.3`, na.rm = TRUE),\n    Mean.Scale.Score = mean(`Mean.Scale.Score`, na.rm = TRUE),\n    Number.Tested = sum(`Number.Tested`, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\n# For ELA (Calculating means by DBN and Year)\nELA_combined_avg &lt;- ELA_combined |&gt;\n  filter(Grade != 'All Grades') |&gt;\n  group_by(DBN, Year) |&gt;\n  summarise(\n    Pct.Level.1 = mean(`Pct.Level.1`, na.rm = TRUE),\n    Pct.Level.2 = mean(`Pct.Level.2`, na.rm = TRUE),\n    Pct.Level.3 = mean(`Pct.Level.3`, na.rm = TRUE),\n    Mean.Scale.Score = mean(`Mean.Scale.Score`, na.rm = TRUE),\n    Number.Tested = sum(`Number.Tested`, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\nELA_combined_avg$Subject &lt;- \"ELA\"\nMATH_combined_avg$Subject &lt;- \"Math\"\n\nCS_200617_data &lt;- rbind(ELA_combined_avg, MATH_combined_avg) \nCS_200617_data$isCharterSchool &lt;- \"Yes\"\n\nELA_06_23_ALL_cs &lt;- ELA_06_23_ALL_avg |&gt; mutate(`Subject` = \"ELA\", `isCharterSchool` = \"No\")\nMATH_06_23_ALL_cs &lt;- MATH_06_23_ALL_avg |&gt;mutate(`Subject` = \"Math\", `isCharterSchool` = \"No\")\n\nAll_CS_combined &lt;- \n  rbind(CS_200617_data, ELA_06_23_ALL_cs, MATH_06_23_ALL_cs)"
  },
  {
    "objectID": "NYCPSFunding.html#school-expenditures",
    "href": "NYCPSFunding.html#school-expenditures",
    "title": "New York City Public Schools Funding & Spending",
    "section": "School Expenditures",
    "text": "School Expenditures\nThe NYC Public School site publishes School Based Expenditure Reports for the years 2017 and 2018. I will scrape the data from the site and combine the two years into one dataset.\n\nexpenditure_201718\nexpenditure_201617\n\n\n\nCode\n# Function to scrape funding data for a given year\nscrape_exp_data &lt;- function(year) {\n  url &lt;- paste0(\"https://www.nycenet.edu/offices/d_chanc_oper/budget/dbor/sber/FY\", year, \"/FY\", year, \"_District_Overview.aspx\")\n  h &lt;- read_html(url)\n  table &lt;- h |&gt;\n    html_node('table') |&gt;   \n    html_table() |&gt;\n    mutate(Year = year)\n  colnames(table) &lt;- table[2, ]  # Set second row as the column names\n  table &lt;- table[-c(1, 2), ]   # Remove the first two rows (header and the one before it)\n  as.data.frame(table)\n}\n\n# Scrape data for 2016-2018\n# Citywide Summary for the School COunt and Title I Count\n# Per Capita Calculation for Dollars, Enrollment, Per Capita\nexpenditure_201718 &lt;- scrape_exp_data(2018)\nexpenditure_201617 &lt;- scrape_exp_data(2017)\n\n# Combine expenditure reports\nexpenditure_201718 &lt;- expenditure_201718 |&gt; rename(`Year` = `2018`)\nexpenditure_201617 &lt;- expenditure_201617 |&gt; rename(`Year` = `2017`)\n\nexpenditure_201618 &lt;- bind_rows(expenditure_201718,\n                                expenditure_201617)"
  },
  {
    "objectID": "NYCPSFunding.html#district-specific-data",
    "href": "NYCPSFunding.html#district-specific-data",
    "title": "New York City Public Schools Funding & Spending",
    "section": "District Specific Data",
    "text": "District Specific Data\nTo perform district level analysis, I will use the following datasets:\n\ndistrict_MATH_2006_12\ndistrict_ELA_2006_12\ndistrict_spending_0412\n\nThe New York Fed website has school funding data by district and borough for 2004 - 2012. I will pair that with Math and ELA scores by district for 2006 - 2012 from the City of New York.\n\n\nCode\nFILE_URL = \"https://www.newyorkfed.org/medialibrary/interactives/nyc-school-spending/nyc-school-spending/downloads/csd_expend.xlsx?v=1.0.0\"\nfname &lt;- \"csd_expend.xlsx\"\n  if(!file.exists(fname)){\n    download.file(FILE_URL, \n                  destfile = fname)\n  }\ndistrict_spending_0412 &lt;- as.data.frame(read_excel(fname, sheet = \"Sheet1\"))\n\n\n\n\nCode\ndistrict_ELA_2006_12 &lt;- get_cony_file(\"2006-2012_English_Language_Arts__ELA__Test_Results_-_District_-_All_Students.csv\", ELA_2006, ELA_URL_2006)\ndistrict_ELA_2006_12 &lt;- district_ELA_2006_12 |&gt;\n  filter(`Grade` == 'All Grades')\n\ndistrict_MATH_2006_12 &lt;- get_math_file(\"2006_-_2012__Math_Test_Results_-_District_-_All_Students.csv\", MATH_2006_12, MATH_URL_2006_12)\ndistrict_MATH_2006_12 &lt;- district_MATH_2006_12 |&gt;\n  filter(`Grade` == 'All Grades')"
  },
  {
    "objectID": "NYCPSFunding.html#nyc-shapefiles",
    "href": "NYCPSFunding.html#nyc-shapefiles",
    "title": "New York City Public Schools Funding & Spending",
    "section": "NYC Shapefiles",
    "text": "NYC Shapefiles\nTo visualize the data, I am using a NYC shapefile downloaded from NYC Gov and reading it into an sf object.\n\n\nCode\nlibrary(ggplot2)\nlibrary(sf)\n\nif(!file.exists(\"nysd_24d.zip\")){\n    download.file(\"https://s-media.nyc.gov/agencies/dcp/assets/files/zip/data-tools/bytes/nysd_24d.zip\", \n              destfile=\"nysd_24d.zip\")\n}\n\ntd &lt;- tempdir(); \nzip_contents &lt;- unzip(\"nysd_24d.zip\", \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_sf &lt;- read_sf(fname_shp)"
  },
  {
    "objectID": "NYCPSFunding.html#academic-performance-1",
    "href": "NYCPSFunding.html#academic-performance-1",
    "title": "New York City Public Schools Funding & Spending",
    "section": "Academic Performance",
    "text": "Academic Performance\nSummary Statistics for Academic Performance Metrics\n\nCode\ngrad_rates_summary &lt;- GRAD_RATES |&gt;\n  summarise(\n    Mean_Graduation_Rate = mean(`% Grads`, na.rm = TRUE),\n    Median_Graduation_Rate = median(`% Grads`, na.rm = TRUE),\n    SD_Graduation_Rate = sd(`% Grads`, na.rm = TRUE),\n    Min_Graduation_Rate = min(`% Grads`, na.rm = TRUE),\n    Max_Graduation_Rate = max(`% Grads`, na.rm = TRUE),\n    Mean_Advanced_Regents = mean(`% Advanced Regents of Cohort`, na.rm = TRUE),\n    Median_Advanced_Regents = median(`% Advanced Regents of Cohort`, na.rm = TRUE),\n    SD_Advanced_Regents = sd(`% Advanced Regents of Cohort`, na.rm = TRUE),\n    Min_Advanced_Regents = min(`% Advanced Regents of Cohort`, na.rm = TRUE),\n    Max_Advanced_Regents = max(`% Advanced Regents of Cohort`, na.rm = TRUE)\n  )\nregents_summary &lt;- REGENTS |&gt;\n  summarise(\n    Mean_Score = mean(`Mean Score`, na.rm = TRUE),\n    Median_Score = median(`Mean Score`, na.rm = TRUE),\n    SD_Score = sd(`Mean Score`, na.rm = TRUE),\n    Min_Score = min(`Mean Score`, na.rm = TRUE),\n    Max_Score = max(`Mean Score`, na.rm = TRUE)\n  )\nregents_cs_summary &lt;- REGENTS_CS |&gt;\n  summarise(\n    Mean_Score = mean(`Mean Score`, na.rm = TRUE),\n    Median_Score = median(`Mean Score`, na.rm = TRUE),\n    SD_Score = sd(`Mean Score`, na.rm = TRUE),\n    Min_Score = min(`Mean Score`, na.rm = TRUE),\n    Max_Score = max(`Mean Score`, na.rm = TRUE)\n  )\nmath_ela_summary &lt;- MATH_ELA_2013_23_ECON |&gt;\n  summarise(\n    Mean_Math_Score = mean(`Mean.Scale.Score.math`, na.rm = TRUE),\n    Median_Math_Score = median(`Mean.Scale.Score.math`, na.rm = TRUE),\n    SD_Math_Score = sd(`Mean.Scale.Score.math`, na.rm = TRUE),\n    Min_Math_Score = min(`Mean.Scale.Score.math`, na.rm = TRUE),\n    Max_Math_Score = max(`Mean.Scale.Score.math`, na.rm = TRUE),\n    \n    Mean_ELA_Score = mean(`Mean.Scale.Score.ela`, na.rm = TRUE),\n    Median_ELA_Score = median(`Mean.Scale.Score.ela`, na.rm = TRUE),\n    SD_ELA_Score = sd(`Mean.Scale.Score.ela`, na.rm = TRUE),\n    Min_ELA_Score = min(`Mean.Scale.Score.ela`, na.rm = TRUE),\n    Max_ELA_Score = max(`Mean.Scale.Score.ela`, na.rm = TRUE)\n  )\n\n# Display GRAD_RATES summary using GT\ngrad_rates_summary |&gt;\n  pivot_longer(cols = everything(), names_to = \"Statistic\", values_to = \"Value\") |&gt;  # Reshape to long format\n  gt() |&gt;\n  tab_header(\n    title = \"Summary Statistics for Graduation Rates\"\n  ) |&gt;\n  cols_label(\n    Statistic = \"Statistic\",\n    Value = \"Value\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Graduation Rates\",\n    columns = c(Statistic, Value)\n  )\n# Display REGENTS summary using GT\nregents_summary |&gt;\n  pivot_longer(cols = everything(), names_to = \"Statistic\", values_to = \"Value\") |&gt;  # Reshape to long format\n  gt() |&gt;\n  tab_header(\n    title = \"Summary Statistics for REGENTS\"\n  ) |&gt;\n  cols_label(\n    Statistic = \"Statistic\",\n    Value = \"Value\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Regents Exam Stats\",\n    columns = c(Statistic, Value)\n  )\n# Display REGENTS_CS summary using GT\nregents_cs_summary |&gt;\n  pivot_longer(cols = everything(), names_to = \"Statistic\", values_to = \"Value\") |&gt;  # Reshape to long format\n  gt() |&gt;\n  tab_header(\n    title = \"Summary Statistics for REGENTS_CS\"\n  ) |&gt;\n  cols_label(\n    Statistic = \"Statistic\",\n    Value = \"Value\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Regents Exam Stats (CS)\",\n    columns = c(Statistic, Value)\n  )\n# Display MATH_ELA_2013_23_ECON summary using GT\nmath_ela_summary |&gt;\n  pivot_longer(cols = everything(), names_to = \"Statistic\", values_to = \"Value\") |&gt;  # Reshape to long format\n  gt() |&gt;\n  tab_header(\n    title = \"Summary Statistics for Math & ELA Scores\"\n  ) |&gt;\n  cols_label(\n    Statistic = \"Statistic\",\n    Value = \"Value\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Math & ELA Scores\",\n    columns = c(Statistic, Value)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for Graduation Rates\n\n\n\nGraduation Rates\n\n\n\nStatistic\nValue\n\n\n\n\nMean_Graduation_Rate\n79.634482\n\n\nMedian_Graduation_Rate\n85.000000\n\n\nSD_Graduation_Rate\n20.067100\n\n\nMin_Graduation_Rate\n0.000000\n\n\nMax_Graduation_Rate\n100.000000\n\n\nMean_Advanced_Regents\n16.501030\n\n\nMedian_Advanced_Regents\n4.672897\n\n\nSD_Advanced_Regents\n23.783665\n\n\nMin_Advanced_Regents\n0.000000\n\n\nMax_Advanced_Regents\n100.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for REGENTS\n\n\n\nRegents Exam Stats\n\n\n\nStatistic\nValue\n\n\n\n\nMean_Score\n66.28208\n\n\nMedian_Score\n65.46349\n\n\nSD_Score\n12.04930\n\n\nMin_Score\n0.00000\n\n\nMax_Score\n98.27473\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for REGENTS_CS\n\n\n\nRegents Exam Stats (CS)\n\n\n\nStatistic\nValue\n\n\n\n\nMean_Score\n66.23786\n\n\nMedian_Score\n66.73881\n\n\nSD_Score\n11.01316\n\n\nMin_Score\n24.64444\n\n\nMax_Score\n96.53846\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics for Math & ELA Scores\n\n\n\nMath & ELA Scores\n\n\n\nStatistic\nValue\n\n\n\n\nMean_Math_Score\n421.7101\n\n\nMedian_Math_Score\n334.8825\n\n\nSD_Math_Score\n137.7098\n\n\nMin_Math_Score\n223.6111\n\n\nMax_Math_Score\n644.2222\n\n\nMean_ELA_Score\n424.0787\n\n\nMedian_ELA_Score\n330.6452\n\n\nSD_ELA_Score\n138.6206\n\n\nMin_ELA_Score\n226.3333\n\n\nMax_ELA_Score\n644.4445"
  },
  {
    "objectID": "NYCPSFunding.html#funding-allocation",
    "href": "NYCPSFunding.html#funding-allocation",
    "title": "New York City Public Schools Funding & Spending",
    "section": "Funding Allocation",
    "text": "Funding Allocation\nClassroom Teacher Salary Allocation\nFor most schools, the majority of their funding allocation is for classroom teachers.\n\n\nCode\nteacher_salary &lt;-  funding_C_201823 |&gt; \n  rename(Classroom_Teachers = `Classroom Teachers`) |&gt;\n  filter(!is.na(Classroom_Teachers)) |&gt;\n  group_by(Year) |&gt;\n  summarise(Mean_Classroom_Teachers = mean(Classroom_Teachers)) |&gt;\n  select(Year, Mean_Classroom_Teachers)\n\nggplot(teacher_salary, aes(x = Year, y = Mean_Classroom_Teachers)) +\n  geom_line(color = \"royalblue\", size = 1) +  \n  geom_point(color = \"royalblue\", size = 2) +   \n  labs(title = \"Mean Total Allocation to Classroom Teacher Salary Across Schools\", x = \"Year\", y = \"Mean Total Allocation($)\") +\n  scale_y_continuous(labels = scales::comma) +  # Format y-axis without scientific notation\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nHere we see that increasing funding for classroom teachers can boost student performance, but only up to a certain point.\n\nCode\n# Calculate percentage of each spending category relative to total funding\n\nfunding_spending &lt;- funding_C_201823 |&gt;\n  mutate(\n    pct_classroom_teachers = `Classroom Teachers` / `Total Allocation by Purpose` * 100,\n    pct_all_other_salaries = `All Other Salaries` / `Total Allocation by Purpose` * 100,\n    pct_employee_benefits = `Employee Benefits` / `Total Allocation by Purpose` * 100\n  )\n# Join Math and ELA data\nMATH_ELA_06_23 &lt;- MATH_06_23_ALL |&gt;\n  left_join(ELA_06_23_ALL, by = c(\"DBN\", \"Year\"), suffix = c(\".math\", \".ela\"))\n\n# Merge funding data with academic performance data \nfunding_performance_df &lt;- MATH_ELA_06_23 |&gt;\n  left_join(funding_spending, by = c(\"DBN\", \"Year\"))\n\n# Select relevant columns\nfunding_performance_select &lt;- funding_performance_df |&gt;\n  select(\n    DBN, \n    Year,\n    Classroom_Teachers = `Classroom Teachers`, \n    All_Other_Salaries = `All Other Salaries`,\n    Employee_Benefits = `Employee Benefits`,\n    BOCES_Services = `BOCES Services`,\n    All_Other = `All Other`,\n    School_Administration = `School Administration`,\n    Instructional_Media = `Instructional Media`,\n    Pupil_Support_Services = `Pupil\\r\\nSupport\\r\\nServices`,\n    State_Local_Funding = `State & Local Funding`,\n    Federal_Funding = `Federal \\r\\nFunding`,\n    Total_Funding_per_Pupil = `Total School Funding per Pupil`,\n    Mean_Scale_Score_Math = `Mean.Scale.Score.math`,  # Math performance\n    Mean_Scale_Score_ELA = `Mean.Scale.Score.ela`   # ELA performance\n  )\nfunding_performance_filterNA &lt;- funding_performance_select |&gt;\n  drop_na(Classroom_Teachers, All_Other_Salaries, Employee_Benefits, BOCES_Services, All_Other,\n         Mean_Scale_Score_Math, Mean_Scale_Score_ELA)\n\n# Function to compute and display the R-squared value for the polynomial model\nadd_r_squared &lt;- function(model) {\n  rsq &lt;- summary(model)$r.squared\n  return(paste(\"R^2: \", round(rsq, 3)))\n}\n\n# Scatter plot for Classroom_Teachers vs. Mean Scale Score (ELA) with polynomial regression and R² label\nggplot(funding_performance_filterNA, aes(x = Classroom_Teachers, y = Mean_Scale_Score_ELA)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), color = \"blue\", se = FALSE) +\n  labs(title = \"Classroom Teachers Funding vs. Mean Scale Score (ELA)\", \n       x = \"Classroom Teachers Funding\", y = \"Mean Scale Score (ELA)\") +\n  scale_x_continuous(labels = comma_format()) + \n  theme_minimal() +\n  annotate(\"text\", \n           x = max(funding_performance_filterNA$Classroom_Teachers) * 0.95, \n           y = max(funding_performance_filterNA$Mean_Scale_Score_ELA) * 0.95, \n           label = add_r_squared(lm(Mean_Scale_Score_ELA ~ poly(Classroom_Teachers, 3), data = funding_performance_filterNA)), \n           color = \"blue\", size = 3, hjust = 1, vjust = 1)\n# Scatter plot for Classroom_Teachers vs. Mean Scale Score (Math) with polynomial regression and R² label\nggplot(funding_performance_filterNA, aes(x = Classroom_Teachers, y = Mean_Scale_Score_Math)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), color = \"blue\", se = FALSE) +\n  labs(title = \"Classroom Teachers Funding vs. Mean Scale Score (Math)\", \n       x = \"Classroom Teachers Funding\", y = \"Mean Scale Score (Math)\") +\n  scale_x_continuous(labels = comma_format()) + \n  theme_minimal() +\n  annotate(\"text\", \n           x = max(funding_performance_filterNA$Classroom_Teachers) * 0.95, \n           y = max(funding_performance_filterNA$Mean_Scale_Score_Math) * 0.95, \n           label = add_r_squared(lm(Mean_Scale_Score_Math ~ poly(Classroom_Teachers, 3), data = funding_performance_filterNA)), \n           color = \"blue\", size = 3, hjust = 1, vjust = 1)"
  },
  {
    "objectID": "NYCPSFunding.html#funding-correlations",
    "href": "NYCPSFunding.html#funding-correlations",
    "title": "New York City Public Schools Funding & Spending",
    "section": "Funding Correlations",
    "text": "Funding Correlations\nLooking at the correlation matrix, we can see the relationships between different funding sources and academic performance metrics. Total Funding per Pupil has weak negative correlations with performance scores, which could imply that schools with more funding per pupil do not necessarily have better performance outcomes. This does makes sense as more funding is actually allocated based student need determined by performance lags.\nOn the flip side, it seems that there are weak positive correlations between Classroom Teacher funding with Mean_Scale_Score_Math (0.155) and Mean_Scale_Score_ELA (0.133). Both Instructional_Media and Pupil_Support_Services also have weak correlations with both Mean_Scale_Score_Math and Mean_Scale_Score_ELA. The weak correlations between funding categories (Classroom Teachers, All Other Salaries, Employee Benefits, etc.) with Mean_Scale_Score_Math and Mean_Scale_Score_ELA suggests that funding allocations may not directly influence student performance in these areas. Rather than driving academic performance, funding is more so being used to bring equity to the education system.\n\n\nCode\nfunding_spending &lt;- funding_C_201823 |&gt;\n  mutate(\n    pct_classroom_teachers = `Classroom Teachers` / `Total Allocation by Purpose` * 100,\n    pct_all_other_salaries = `All Other Salaries` / `Total Allocation by Purpose` * 100,\n    pct_employee_benefits = `Employee Benefits` / `Total Allocation by Purpose` * 100\n  )\n# Join Math and ELA data\nMATH_ELA_06_23 &lt;- MATH_06_23_ALL |&gt;\n  left_join(ELA_06_23_ALL, by = c(\"DBN\", \"Year\"), suffix = c(\".math\", \".ela\"))\n\n# Merge funding data with academic performance data \nfunding_performance_df &lt;- MATH_ELA_06_23 |&gt;\n  left_join(funding_spending, by = c(\"DBN\", \"Year\"))\n\n# Select relevant columns\nfunding_performance_select &lt;- funding_performance_df |&gt;\n  select(\n    DBN, \n    Year,\n    Classroom_Teachers = `Classroom Teachers`, \n    All_Other_Salaries = `All Other Salaries`,\n    Employee_Benefits = `Employee Benefits`,\n    BOCES_Services = `BOCES Services`,\n    All_Other = `All Other`,\n    School_Administration = `School Administration`,\n    Instructional_Media = `Instructional Media`,\n    Pupil_Support_Services = `Pupil\\r\\nSupport\\r\\nServices`,\n    State_Local_Funding = `State & Local Funding`,\n    Federal_Funding = `Federal \\r\\nFunding`,\n    Total_Funding_per_Pupil = `Total School Funding per Pupil`,\n    Mean_Scale_Score_Math = `Mean.Scale.Score.math`,  # Math performance\n    Mean_Scale_Score_ELA = `Mean.Scale.Score.ela`   # ELA performance\n  )\nfunding_performance_filterNA &lt;- funding_performance_select |&gt;\n  drop_na(Classroom_Teachers, All_Other_Salaries, Employee_Benefits, BOCES_Services, All_Other,\n         Mean_Scale_Score_Math, Mean_Scale_Score_ELA)\n\ncorrelation_matrix &lt;- funding_performance_filterNA |&gt;\n  select(\n    Classroom_Teachers,\n    All_Other_Salaries,\n    Employee_Benefits,\n    BOCES_Services,\n    All_Other,\n    School_Administration,\n    Instructional_Media,\n    Pupil_Support_Services,\n    State_Local_Funding,\n    Federal_Funding,\n    Total_Funding_per_Pupil,\n    Mean_Scale_Score_Math,\n    Mean_Scale_Score_ELA\n  ) |&gt;\n  cor(use = \"complete.obs\") \n\ncorrelation_matrix_melted &lt;- melt(correlation_matrix)\n\n# Plot the correlation heatmap\nggplot(correlation_matrix_melted, aes(Var1, Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1, 1)) +\n  theme_minimal() +\n  labs(title = \"Correlation between Funding and Performance Metrics\",\n       x = \"Variables\",\n       y = \"Variables\",\n       fill = \"Correlation\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nCode\n# Function to compute and display the R-squared value for the polynomial model\nadd_r_squared &lt;- function(model) {\n  rsq &lt;- summary(model)$r.squared\n  return(paste(\"R^2: \", round(rsq, 3)))\n}\n\n# Scatter plot for All_Other_Salaries vs. Mean Scale Score (ELA) with polynomial regression and R² label\nggplot(funding_performance_filterNA, aes(x = All_Other_Salaries, y = Mean_Scale_Score_ELA)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), color = \"blue\", se = FALSE) +\n  labs(title = \"All Other Salaries Funding vs. Mean Scale Score (ELA)\", \n       x = \"All Other Salaries Funding\", y = \"Mean Scale Score (ELA)\") +\n  scale_x_continuous(labels = comma_format()) + \n  theme_minimal() +\n  annotate(\"text\", \n           x = max(funding_performance_filterNA$All_Other_Salaries) * 0.95, # position on x-axis (slightly left from max)\n           y = max(funding_performance_filterNA$Mean_Scale_Score_ELA) * 0.95, # position on y-axis (slightly down from max)\n           label = add_r_squared(lm(Mean_Scale_Score_ELA ~ poly(All_Other_Salaries, 2), data = funding_performance_filterNA)), \n           color = \"blue\", size = 3, hjust = 1, vjust = 1)\n# Scatter plot for Employee_Benefits vs. Mean Scale Score (ELA) with polynomial regression and R² label\nggplot(funding_performance_filterNA, aes(x = Employee_Benefits, y = Mean_Scale_Score_ELA)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), color = \"blue\", se = FALSE) +\n  labs(title = \"Employee Benefits Funding vs. Mean Scale Score (ELA)\", \n       x = \"Employee Benefits Funding\", y = \"Mean Scale Score (ELA)\") +\n  scale_x_continuous(labels = comma_format()) + \n  theme_minimal() +\n  annotate(\"text\", \n           x = max(funding_performance_filterNA$Employee_Benefits) * 0.95, \n           y = max(funding_performance_filterNA$Mean_Scale_Score_ELA) * 0.95, \n           label = add_r_squared(lm(Mean_Scale_Score_ELA ~ poly(Employee_Benefits, 3), data = funding_performance_filterNA)), \n           color = \"blue\", size = 3, hjust = 1, vjust = 1)\n# Scatter plot for All_Other vs. Mean Scale Score (ELA) with polynomial regression and R² label\nggplot(funding_performance_filterNA, aes(x = All_Other, y = Mean_Scale_Score_ELA)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), color = \"blue\", se = FALSE) +\n  labs(title = \"All Other Funding vs. Mean Scale Score (ELA)\", \n       x = \"All Other Funding\", y = \"Mean Scale Score (ELA)\") +\n  scale_x_continuous(labels = comma_format()) + \n  theme_minimal() +\n  annotate(\"text\", \n           x = max(funding_performance_filterNA$All_Other) * 0.95, \n           y = max(funding_performance_filterNA$Mean_Scale_Score_ELA) * 0.95, \n           label = add_r_squared(lm(Mean_Scale_Score_ELA ~ poly(All_Other, 3), data = funding_performance_filterNA)), \n           color = \"blue\", size = 3, hjust = 1, vjust = 1)\n# Scatter plot for All_Other_Salaries vs. Mean Scale Score (Math) with polynomial regression and R² label\nggplot(funding_performance_filterNA, aes(x = All_Other_Salaries, y = Mean_Scale_Score_Math)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), color = \"blue\", se = FALSE) +\n  labs(title = \"All Other Salaries Funding vs. Mean Scale Score (Math)\", \n       x = \"All Other Salaries Funding\", y = \"Mean Scale Score (Math)\") +\n  scale_x_continuous(labels = comma_format()) + \n  theme_minimal() +\n  annotate(\"text\", \n           x = max(funding_performance_filterNA$All_Other_Salaries) * 0.95, \n           y = max(funding_performance_filterNA$Mean_Scale_Score_Math) * 0.95, \n           label = add_r_squared(lm(Mean_Scale_Score_Math ~ poly(All_Other_Salaries, 3), data = funding_performance_filterNA)), \n           color = \"blue\", size = 3, hjust = 1, vjust = 1)\n# Scatter plot for Employee_Benefits vs. Mean Scale Score (Math) with polynomial regression and R² label\nggplot(funding_performance_filterNA, aes(x = Employee_Benefits, y = Mean_Scale_Score_Math)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), color = \"blue\", se = FALSE) +\n  labs(title = \"Employee Benefits Funding vs. Mean Scale Score (Math)\", \n       x = \"Employee Benefits Funding\", y = \"Mean Scale Score (Math)\") +\n  scale_x_continuous(labels = comma_format()) + \n  theme_minimal() +\n  annotate(\"text\", \n           x = max(funding_performance_filterNA$Employee_Benefits) * 0.95, \n           y = max(funding_performance_filterNA$Mean_Scale_Score_Math) * 0.95, \n           label = add_r_squared(lm(Mean_Scale_Score_Math ~ poly(Employee_Benefits, 3), data = funding_performance_filterNA)), \n           color = \"blue\", size = 3, hjust = 1, vjust = 1)\n# Scatter plot for All_Other vs. Mean Scale Score (Math) with polynomial regression and R² label\nggplot(funding_performance_filterNA, aes(x = All_Other, y = Mean_Scale_Score_Math)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), color = \"blue\", se = FALSE) +\n  labs(title = \"All Other Funding vs. Mean Scale Score (Math)\", \n       x = \"All Other Funding\", y = \"Mean Scale Score (Math)\") +\n  scale_x_continuous(labels = comma_format()) + \n  theme_minimal() +\n  annotate(\"text\", \n           x = max(funding_performance_filterNA$All_Other) * 0.95, \n           y = max(funding_performance_filterNA$Mean_Scale_Score_Math) * 0.95, \n           label = add_r_squared(lm(Mean_Scale_Score_Math ~ poly(All_Other, 3), data = funding_performance_filterNA)), \n           color = \"blue\", size = 3, hjust = 1, vjust = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegents\nIn addition to Mean Scale Scores for Math and ELA, we can take a look at Regent exam scores. The Regents exams are standardized tests in New York State that are administered to high school students. These exams are used to measure student achievement and determine whether students have met the minimum requirements for graduation. The exams cover a range of subjects, including English, math, science, and social studies.\n\n\nCode\n# Split the REGENTS data by Regents Exam type\nregents_split &lt;- REGENTS  |&gt;\n  group_by(DBN, Year, `Regents Exam`)  |&gt;\n  summarise(\n    total_tested = sum(`Total Tested`, na.rm = TRUE),\n    mean_score = mean(`Mean Score`, na.rm = TRUE),\n    number_below_65 = sum(`Number Scoring Below 65`, na.rm = TRUE),\n    percent_below_65 = mean(`Percent Scoring Below 65`, na.rm = TRUE),\n    number_65_or_above = sum(`Number Scoring 65 or Above`, na.rm = TRUE),\n    percent_65_or_above = mean(`Percent Scoring 65 or Above`, na.rm = TRUE),\n    number_80_or_above = sum(`Number Scoring 80 or Above`, na.rm = TRUE),\n    percent_80_or_above = mean(`Percent Scoring 80 or Above`, na.rm = TRUE),\n    number_cuny_proficiency = sum(`Number meeting CUNY proficiency requirmenets`, na.rm = TRUE),\n    percent_cuny_proficiency = mean(`Percent meeting CUNY proficiency requirmenets`, na.rm = TRUE)\n  )  |&gt;\n  ungroup()\n\n\nregents_wide &lt;- regents_split |&gt;\n  pivot_wider(\n    names_from = `Regents Exam`,\n    values_from = c(\n      total_tested, mean_score, number_below_65, percent_below_65,\n      number_65_or_above, percent_65_or_above, number_80_or_above,\n      percent_80_or_above, number_cuny_proficiency, percent_cuny_proficiency\n    )\n  )\n\n# join regents with funding\nregents_funding &lt;- REGENTS |&gt;\n  filter(`Year` &gt;= 2018) |&gt;\n  left_join(funding_C_201823, by = c(\"DBN\", \"Year\"))\n\n# Calculate correlation between Total Funding per Pupil and Mean Score\n\nregent_corr&lt;-cor(regents_funding$`Total School Funding per Pupil`, regents_funding$`Mean Score`, use = \"complete.obs\")\ncat(\"Correlation between Total School Funding per Pupil and Mean Regent Score: \\n \", regent_corr, \"\\n\")\n\n\nCorrelation between Total School Funding per Pupil and Mean Regent Score: \n  -0.2325824 \n\n\nCode\nggplot(regents_funding, aes(x = `Total School Funding per Pupil`, y = `Mean Score`)) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"Total Funding per Pupil vs Mean Regent Score\",\n       x = \"Total Funding per Pupil\",\n       y = \"Mean Regent Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nregent_corr&lt;-cor(regents_funding$`Total School Funding per Pupil`, regents_funding$`Percent Scoring 65 or Above`, use = \"complete.obs\")\ncat(\"Correlation between Total School Funding per Pupil and Percent Scoring 65 or Above: \\n\", regent_corr, \"\\n\")\n\n\nCorrelation between Total School Funding per Pupil and Percent Scoring 65 or Above: \n -0.1992125 \n\n\nCode\nggplot(regents_funding, aes(x = `Total School Funding per Pupil`, y = `Percent Scoring 65 or Above`)) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"Total School Funding per Pupil vs Percent Scoring Above 65 on Regents\",\n       x = \"Total School Funding per Pupil\",\n       y = \"Percent Scoring 65 or Above\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Clean column names by removing unwanted characters (e.g., spaces, newline characters)\ncolnames(regents_funding) &lt;- gsub(\"[[:space:][:cntrl:]]\", \"_\", colnames(regents_funding))\n\ncolnames(regents_funding) &lt;- gsub(\"School_Name.x\", \"School_Name\", colnames(regents_funding))\ncolnames(regents_funding) &lt;- gsub(\"School_Name.y\", \"School_Name\", colnames(regents_funding))\ncolnames(regents_funding) &lt;- gsub(\"Pupil_rnSupport_rnServices\", \"Pupil_Support_Services\", colnames(regents_funding))\ncolnames(regents_funding) &lt;- gsub(\"Federal_rnFunding\", \"Federal_Funding\", colnames(regents_funding))\ncolnames(regents_funding) &lt;- gsub(\"State_&_Local_Funding_per_Pupil\", \"State_&_Local_Funding_per_Pupil\", colnames(regents_funding))\n\nfunding_columns &lt;- c(\n  \"State_&_Local_Funding\", \n  \"Federal___Funding\", \n  \"State_&_Local__Funding_per_Pupil\", \n  \"Federal_Funding___per_Pupil\", \n  \"Total_School_Funding_per_Pupil\",\n  \"Total_School_Allocation_w/_Central_District_Costs\"\n)\n\n# Define exam performance columns\nexam_columns &lt;- c(\n  \"Mean_Score\",\n  \"Percent_Scoring_65_or_Above\",\n  \"Percent_Scoring_80_or_Above\"\n)\n\n# Subset the data\nregents_funding_clean &lt;- regents_funding |&gt;\n  select(all_of(c(funding_columns, exam_columns)))\n\n\nIn the polynomial regression between Total School Funding per Pupil and Mean Regent Score, about 6.7% of the variation in Mean Score can be explained by Total School Funding per Pupil. This suggests that there are likely other factors influencing mean scores, as funding per pupil is not a strong predictor of the mean exam score in this model.\n\n\nCode\n# drop nas and run model\nregents_funding_na &lt;- regents_funding_clean |&gt;\n  drop_na(`Mean_Score`, `Total_School_Funding_per_Pupil`)\nmodel_poly_1 &lt;- lm(`Mean_Score` ~ poly(`Total_School_Funding_per_Pupil`, 2), data = regents_funding_na)\nsummary(model_poly_1)\n\n\n\nCall:\nlm(formula = Mean_Score ~ poly(Total_School_Funding_per_Pupil, \n    2), data = regents_funding_na)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-71.639  -8.885  -0.351   8.478  38.867 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                                63.793      0.112  569.66   &lt;2e-16\npoly(Total_School_Funding_per_Pupil, 2)1 -320.394     12.207  -26.25   &lt;2e-16\npoly(Total_School_Funding_per_Pupil, 2)2  156.890     12.207   12.85   &lt;2e-16\n                                            \n(Intercept)                              ***\npoly(Total_School_Funding_per_Pupil, 2)1 ***\npoly(Total_School_Funding_per_Pupil, 2)2 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.21 on 11880 degrees of freedom\nMultiple R-squared:  0.06707,   Adjusted R-squared:  0.06691 \nF-statistic:   427 on 2 and 11880 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nggplot(regents_funding_na, aes(x = `Total_School_Funding_per_Pupil`, y = `Mean_Score`)) +\n  geom_point(color = \"skyblue\", alpha = 0.5) +  # Scatter points\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), color = \"navy\", se = FALSE) +  # Polynomial line\n  labs(\n    title = \"Mean Regent Score vs Total School Funding per Pupil\",\n    subtitle = \"Polynomial Regression\",\n    x = \"Total School Funding per Pupil\",\n    y = \"Mean Regent Score\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "NYCPSFunding.html#charter-schools",
    "href": "NYCPSFunding.html#charter-schools",
    "title": "New York City Public Schools Funding & Spending",
    "section": "Charter Schools",
    "text": "Charter Schools\nDo charter schools receive more funding and how are their funds spent? Moving on to charter schools. Charter schools are publicly funded but operate independently. Charter school funding averages 30% less than traditional public schools. Charter schools in major cities received, on average, $7,147 less per pupil during the 2019-20 school year, a new study finds.7 While they need to meet certain performance goals, they receive less per-pupil funding than district schools, which can impact their ability to offer additional resources and support for students. The range of scores remains similar to that ofnon charter schools, however, charter schools have a lower median score in comparison to district-run schools.\n\n\nCode\n# Calculate the average Max Scale Score for charter and non-charter schools for each subject\nperformance_comparison &lt;- All_CS_combined |&gt;\n  group_by(Subject, isCharterSchool) |&gt;\n  summarise(\n    Average_Score = mean(Mean.Scale.Score, na.rm = TRUE),\n    SD_Score = sd(Mean.Scale.Score, na.rm = TRUE),\n    N = n(), \n    .groups = 'drop'\n  )\n\n# Plot mean scale score by charter school status\nggplot(performance_comparison, aes(x = isCharterSchool, y = Average_Score, fill = isCharterSchool)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ Subject) +  # Separate by Subject (ELA and Math)\n  labs(\n    title = \"Comparison of Mean Scale Score by Charter School Status\",\n    x = \"Charter School Status\",\n    y = \"Average Mean Scale Score\"\n  ) +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"No\" = \"lightblue\", \"Yes\" = \"lightcoral\"))\n\n\n\n\n\n\n\n\n\nCode\nAll_CS_combined_clean &lt;- All_CS_combined |&gt;\n  filter(is.finite(Mean.Scale.Score))\n\n# Boxplots for Charter School status\nggplot(All_CS_combined, aes(x = isCharterSchool, y = Mean.Scale.Score, fill = isCharterSchool)) +\n  geom_boxplot() +  \n  facet_wrap(~ Subject) +  \n  labs(\n    title = \"Comparison of Mean Scale Score by Charter School Status\",\n    x = \"Charter School Status\",\n    y = \"Mean Scale Score\"\n  ) +\n  theme_minimal() + \n  scale_fill_manual(values = c(\"No\" = \"lightblue\", \"Yes\" = \"palegreen3\")) +  \n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)  \n  )"
  },
  {
    "objectID": "NYCPSFunding.html#are-there-certain-schools-that-are-more-likely-to-be-successful-in-obtaining-sources-of-funding",
    "href": "NYCPSFunding.html#are-there-certain-schools-that-are-more-likely-to-be-successful-in-obtaining-sources-of-funding",
    "title": "New York City Public Schools Funding & Spending",
    "section": "Are there certain schools that are more likely to be successful in obtaining sources of funding?",
    "text": "Are there certain schools that are more likely to be successful in obtaining sources of funding?\nAs we saw above, the majority of public school funding comes from state and local sources with local resources filling the gap between State aid and the cost of instruction. The data we are using in this analysis, however, does not distinguish between State and Local funding. As we can see from the Rockefeller Institute’s report, 54% of education revenue was local.8 This is a significant portion of funding that is not distinguished in this analysis."
  },
  {
    "objectID": "NYCPSFunding.html#how-does-city-wide-district-wide-and-school-wide-per-pupil-spending-relate-to-academic-performance",
    "href": "NYCPSFunding.html#how-does-city-wide-district-wide-and-school-wide-per-pupil-spending-relate-to-academic-performance",
    "title": "New York City Public Schools Funding & Spending",
    "section": "How does city-wide, district-wide, and school-wide per-pupil spending relate to academic performance?",
    "text": "How does city-wide, district-wide, and school-wide per-pupil spending relate to academic performance?\nIn the code below, I start by merging two datasets on district and year: district_spending_0412 (school district spending data) with two academic performance datasets: district_ELA_2006_12 and district_MATH_2006_12. I filter out the years 2004 and 2005 are excluded due to incomplete academic performance data for these years. Joining with the nyc shapefile, we can map the spending data by district.\n\n\nCode\ndistrict_spending_ELA&lt;- district_spending_0412 |&gt;\n  left_join(district_ELA_2006_12, by = c(\"csd\" = \"District\", \"year\" = \"Year\")) |&gt;\n    filter(year != 2004 & year != 2005) # filter out 2004-2005\n\ndistrict_spending_MATH &lt;- district_spending_0412 |&gt;\n  left_join(district_MATH_2006_12, by = c(\"csd\" = \"District\", \"year\" = \"Year\")) |&gt;\n    filter(year != 2004 & year != 2005) # filter out 2004-2005\n\n\nfunding_map_df &lt;- nyc_sf |&gt;\n  left_join(district_spending_0412, by = c(\"SchoolDist\" = \"csd\")) \n\ngrand_total_df &lt;- funding_map_df[, c(\"year\", \"SchoolDist\", \"grand_total_csd\", \"grand_total_boro\", \"grand_total_nyc\")]\n\nfunding_map_df$grand_total_nyc &lt;- as.numeric(funding_map_df$grand_total_nyc)\n\nfunding_map_gif &lt;- funding_map_df |&gt; \n  ggplot() + \n  geom_sf(aes(fill = grand_total_csd), color = \"white\", size = 0.2) + \n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\", name = \"Total NYC Funding\") +\n  labs(\n    title = \"Funding by District in NYC (Total)\", \n    caption = \"Data Source: NYC School Spending\"\n  )  + \n  theme_minimal() +   \n  theme(\n    legend.position = \"bottom\", \n    legend.key.width = unit(4, \"lines\"), \n    legend.text = element_text(size = 10), \n    legend.title = element_text(size = 12), \n    plot.margin = margin(10, 10, 10, 10) \n  ) + \n  geom_sf_text(data = nyc_sf, aes(geometry = geometry, label = SchoolDist), color = \"white\", size = 2) +\n  transition_time(as.integer(year)) + \n  labs(title = \"Funding by District in NYC (Total) - Year: {frame_time}\")\n\n# save_animation(funding_map_gif, \"fundingmap.gif\")\n\n\n\nTo get a look at some more recent years after, we can use the district expenditure data for 2017 and 2018.\n\n\nCode\nexpenditure_201718_filtered &lt;- expenditure_201618 |&gt;\n  filter(str_starts(`By District`, \"District\")) |&gt;\n  mutate(District = str_extract(`By District`, \"\\\\d{2}\"),\n         District = as.numeric(`District`),\n    `Dollar_Amt` = str_extract(`Dollars`, \"(?&lt;=\\\\$).*\"),\n     Dollar_Amt = gsub(\",\", \"\", Dollar_Amt),  \n    Dollar_Amt = as.numeric(Dollar_Amt),   \n    Per_Capita_Amt = str_extract(`Per Capita`, \"(?&lt;=\\\\$).*\"),\n    Per_Capita_Amt = gsub(\",\", \"\", Per_Capita_Amt), \n    Per_Capita_Amt = as.numeric(Per_Capita_Amt),\n  )\n\n# Join with Map Shapefile data\n\nexpenditure_map &lt;- nyc_sf |&gt;\n  left_join(expenditure_201718_filtered, by = c(\"SchoolDist\" = \"District\")) \n\nexpenditure_map_gif &lt;- expenditure_map |&gt; \n  ggplot() + \n  geom_sf(aes(fill = `Dollar_Amt`), color = \"white\", size = 0.2) + \n  scale_fill_gradient(low = \"lightyellow\", high = \"darkgreen\", name = \"Total Expenditure ($)\") +\n  labs(\n    title = \"Expenditure by District in NYC (Total)\", \n    caption = \"Data Source: NYC School Expenditures\"\n  ) + \n  theme_minimal() +   \n theme(\n    legend.position = \"bottom\",\n    legend.key.width = unit(4, \"lines\"),  \n    legend.text = element_text(size = 10),   \n    legend.title = element_text(size = 12),  \n    plot.margin = margin(10, 10, 10, 10)\n  ) +\n  geom_sf_text(data = nyc_sf, aes(geometry = geometry, label = SchoolDist), color = \"white\", size = 2) +\n  transition_time(as.integer(Year)) + \n  labs(title = \"Expenditure  by District in NYC (Total) - Year: {frame_time}\")\n\n# expenditure_map_gif\n\n# anim_save(filename=\"expendituremap.gif\", expenditure_map_gif)\n\n\n\nWhen performing a linear regression between Mean Scale Score and grand_total_csd, there is a negative relationship between total school funding per pupil and mean scale scores. The model explains only 15.56% of the variation in the mean scale scores for math. However, the relationship between funding and performance is highly statistically significant (p-value &lt; 0.001), meaning that the association between total funding and academic performance is unlikely to be due to chance.\n\n\nCode\n# polynomial regression model: Mean Score math as a function of spending\nmodel_lm_m &lt;- lm(Mean.Scale.Score ~ grand_total_csd, data = district_spending_MATH)\n\n\nsummary(model_lm_m)\n\n\n\nCall:\nlm(formula = Mean.Scale.Score ~ grand_total_csd, data = district_spending_MATH)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.028  -7.436   1.819   8.449  28.288 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      7.306e+02  1.147e+01  63.706  &lt; 2e-16 ***\ngrand_total_csd -3.318e-03  6.494e-04  -5.109 6.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.94 on 222 degrees of freedom\nMultiple R-squared:  0.1052,    Adjusted R-squared:  0.1012 \nF-statistic:  26.1 on 1 and 222 DF,  p-value: 6.964e-07\n\n\nLikewise, for ELA the linear regression results below indicates a small negative relationship between grand_total_csd and Mean Scale Score. The coefficient for grand_total_csd (total funding) is negative, suggesting that higher funding is associated with slightly lower mean scale scores. 15% of the variation in mean ELA scale scores can be explained by total district spending.\n\n\nCode\n# polynomial regression model: Mean Score ela as a function of spending\nmodel_lm_e &lt;- lm(Mean.Scale.Score ~ grand_total_csd, data = district_spending_ELA)\nsummary(model_lm_e)\n\n\n\nCall:\nlm(formula = Mean.Scale.Score ~ grand_total_csd, data = district_spending_ELA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.295  -4.656   1.417   5.594  20.428 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      7.061e+02  7.747e+00  91.136  &lt; 2e-16 ***\ngrand_total_csd -2.806e-03  4.387e-04  -6.395 9.35e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.417 on 222 degrees of freedom\nMultiple R-squared:  0.1556,    Adjusted R-squared:  0.1518 \nF-statistic:  40.9 on 1 and 222 DF,  p-value: 9.346e-10\n\n\n\nCode\nggplot(district_spending_MATH, aes(x = `grand_total_csd`, y = `Mean.Scale.Score`)) +\n  geom_point(color = \"skyblue\", alpha = 0.5) +  # Scatter points\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), color = \"navy\", se = FALSE) +  # Polynomial line\n  labs(\n    title = \"Mean Math Score vs Total District Spending\",\n    subtitle = \"Polynomial Regression\",\n    x = \"TotalTotal District Spending\",\n    y = \"Mean Math Score\"\n  ) +\n  theme_minimal()\nggplot(district_spending_ELA, aes(x = `grand_total_csd`, y = `Mean.Scale.Score`)) +\n  geom_point(color = \"skyblue\", alpha = 0.5) +  # Scatter points\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), color = \"navy\", se = FALSE) +  # Polynomial line\n  labs(\n    title = \"Mean ELA Score vs Total District Spending\",\n    subtitle = \"Polynomial Regression\",\n    x = \"Total District Spending\",\n    y = \"Mean ELA Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nDoes spending on certain purposes or functions correlate with higher academic performance? The highest-spending districts in New York City were not necessarily the highest-performing.\nIn most districts, classroom spending is around 50% of total district funding. Classroom spending is the largest category of spending, followed by support services, leadership, ancillary services, and building services.\n\n\nCode\ndistrict_spending_pct &lt;- district_spending_0412 |&gt;\n  mutate(\n    classroom_csd_pct = classroom_csd/grand_total_csd,\n         support_csd_pct = support_csd/grand_total_csd, \nleadership_csd_pct = leadership_csd/grand_total_csd,\nancillary_csd_pct = ancillary_csd/grand_total_csd,\nbuilding_csd_pct = building_csd/grand_total_csd)\n\n\n# Scatter plot of classroom spending vs. total district spending\nggplot(district_spending_pct, aes(x = csd, y = classroom_csd_pct)) +\n  geom_point(color = \"skyblue\", alpha = 0.5) +  \n  geom_smooth(method = \"lm\", color = \"navy\", se = FALSE) + \n  labs(\n    title = \"Classroom Spending vs  District\",\n    x = \"Total District Spending\",\n    y = \"Classroom Spending (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nClassroom Spending vs. Mean Scale Score Given the sizeable portion of classroom spending,let’s take a look at its relation to student’s academic performance. The scatter plot below shows the relationship between classroom spending and mean scale scores for ELA and Math. The blue line represents a non-linear smoothing of the data, showing a slight negative trend between classroom spending and mean scale scores.\n\nCode\nggplot(district_spending_ELA, aes(x = classroom_csd, y = Mean.Scale.Score)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", color = \"blue\", se = FALSE) +  # Non-linear smoothing\n  labs(\n    title = \"Classroom Spending vs. Mean Scale Score (ELA)\",\n    x = \"Classroom Spending (CSD)\",\n    y = \"Mean Scale Score (ELA)\"\n  ) +\n  theme_minimal()\nggplot(district_spending_MATH, aes(x = classroom_csd, y = Mean.Scale.Score)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", color = \"red\", se = FALSE) +  # Non-linear smoothing\n  labs(\n    title = \"Classroom Spending vs. Mean Scale Score (Math)\",\n    x = \"Classroom Spending (CSD)\",\n    y = \"Mean Scale Score (Math)\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "NYCPSFunding.html#footnotes",
    "href": "NYCPSFunding.html#footnotes",
    "title": "New York City Public Schools Funding & Spending",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://rockinst.org/wp-content/uploads/2024/12/2024-12-Foundation-Aid-Report.pdf↩︎\nhttps://rockinst.org/wp-content/uploads/2024/12/2024-12-Foundation-Aid-Report.pdf↩︎\nhttps://fiscalpolicy.org/understanding-foundation-aid-how-public-school-funding-works-in-new-york-state↩︎\nhttps://www.chalkbeat.org/newyork/2024/12/03/nyc-foundation-aid-study-proposes-updates-to-school-funding-formula/↩︎\nhttps://www.schools.nyc.gov/about-us/funding/funding-our-schools/fair-student-funding-fsf↩︎\nhttps://www.schools.nyc.gov/learning/special-education/school-settings/district-75↩︎\nhttps://www.k12dive.com/news/charters-less-funding-traditional-public-schools/690326/↩︎\nhttps://rockinst.org/wp-content/uploads/2024/12/2024-12-Foundation-Aid-Report.pdf↩︎\nhttps://rockinst.org/wp-content/uploads/2024/12/2024-12-Foundation-Aid-Report.pdf↩︎"
  }
]